{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#import pandas as pd\n",
    "#import pandas_profiling as pd_pi\n",
    "#import seaborn as sns\n",
    "#%matplotlib inline\n",
    "#from sklearn import preprocessing as pre\n",
    "#from sklearn.pipeline import Pipeline as pipe\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.linear_model import RidgeClassifier\n",
    "##from sklearn.linear_model import Lasso\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.linear_model import SGDClassifier\n",
    "#from sklearn import metrics\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "#from sklearn.model_selection import RandomizedSearchCV\n",
    "#from scipy import stats\n",
    "#import random\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#from sklearn.model_selection import cross_val_predict\n",
    "#from sklearn.model_selection import cross_validate\n",
    "#from sklearn.ensemble import AdaBoostClassifier\n",
    "#from sklearn.ensemble import GradientBoostingClassifier\n",
    "#from sklearn.model_selection import StratifiedKFold\n",
    "#from sklearn.metrics import roc_auc_score\n",
    "#from sklearn.metrics import cohen_kappa_score\n",
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ORANGE JUICE DATA\n",
    "###################\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "data = pd.read_csv('OJ.csv')\n",
    "data = data.drop(columns='Id')\n",
    "data['Purchase'] = data['Purchase'].replace(['CH','MM'],[1,0])\n",
    "data['Store7'] = data['Store7'].replace(['Yes','No'],[1,0])\n",
    "data = data.drop('STORE',axis = 1)\n",
    "data_dummy = pd.get_dummies(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(data, \n",
    "          target, \n",
    "          split=0.7):\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X = data.drop(target,axis=1)\n",
    "    y = data[target]\n",
    "    global X_train, X_test, y_train, y_test, seed\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1-split)\n",
    "    import random\n",
    "    seed = random.randint(150,900)\n",
    "    return X_train, X_test, y_train, y_test, seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = setup(data, 'Purchase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(estimator = None, \n",
    "                 ensemble = False, \n",
    "                 method = 'Bagging', \n",
    "                 fold = 10, \n",
    "                 round = 4,  \n",
    "                 verbose = True):\n",
    "    \n",
    "     \n",
    "    \"\"\"  \n",
    "     \n",
    "  Description:\n",
    "  ------------\n",
    "  This function creates a model and scores it using Stratified Cross Validation. \n",
    "  The output prints the score grid that shows Accuracy, AUC, Recall, Precision, \n",
    "  F1 and Kappa by fold (default = 10). \n",
    "  \n",
    "  Function also returns a trained model object that can be used for further \n",
    "  processing in pycaret or can be used to call any method available in sklearn. \n",
    "  \n",
    "  setup() function must be called before using create_model()\n",
    "  \n",
    "    Example\n",
    "    -------\n",
    "    lr = create_model('lr')\n",
    "    \n",
    "    This will return trained Logistic Regression.\n",
    "  \n",
    "  Parameters\n",
    "  ----------\n",
    "  \n",
    "  estimator : string, default = None\n",
    "  \n",
    "  Enter abbreviated string of the estimator class. List of estimators supported:\n",
    "  \n",
    "  Estimator                   Abbreviated String     Original Implementation \n",
    "  ---------                   ------------------     -----------------------\n",
    "  Logistic Regression         'lr'                   linear_model.LogisticRegression\n",
    "  K Nearest Neighbour         'knn'                  neighbors.KNeighborsClassifier\n",
    "  Naives Bayes                'nb'                   naive_bayes.GaussianNB\n",
    "  Decision Tree               'dt'                   tree.DecisionTreeClassifier\n",
    "  SVM (Linear)                'svm'                  linear_model.SGDClassifier\n",
    "  SVM (RBF)                   'rbfsvm'               svm.SVC\n",
    "  Gaussian Process            'gpc'                  gaussian_process.GPC\n",
    "  Multi Level Perceptron      'mlp'                  neural_network.MLPClassifier\n",
    "  Ridge Classifier            'ridge'                linear_model.RidgeClassifier\n",
    "  Random Forest               'rf'                   ensemble.RandomForestClassifier\n",
    "  Quadratic Disc. Analysis    'qda'                  discriminant_analysis.QDA\n",
    "  AdaBoost                    'ada'                  ensemble.AdaBoostClassifier\n",
    "  Gradient Boosting           'gbc'                  ensemble.GradientBoostingClassifier\n",
    "  Linear Disc. Analysis       'lda'                  discriminant_analysis.LDA\n",
    "  Extra Trees Classifier      'et'                   ensemble.ExtraTreesClassifier\n",
    "  \n",
    "  ensemble: Boolean, default = False\n",
    "  True would enable ensembling of models through Bagging/Boosting method to be defined by 'method'.\n",
    "  \n",
    "  method: String, 'Bagging' or 'Boosting', default = Bagging\n",
    "  method comes into effect only when ensemble = True. Default is set to Bagging.\n",
    "  \n",
    "  fold: integer, default = 10\n",
    "  Number of folds will determine how many folds would be done in the Kfold cross validation.\n",
    "  \n",
    "  round: integer, default = 4\n",
    "  The number indicates the number of decimal places metrics will be rounded to. \n",
    "\n",
    "  verbose: Boolean, default = True\n",
    "  Score grid is not printed when verbose is set to False.\n",
    "  \n",
    "  Returns:\n",
    "  --------\n",
    "  \n",
    "  score grid:   A table containing the scores of the model across the kfolds. \n",
    "  -----------   Scoring metrics used are Accuracy, AUC, Recall, Precision, F1 \n",
    "                and Kappa. Mean and standard deviation of the scores across the \n",
    "                folds is also returned.\n",
    "  \n",
    "  model:        trained model object\n",
    "  -----------\n",
    "\n",
    "  Warnings:\n",
    "  ---------\n",
    "  None\n",
    "  \n",
    "    \"\"\"\n",
    "    \n",
    "    #progress bar\n",
    "    import ipywidgets as ipw\n",
    "    from IPython.display import display, HTML, clear_output\n",
    "    progress = ipw.IntProgress(value=0, min=0, max=fold+3, step=1 , description='Processing: ')\n",
    "    display(progress)\n",
    "    \n",
    "    #defining X_train and y_train called from setup() into variable data_X and data_y to be used in cross validation   \n",
    "    data_X = X_train\n",
    "    data_y = y_train\n",
    "  \n",
    "    #ignore warnings\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore') \n",
    "  \n",
    "    #dependencies\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn import metrics\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    import sys\n",
    "    \n",
    "    progress.value += 1\n",
    "    \n",
    "    #setting cross validation\n",
    "    kf = StratifiedKFold(fold, random_state=seed)\n",
    "\n",
    "    score_auc =np.empty((0,0))\n",
    "    score_acc =np.empty((0,0))\n",
    "    score_recall =np.empty((0,0))\n",
    "    score_precision =np.empty((0,0))\n",
    "    score_f1 =np.empty((0,0))\n",
    "    score_kappa =np.empty((0,0))\n",
    "    avgs_auc =np.empty((0,0))\n",
    "    avgs_acc =np.empty((0,0))\n",
    "    avgs_recall =np.empty((0,0))\n",
    "    avgs_precision =np.empty((0,0))\n",
    "    avgs_f1 =np.empty((0,0))\n",
    "    avgs_kappa =np.empty((0,0))\n",
    "  \n",
    "    #error handling\n",
    "    \n",
    "    if estimator == None:\n",
    "        print(\"Please enter your custom model as on object or choose from model library. If you have previously defined the estimator, the output is generated using the same estimator\") \n",
    "        sys.exit('Exception Handling XXX')\n",
    "\n",
    "    elif estimator == 'lr':\n",
    "\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        model = LogisticRegression(random_state=seed)\n",
    "        full_name = 'Logistic Regression'\n",
    "\n",
    "    elif estimator == 'knn':\n",
    "\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        model = KNeighborsClassifier()\n",
    "        full_name = 'K Nearest Neighbours'\n",
    "\n",
    "    elif estimator == 'nb':\n",
    "\n",
    "        from sklearn.naive_bayes import GaussianNB\n",
    "        model = GaussianNB()\n",
    "        full_name = 'Naive Bayes'\n",
    "\n",
    "    elif estimator == 'dt':\n",
    "\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        model = DecisionTreeClassifier(random_state=seed)\n",
    "        full_name = 'Decision Tree'\n",
    "\n",
    "    elif estimator == 'svm':\n",
    "\n",
    "        from sklearn.linear_model import SGDClassifier\n",
    "        model = SGDClassifier(max_iter=1000, tol=0.001, random_state=seed)\n",
    "        full_name = 'Support Vector Machine'\n",
    "\n",
    "    elif estimator == 'rbfsvm':\n",
    "\n",
    "        from sklearn.svm import SVC\n",
    "        model = SVC(gamma='auto', C=1, probability=True, kernel='rbf', random_state=seed)\n",
    "        full_name = 'RBF SVM'\n",
    "\n",
    "    elif estimator == 'gpc':\n",
    "\n",
    "        from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "        model = GaussianProcessClassifier(random_state=seed)\n",
    "        full_name = 'Gaussian Process Classifier'\n",
    "\n",
    "    elif estimator == 'mlp':\n",
    "\n",
    "        from sklearn.neural_network import MLPClassifier\n",
    "        model = MLPClassifier(max_iter=500, random_state=seed)\n",
    "        full_name = 'Multi Level Perceptron'    \n",
    "\n",
    "    elif estimator == 'ridge':\n",
    "\n",
    "        from sklearn.linear_model import RidgeClassifier\n",
    "        model = RidgeClassifier(random_state=seed)\n",
    "        full_name = 'Ridge Classifier'        \n",
    "\n",
    "    elif estimator == 'rf':\n",
    "\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        model = RandomForestClassifier(n_estimators=10, random_state=seed)\n",
    "        full_name = 'Random Forest Classifier'    \n",
    "\n",
    "    elif estimator == 'qda':\n",
    "\n",
    "        from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "        model = QuadraticDiscriminantAnalysis()\n",
    "        full_name = 'Quadratic Discriminant Analysis' \n",
    "\n",
    "    elif estimator == 'ada':\n",
    "\n",
    "        from sklearn.ensemble import AdaBoostClassifier\n",
    "        model = AdaBoostClassifier(random_state=seed)\n",
    "        full_name = 'AdaBoost Classifier'        \n",
    "\n",
    "    elif estimator == 'gbc':\n",
    "\n",
    "        from sklearn.ensemble import GradientBoostingClassifier    \n",
    "        model = GradientBoostingClassifier(random_state=seed)\n",
    "        full_name = 'Gradient Boosting Classifier'    \n",
    "\n",
    "    elif estimator == 'lda':\n",
    "\n",
    "        from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "        model = LinearDiscriminantAnalysis()\n",
    "        full_name = 'Linear Discriminant Analysis'\n",
    "\n",
    "    elif estimator == 'et':\n",
    "\n",
    "        from sklearn.ensemble import ExtraTreesClassifier \n",
    "        model = ExtraTreesClassifier(random_state=seed)\n",
    "        full_name = 'Extra Trees Classifier'\n",
    "\n",
    "    else:\n",
    "        model = estimator\n",
    "        full_name = str(model).split(\"(\")[0]\n",
    "    \n",
    "    progress.value += 1\n",
    "    \n",
    "    #checking ensemble method\n",
    "\n",
    "    if ensemble and method == 'Bagging':\n",
    "        \n",
    "        from sklearn.ensemble import BaggingClassifier\n",
    "        model = BaggingClassifier(model,bootstrap=True,n_estimators=10, random_state=seed)\n",
    "\n",
    "    elif ensemble and method == 'Boosting':\n",
    "\n",
    "        from sklearn.ensemble import AdaBoostClassifier\n",
    "        model = AdaBoostClassifier(model, random_state=seed)\n",
    "\n",
    "    elif method == 'Boosting':\n",
    "\n",
    "        from sklearn.ensemble import AdaBoostClassifier\n",
    "        model = AdaBoostClassifier(model, random_state=seed)\n",
    "     \n",
    "    for train_i , test_i in kf.split(data_X,data_y):\n",
    "    \n",
    "        Xtrain,Xtest = data_X.iloc[train_i], data_X.iloc[test_i]\n",
    "        ytrain,ytest = data_y.iloc[train_i], data_y.iloc[test_i]\n",
    "        \n",
    "    \n",
    "        if hasattr(model, 'predict_proba'):\n",
    "        \n",
    "            model.fit(Xtrain,ytrain)\n",
    "            pred_prob = model.predict_proba(Xtest)\n",
    "            pred_prob = pred_prob[:,1]\n",
    "            pred_ = model.predict(Xtest)\n",
    "            sca = metrics.accuracy_score(ytest,pred_)\n",
    "            sc = metrics.roc_auc_score(ytest,pred_prob)\n",
    "            recall = metrics.recall_score(ytest,pred_)\n",
    "            precision = metrics.precision_score(ytest,pred_)\n",
    "            kappa = metrics.cohen_kappa_score(ytest,pred_)\n",
    "            f1 = metrics.f1_score(ytest,pred_)\n",
    "            score_acc = np.append(score_acc,sca)\n",
    "            score_auc = np.append(score_auc,sc)\n",
    "            score_recall = np.append(score_recall,recall)\n",
    "            score_precision = np.append(score_precision,precision)\n",
    "            score_f1 =np.append(score_f1,f1)\n",
    "            score_kappa =np.append(score_kappa,kappa)\n",
    "\n",
    "        else:\n",
    "            \n",
    "            model.fit(Xtrain,ytrain)\n",
    "            pred_prob = 0.00\n",
    "            pred_prob = 0.00\n",
    "            pred_ = model.predict(Xtest)\n",
    "            sca = metrics.accuracy_score(ytest,pred_)\n",
    "            sc = 0.00\n",
    "            recall = metrics.recall_score(ytest,pred_)\n",
    "            precision = metrics.precision_score(ytest,pred_) #change pred_prob to pred_\n",
    "            kappa = metrics.cohen_kappa_score(ytest,pred_)\n",
    "            f1 = metrics.f1_score(ytest,pred_)\n",
    "            score_acc = np.append(score_acc,sca)\n",
    "            score_auc = np.append(score_auc,sc)\n",
    "            score_recall = np.append(score_recall,recall)\n",
    "            score_precision = np.append(score_precision,precision)\n",
    "            score_f1 =np.append(score_f1,f1)\n",
    "            score_kappa =np.append(score_kappa,kappa) \n",
    "       \n",
    "        progress.value += 1\n",
    "        \n",
    "    mean_acc=np.mean(score_acc)\n",
    "    mean_auc=np.mean(score_auc)\n",
    "    mean_recall=np.mean(score_recall)\n",
    "    mean_precision=np.mean(score_precision)\n",
    "    mean_f1=np.mean(score_f1)\n",
    "    mean_kappa=np.mean(score_kappa)\n",
    "    std_acc=np.std(score_acc)\n",
    "    std_auc=np.std(score_auc)\n",
    "    std_recall=np.std(score_recall)\n",
    "    std_precision=np.std(score_precision)\n",
    "    std_f1=np.std(score_f1)\n",
    "    std_kappa=np.std(score_kappa)\n",
    "    \n",
    "    avgs_acc = np.append(avgs_acc, mean_acc)\n",
    "    avgs_acc = np.append(avgs_acc, std_acc) \n",
    "    avgs_auc = np.append(avgs_auc, mean_auc)\n",
    "    avgs_auc = np.append(avgs_auc, std_auc)\n",
    "    avgs_recall = np.append(avgs_recall, mean_recall)\n",
    "    avgs_recall = np.append(avgs_recall, std_recall)\n",
    "    avgs_precision = np.append(avgs_precision, mean_precision)\n",
    "    avgs_precision = np.append(avgs_precision, std_precision)\n",
    "    avgs_f1 = np.append(avgs_f1, mean_f1)\n",
    "    avgs_f1 = np.append(avgs_f1, std_f1)\n",
    "    avgs_kappa = np.append(avgs_kappa, mean_kappa)\n",
    "    avgs_kappa = np.append(avgs_kappa, std_kappa)\n",
    "    \n",
    "    progress.value += 1\n",
    "    \n",
    "    model_results = pd.DataFrame({'Accuracy': score_acc, 'AUC': score_auc, 'Recall' : score_recall, 'Prec.' : score_precision , \n",
    "                     'F1' : score_f1, 'Kappa' : score_kappa})\n",
    "    model_results_unpivot = pd.melt(model_results,value_vars=['Accuracy', 'AUC', 'Recall', 'Prec.', 'F1', 'Kappa'])\n",
    "    model_results_unpivot.columns = ['Metric', 'Measure']\n",
    "    model_avgs = pd.DataFrame({'Accuracy': avgs_acc, 'AUC': avgs_auc, 'Recall' : avgs_recall, 'Prec.' : avgs_precision , \n",
    "                     'F1' : avgs_f1, 'Kappa' : avgs_kappa},index=['Mean', 'SD'])\n",
    "\n",
    "    model_results = model_results.append(model_avgs)\n",
    "    model_results = model_results.round(round)  \n",
    " \n",
    "    if verbose:\n",
    "        clear_output()\n",
    "        display(HTML(model_results.to_html()))\n",
    "        return model\n",
    "    else:\n",
    "        clear_output()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_model(estimator,\n",
    "                   method = 'Bagging', \n",
    "                   fold = 10,\n",
    "                   n_estimators = 10,\n",
    "                   round = 4,  \n",
    "                   verbose = True):\n",
    "    \"\"\"\n",
    "    \n",
    "  Description:\n",
    "  ------------\n",
    "  This function ensemble the trained base estimator using method defined in 'method' \n",
    "  param. The output prints the score grid that shows Accuracy, AUC, Recall, Precision, \n",
    "  F1 and Kappa by fold (default = 10). \n",
    "  \n",
    "  Function also returns a trained model object that can be used for further \n",
    "  processing in pycaret or can be used to call any method available in sklearn. \n",
    "  \n",
    "  model must be created using create_model() or tune_model() in pycaret or using any\n",
    "  other package that returns sklearn object.\n",
    "  \n",
    "    Example:\n",
    "    --------\n",
    "    \n",
    "    ensembled_lr = ensemble_model(lr)\n",
    "    \n",
    "    This will return ensembled Logistic Regression.\n",
    "    variable 'lr' is created used lr = create_model('lr')\n",
    "    Using ensemble = True in create_model() is equivalent to using ensemble_model(lr)\n",
    "    \n",
    "  \n",
    "  Parameters\n",
    "  ----------\n",
    "  \n",
    "  estimator : object, default = None\n",
    "     \n",
    "  method: String, default = 'Bagging' \n",
    "  Bagging implementation is based on sklearn.ensemble.BaggingClassifier\n",
    "  Boosting implementation is based on sklearn.ensemble.AdaBoostClassifier\n",
    "  \n",
    "  fold: integer, default = 10\n",
    "  Number of folds will determine how many folds would be done in the Kfold cross validation.\n",
    "  \n",
    "  round: integer, default = 4\n",
    "  The number of decimal places metrics will be rounded to. \n",
    "\n",
    "  n_estimators: integer, default = 10\n",
    "  The number of base estimators in the ensemble.\n",
    "  In case of perfect fit, the learning procedure is stopped early.\n",
    "  \n",
    "  verbose: Boolean, default = True\n",
    "  Score grid is not printed when verbose is set to False.\n",
    "  \n",
    "  \n",
    "  Returns:\n",
    "  --------\n",
    "  \n",
    "  score grid:   A table containing the scores of the model across the kfolds. \n",
    "  -----------   Scoring metrics used are Accuracy, AUC, Recall, Precision, F1 \n",
    "                and Kappa. Mean and standard deviation of the scores across the \n",
    "                folds is also returned.\n",
    "  \n",
    "  model:        trained ensembled model object\n",
    "  -----------\n",
    "  \n",
    "  Warnings:\n",
    "  ---------\n",
    "  None\n",
    "      \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #progress bar\n",
    "    import ipywidgets as ipw\n",
    "    from IPython.display import display, HTML, clear_output\n",
    "    progress = ipw.IntProgress(value=0, min=0, max=fold+3, step=1 , description='Processing: ')\n",
    "    display(progress)\n",
    "    \n",
    "    #dependencies\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn import metrics\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    import sys    \n",
    "    \n",
    "    #defining X_train and y_train    \n",
    "    data_X = X_train\n",
    "    data_y = y_train\n",
    "  \n",
    "    #ignore co-linearity warnings for qda and lda \n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore') \n",
    "    \n",
    "    progress.value += 1\n",
    "    \n",
    "    #defining estimator as model\n",
    "    model = estimator\n",
    "     \n",
    "    if method == 'Bagging':\n",
    "        from sklearn.ensemble import BaggingClassifier\n",
    "        model = BaggingClassifier(model,bootstrap=True,n_estimators=n_estimators, random_state=seed)\n",
    "        \n",
    "    else:\n",
    "        from sklearn.ensemble import AdaBoostClassifier\n",
    "        model = AdaBoostClassifier(model, random_state=seed)\n",
    "    \n",
    "    progress.value += 1\n",
    "    \n",
    "    kf = StratifiedKFold(fold, random_state=seed)\n",
    "    \n",
    "    score_auc =np.empty((0,0))\n",
    "    score_acc =np.empty((0,0))\n",
    "    score_recall =np.empty((0,0))\n",
    "    score_precision =np.empty((0,0))\n",
    "    score_f1 =np.empty((0,0))\n",
    "    score_kappa =np.empty((0,0))\n",
    "    avgs_auc =np.empty((0,0))\n",
    "    avgs_acc =np.empty((0,0))\n",
    "    avgs_recall =np.empty((0,0))\n",
    "    avgs_precision =np.empty((0,0))\n",
    "    avgs_f1 =np.empty((0,0))\n",
    "    avgs_kappa =np.empty((0,0))\n",
    "    \n",
    "    for train_i , test_i in kf.split(data_X,data_y):\n",
    "        \n",
    "        Xtrain,Xtest = data_X.iloc[train_i], data_X.iloc[test_i]\n",
    "        ytrain,ytest = data_y.iloc[train_i], data_y.iloc[test_i]\n",
    "    \n",
    "        if hasattr(model, 'predict_proba'):\n",
    "        \n",
    "            model.fit(Xtrain,ytrain)\n",
    "            pred_prob = model.predict_proba(Xtest)\n",
    "            pred_prob = pred_prob[:,1]\n",
    "            pred_ = model.predict(Xtest)\n",
    "            sca = metrics.accuracy_score(ytest,pred_)\n",
    "            sc = metrics.roc_auc_score(ytest,pred_prob)\n",
    "            recall = metrics.recall_score(ytest,pred_)\n",
    "            precision = metrics.precision_score(ytest,pred_)\n",
    "            kappa = metrics.cohen_kappa_score(ytest,pred_)\n",
    "            f1 = metrics.f1_score(ytest,pred_)\n",
    "            score_acc = np.append(score_acc,sca)\n",
    "            score_auc = np.append(score_auc,sc)\n",
    "            score_recall = np.append(score_recall,recall)\n",
    "            score_precision = np.append(score_precision,precision)\n",
    "            score_f1 =np.append(score_f1,f1)\n",
    "            score_kappa =np.append(score_kappa,kappa)\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            model.fit(Xtrain,ytrain)\n",
    "            pred_prob = 0.00\n",
    "            pred_prob = 0.00\n",
    "            pred_ = model.predict(Xtest)\n",
    "            sca = metrics.accuracy_score(ytest,pred_)\n",
    "            sc = 0.00\n",
    "            recall = metrics.recall_score(ytest,pred_)\n",
    "            precision = metrics.precision_score(ytest,pred_) #change pred_prob to pred_\n",
    "            kappa = metrics.cohen_kappa_score(ytest,pred_)\n",
    "            f1 = metrics.f1_score(ytest,pred_)\n",
    "            score_acc = np.append(score_acc,sca)\n",
    "            score_auc = np.append(score_auc,sc)\n",
    "            score_recall = np.append(score_recall,recall)\n",
    "            score_precision = np.append(score_precision,precision)\n",
    "            score_f1 =np.append(score_f1,f1)\n",
    "            score_kappa =np.append(score_kappa,kappa) \n",
    "        \n",
    "        progress.value += 1\n",
    "        \n",
    "    mean_acc=np.mean(score_acc)\n",
    "    mean_auc=np.mean(score_auc)\n",
    "    mean_recall=np.mean(score_recall)\n",
    "    mean_precision=np.mean(score_precision)\n",
    "    mean_f1=np.mean(score_f1)\n",
    "    mean_kappa=np.mean(score_kappa)\n",
    "    std_acc=np.std(score_acc)\n",
    "    std_auc=np.std(score_auc)\n",
    "    std_recall=np.std(score_recall)\n",
    "    std_precision=np.std(score_precision)\n",
    "    std_f1=np.std(score_f1)\n",
    "    std_kappa=np.std(score_kappa)\n",
    "\n",
    "    avgs_acc = np.append(avgs_acc, mean_acc)\n",
    "    avgs_acc = np.append(avgs_acc, std_acc) \n",
    "    avgs_auc = np.append(avgs_auc, mean_auc)\n",
    "    avgs_auc = np.append(avgs_auc, std_auc)\n",
    "    avgs_recall = np.append(avgs_recall, mean_recall)\n",
    "    avgs_recall = np.append(avgs_recall, std_recall)\n",
    "    avgs_precision = np.append(avgs_precision, mean_precision)\n",
    "    avgs_precision = np.append(avgs_precision, std_precision)\n",
    "    avgs_f1 = np.append(avgs_f1, mean_f1)\n",
    "    avgs_f1 = np.append(avgs_f1, std_f1)\n",
    "    avgs_kappa = np.append(avgs_kappa, mean_kappa)\n",
    "    avgs_kappa = np.append(avgs_kappa, std_kappa)\n",
    "\n",
    "    model_results = pd.DataFrame({'Accuracy': score_acc, 'AUC': score_auc, 'Recall' : score_recall, 'Prec.' : score_precision , \n",
    "                     'F1' : score_f1, 'Kappa' : score_kappa})\n",
    "    model_results_unpivot = pd.melt(model_results,value_vars=['Accuracy', 'AUC', 'Recall', 'Prec.', 'F1', 'Kappa'])\n",
    "    model_results_unpivot.columns = ['Metric', 'Measure']\n",
    "    model_avgs = pd.DataFrame({'Accuracy': avgs_acc, 'AUC': avgs_auc, 'Recall' : avgs_recall, 'Prec.' : avgs_precision , \n",
    "                     'F1' : avgs_f1, 'Kappa' : avgs_kappa},index=['Mean', 'SD'])\n",
    "\n",
    "    model_results = model_results.append(model_avgs)\n",
    "    model_results = model_results.round(round)  \n",
    "    \n",
    "    progress.value += 1\n",
    "    \n",
    "    model = model\n",
    "    \n",
    "    \n",
    "    if verbose:\n",
    "        clear_output()\n",
    "        display(model_results)\n",
    "        return model\n",
    "    else:\n",
    "        clear_output()\n",
    "        return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model(estimator, \n",
    "               plot = 'auc',  \n",
    "               manifold='tsne', \n",
    "               features=5): \n",
    "    \n",
    "    \"\"\"\n",
    "   \n",
    "  Description:\n",
    "  ------------\n",
    "  This function takes a trained model object and returns the plot on test set.\n",
    "  Model may get re-trained in the process, as required in certain cases.\n",
    "  See list of plots supported below. \n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    \n",
    "    plot_model(lr)\n",
    "    \n",
    "    This will return AUC plot of trained Logistic Regression.\n",
    "    variable 'lr' is created used lr = create_model('lr')\n",
    "\n",
    "  \n",
    "  Parameters\n",
    "  ----------\n",
    "  \n",
    "  estimator : object, default=none\n",
    "  \n",
    "  A trained model object should be passed as an estimator. \n",
    "  Model must be created using create_model() or tune_model() in pycaret or using any\n",
    "  other package that returns sklearn object.\n",
    "  \n",
    "  plot : string, default=auc\n",
    "  Enter abbreviation of type of plot. The current list of plots supported are:\n",
    "  \n",
    "  Name                        Abbreviated String     Original Implementation \n",
    "  ---------                   ------------------     -----------------------\n",
    "  Area Under the Curve         'auc'                 .. / rocauc.html\n",
    "  Discrimination Threshold     'threshold'           .. / threshold.html\n",
    "  Precision Recall Curve       'pr'                  .. / prcurve.html\n",
    "  Confusion Matrix             'confusion_matrix'    .. / confusion_matrix.html\n",
    "  Class Prediction Error       'error'               .. / class_prediction_error.html\n",
    "  Classification Report        'class_report'        .. / classification_report.html\n",
    "  Decision Boundary            'boundary'            .. / boundaries.html\n",
    "  Recursive Feat. Selection    'rfe'                 .. / rfecv.html\n",
    "  Learning Curve               'learning'            .. / learning_curve.html\n",
    "  Manifold Learning            'manifold'            .. / manifold.html\n",
    "  Calibration Curve            'calibration'         .. / calibration_curve.html\n",
    "  Validation Curve             'vc'                  .. / validation_curve.html\n",
    "  Dimension Learning           'dimension'           .. / radviz.html\n",
    "  Feature Importance           'feature'             ..... N/A .....\n",
    "  \n",
    "  ** https://www.scikit-yb.org/en/latest/api/classifier/<reference>\n",
    "  \n",
    "  manifold: string, default = 'tsne'\n",
    "  This parameter is only needed for 'manifold' plot. \n",
    "  Other options for this parameter are:\n",
    "  'lle', 'ltsa', 'hessian', 'modified', 'isomap', 'mds' and 'spectral'\n",
    "\n",
    "  feature: integer, default = 5\n",
    "  This parameter is only needed for 'dimension' plot. It is used to reduce the \n",
    "  dimensionality of feature set.\n",
    "\n",
    "  Returns:\n",
    "  --------\n",
    "  \n",
    "  Visual Plot:  Prints the visual plot. Returns an object of type None.  \n",
    "  \n",
    "  Warnings:\n",
    "  ---------\n",
    "  None\n",
    "    \n",
    "    \"\"\"  \n",
    "    \n",
    "    #progress bar\n",
    "    import ipywidgets as ipw\n",
    "    from IPython.display import display, HTML, clear_output\n",
    "    progress = ipw.IntProgress(value=0, min=0, max=5, step=1 , description='Processing: ')\n",
    "    display(progress)\n",
    "    \n",
    "    #dependencies\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import sys\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    progress.value += 1\n",
    "    \n",
    "    model = estimator\n",
    "    \n",
    "    progress.value += 1\n",
    "    \n",
    "    if plot == 'auc':\n",
    "        \n",
    "        \n",
    "        from yellowbrick.classifier import ROCAUC\n",
    "        progress.value += 1\n",
    "        visualizer = ROCAUC(model)\n",
    "        visualizer.fit(X_train, y_train)\n",
    "        progress.value += 1\n",
    "        visualizer.score(X_test, y_test)\n",
    "        progress.value += 1\n",
    "        clear_output()\n",
    "        visualizer.poof()\n",
    "        \n",
    "    elif plot == 'threshold':\n",
    "        \n",
    "        from yellowbrick.classifier import DiscriminationThreshold\n",
    "        progress.value += 1\n",
    "        visualizer = DiscriminationThreshold(model, random_state=seed)\n",
    "        visualizer.fit(X_train, y_train)\n",
    "        progress.value += 1\n",
    "        visualizer.score(X_test, y_test)\n",
    "        progress.value += 1\n",
    "        clear_output()\n",
    "        visualizer.poof()\n",
    "    \n",
    "    elif plot == 'pr':\n",
    "        \n",
    "        from yellowbrick.classifier import PrecisionRecallCurve\n",
    "        progress.value += 1\n",
    "        visualizer = PrecisionRecallCurve(model, random_state=seed)\n",
    "        visualizer.fit(X_train, y_train)\n",
    "        progress.value += 1\n",
    "        visualizer.score(X_test, y_test)\n",
    "        progress.value += 1\n",
    "        clear_output()\n",
    "        visualizer.poof()\n",
    "\n",
    "    elif plot == 'confusion_matrix':\n",
    "        \n",
    "        from yellowbrick.classifier import ConfusionMatrix\n",
    "        progress.value += 1\n",
    "        visualizer = ConfusionMatrix(model, random_state=seed, fontsize=15, cmap=\"Greens\")\n",
    "        visualizer.fit(X_train, y_train)\n",
    "        progress.value += 1\n",
    "        visualizer.score(X_test, y_test)\n",
    "        progress.value += 1\n",
    "        clear_output()\n",
    "        visualizer.poof()\n",
    "    \n",
    "    elif plot == 'error':\n",
    "        \n",
    "        from yellowbrick.classifier import ClassPredictionError\n",
    "        progress.value += 1\n",
    "        visualizer = ClassPredictionError(model, random_state=seed)\n",
    "        visualizer.fit(X_train, y_train)\n",
    "        progress.value += 1\n",
    "        visualizer.score(X_test, y_test)\n",
    "        progress.value += 1\n",
    "        clear_output()\n",
    "        visualizer.poof()\n",
    "\n",
    "    elif plot == 'class_report':\n",
    "        \n",
    "        from yellowbrick.classifier import ClassificationReport\n",
    "        progress.value += 1\n",
    "        visualizer = ClassificationReport(model, random_state=seed, support=True)\n",
    "        visualizer.fit(X_train, y_train)\n",
    "        progress.value += 1\n",
    "        visualizer.score(X_test, y_test)\n",
    "        progress.value += 1\n",
    "        clear_output()\n",
    "        visualizer.poof()\n",
    "        \n",
    "    elif plot == 'boundary':\n",
    "        \n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        from sklearn.decomposition import PCA\n",
    "        from yellowbrick.contrib.classifier import DecisionViz        \n",
    "        \n",
    "        progress.value += 1\n",
    "        \n",
    "        X_train_transformed = X_train.select_dtypes(include='float64')\n",
    "        X_test_transformed = X_test.select_dtypes(include='float64')\n",
    "        X_train_transformed = StandardScaler().fit_transform(X_train_transformed)\n",
    "        X_test_transformed = StandardScaler().fit_transform(X_test_transformed)\n",
    "        pca = PCA(n_components=2, random_state = seed)\n",
    "        X_train_transformed = pca.fit_transform(X_train_transformed)\n",
    "        X_test_transformed = pca.fit_transform(X_test_transformed)\n",
    "        \n",
    "        progress.value += 1\n",
    "        \n",
    "        y_train_transformed = np.array(y_train)\n",
    "        y_test_transformed = np.array(y_test)\n",
    "        \n",
    "        model_transformed = model\n",
    "        \n",
    "        viz = DecisionViz(model_transformed)\n",
    "        viz.fit(X_train_transformed, y_train_transformed, features=['Feature One', 'Feature Two'], classes=['A', 'B'])\n",
    "        viz.draw(X_test_transformed, y_test_transformed)\n",
    "        progress.value += 1\n",
    "        clear_output()\n",
    "        viz.poof()\n",
    "        \n",
    "    elif plot == 'rfe':\n",
    "        \n",
    "        from yellowbrick.model_selection import RFECV \n",
    "        progress.value += 1\n",
    "        visualizer = RFECV(model, cv=10)\n",
    "        progress.value += 1\n",
    "        visualizer.fit(X_train, y_train)\n",
    "        progress.value += 1\n",
    "        clear_output()\n",
    "        visualizer.poof()\n",
    "           \n",
    "    elif plot == 'learning':\n",
    "        \n",
    "        from yellowbrick.model_selection import LearningCurve\n",
    "        progress.value += 1\n",
    "        sizes = np.linspace(0.3, 1.0, 10)  \n",
    "        visualizer = LearningCurve(model, cv=10, scoring='f1_weighted', train_sizes=sizes, n_jobs=1, random_state=seed)\n",
    "        progress.value += 1\n",
    "        visualizer.fit(X_train, y_train)\n",
    "        progress.value += 1\n",
    "        clear_output()\n",
    "        visualizer.poof()\n",
    "        \n",
    "    elif plot == 'manifold':\n",
    "        \n",
    "        from yellowbrick.features import Manifold\n",
    "        progress.value += 1\n",
    "        X_train_transformed = X_train.select_dtypes(include='float64') \n",
    "        visualizer = Manifold(manifold=manifold, random_state = seed)\n",
    "        progress.value += 1\n",
    "        visualizer.fit_transform(X_train_transformed, y_train)\n",
    "        progress.value += 1\n",
    "        clear_output()\n",
    "        visualizer.poof()       \n",
    "        \n",
    "    elif plot == 'calibration':      \n",
    "                \n",
    "        from sklearn.calibration import calibration_curve\n",
    "        \n",
    "        model_name = str(model).split(\"(\")[0]\n",
    "        \n",
    "        plt.figure(figsize=(7, 6))\n",
    "        ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "\n",
    "        ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "        progress.value += 1\n",
    "        prob_pos = model.predict_proba(X_test)[:, 1]\n",
    "        prob_pos = (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())\n",
    "        fraction_of_positives, mean_predicted_value = calibration_curve(y_test, prob_pos, n_bins=10)\n",
    "        progress.value += 1\n",
    "        ax1.plot(mean_predicted_value, fraction_of_positives, \"s-\",label=\"%s\" % (model_name, ))\n",
    "    \n",
    "        ax1.set_ylabel(\"Fraction of positives\")\n",
    "        ax1.set_ylim([0, 1])\n",
    "        ax1.set_xlim([0, 1])\n",
    "        ax1.legend(loc=\"lower right\")\n",
    "        ax1.set_title('Calibration plots  (reliability curve)')\n",
    "        ax1.set_facecolor('white')\n",
    "        ax1.grid(b=True, color='grey', linewidth=0.5, linestyle = '-')\n",
    "        plt.tight_layout()\n",
    "        progress.value += 1\n",
    "        clear_output()\n",
    "        plt.show() \n",
    "        \n",
    "    elif plot == 'vc':\n",
    "    \n",
    "        if hasattr(model, 'max_depth'):\n",
    "            param_name='max_depth'\n",
    "        else:\n",
    "            param_name='xxx'\n",
    "        progress.value += 1\n",
    "        \n",
    "        from yellowbrick.model_selection import ValidationCurve\n",
    "        viz = ValidationCurve(model, param_name=param_name, param_range=np.arange(1,11), scoring='f1_weighted',cv=10, \n",
    "                              random_state=seed)\n",
    "        progress.value += 1\n",
    "        viz.fit(X_train, y_train)\n",
    "        progress.value += 1\n",
    "        clear_output()\n",
    "        viz.poof()\n",
    "        \n",
    "    elif plot == 'dimension':\n",
    "    \n",
    "        from yellowbrick.features import RadViz\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        from sklearn.decomposition import PCA\n",
    "        progress.value += 1\n",
    "        X_train_transformed = X_train.select_dtypes(include='float64') \n",
    "        X_train_transformed = StandardScaler().fit_transform(X_train_transformed)\n",
    "        y_train_transformed = np.array(y_train)\n",
    "\n",
    "        pca = PCA(n_components=features, random_state=seed)\n",
    "        X_train_transformed = pca.fit_transform(X_train_transformed)\n",
    "        progress.value += 1\n",
    "        classes = [\"1\", \"0\"]\n",
    "        visualizer = RadViz(classes=classes, alpha=0.25)\n",
    "        visualizer.fit(X_train_transformed, y_train_transformed)     \n",
    "        visualizer.transform(X_train_transformed)\n",
    "        progress.value += 1\n",
    "        clear_output()\n",
    "        visualizer.poof()\n",
    "        \n",
    "    elif plot == 'feature':\n",
    "        variables = abs(model.coef_[0])\n",
    "        col_names = np.array(X_train.columns)\n",
    "        coef_df = pd.DataFrame({'Variable': X_train.columns, 'Value': variables})\n",
    "        sorted_df = coef_df.sort_values(by='Value')\n",
    "        my_range=range(1,len(sorted_df.index)+1)\n",
    "        progress.value += 1\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.hlines(y=my_range, xmin=0, xmax=sorted_df['Value'], color='skyblue')\n",
    "        plt.plot(sorted_df['Value'], my_range, \"o\")\n",
    "        progress.value += 1\n",
    "        plt.yticks(my_range, sorted_df['Variable'])\n",
    "        plt.title(\"Feature Importance Plot\")\n",
    "        progress.value += 1\n",
    "        clear_output()\n",
    "        plt.xlabel('Variable Importance')\n",
    "        plt.ylabel('Features') \n",
    "        var_imp = sorted_df.reset_index(drop=True)\n",
    "        var_imp_array = np.array(var_imp['Variable'])\n",
    "        var_imp_array_top_n = var_imp_array[0:len(var_imp_array)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(model_library = 'All', \n",
    "                   fold = 10, \n",
    "                   round = 4, \n",
    "                   sort = 'Accuracy', \n",
    "                   blacklist = None):\n",
    "    \n",
    "    \"\"\"\n",
    "   \n",
    "  Description:\n",
    "  ------------\n",
    "  This function creates multiple model and scores it using Stratified Cross Validation. \n",
    "  The output prints the score grid that shows Accuracy, AUC, Recall, Precision, \n",
    "  F1 and Kappa by fold (default = 10) of all the available model in model library. \n",
    "  \n",
    "  List of models in Model Library\n",
    "  \n",
    "  Estimator                   Abbreviated String     sklearn Implementation \n",
    "  ---------                   ------------------     -----------------------\n",
    "  Logistic Regression         'lr'                   linear_model.LogisticRegression\n",
    "  K Nearest Neighbour         'knn'                  neighbors.KNeighborsClassifier\n",
    "  Naives Bayes                'nb'                   naive_bayes.GaussianNB\n",
    "  Decision Tree               'dt'                   tree.DecisionTreeClassifier\n",
    "  SVM (Linear)                'svm'                  linear_model.SGDClassifier\n",
    "  SVM (RBF)                   'rbfsvm'               svm.SVC\n",
    "  Gaussian Process            'gpc'                  gaussian_process.GPC\n",
    "  Multi Level Perceptron      'mlp'                  neural_network.MLPClassifier\n",
    "  Ridge Classifier            'ridge'                linear_model.RidgeClassifier\n",
    "  Random Forest               'rf'                   ensemble.RandomForestClassifier\n",
    "  Quadratic Disc. Analysis    'qda'                  discriminant_analysis.QDA \n",
    "  AdaBoost                    'ada'                  ensemble.AdaBoostClassifier\n",
    "  Gradient Boosting           'gbc'                  ensemble.GradientBoostingClassifier\n",
    "  Linear Disc. Analysis       'lda'                  discriminant_analysis.LDA \n",
    "  Extra Trees Classifier      'et'                   ensemble.ExtraTreesClassifier\n",
    "  \n",
    "    Example:\n",
    "    --------\n",
    "    \n",
    "    compare_models() \n",
    "    \n",
    "    This will return score grid of all the models. \n",
    "    ** all other parameters (see below) for compare_models are optional.\n",
    "    \n",
    "      Alternate use of compare_models() could be:\n",
    "    \n",
    "      compare_models( [lr, rf] )\n",
    "      where lr and rf variable is created used create_model()\n",
    "      \n",
    "      If used this way, the function will return averaged result\n",
    "      comparison of lr and rf object instead of all models.\n",
    " \n",
    "  Parameters\n",
    "  ----------\n",
    "  \n",
    "  model_library : string or object, default = 'All'\n",
    "  ** Only 'All' can be passed as string. \n",
    "  \n",
    "  fold: integer, default = 10\n",
    "  Number of folds will determine how many folds would be done in the Kfold cross validation.\n",
    "  \n",
    "  round: integer, default = 4\n",
    "  The number of decimal places metrics will be rounded to.\n",
    "\n",
    "  sort: string, default = 'Accuracy'\n",
    "  The scoring measure specified is used for sorting the models based on their \n",
    "  performance score on the specified scoring measure. \n",
    "  Other options are 'AUC', 'Recall', 'Prec.', 'F1' and 'Kappa'\n",
    "\n",
    "  blacklist: string, default = None\n",
    "  In order to omit certain models from the comparison, the abbreviation string \n",
    "  of such models (see above list) can be passed as  list of strings. This is \n",
    "  normally done to be more efficient with time. By default, None is chosen, \n",
    "  which means no models are black listed.\n",
    "  \n",
    "    Example\n",
    "    -------\n",
    "    compare_models( blacklist = [ 'rbfsvm', 'mlp' ] ) \n",
    "    \n",
    "    This will return comparison of all models except \n",
    "    Support Vector Machine (RBF) and Multi Level Perceptron.\n",
    "  \n",
    "  Returns:\n",
    "  --------\n",
    "  \n",
    "  score grid:   A table containing the scores of the model across the kfolds. \n",
    "  -----------   Scoring metrics used are Accuracy, AUC, Recall, Precision, F1 \n",
    "                and Kappa. Mean and standard deviation of the scores across the \n",
    "                folds is also returned.\n",
    "\n",
    "  Warnings:\n",
    "  ---------\n",
    "  compare_all() though attractive, might be time consuming with large datasets\n",
    "  and users might want to limit the models they chose to compare by either\n",
    "  blacklisting certain models or only passing certain models as object \n",
    "  in model_library parameter.\n",
    "  \n",
    "    \n",
    "    \"\"\"\n",
    "    #progress bar\n",
    "    import ipywidgets as ipw\n",
    "    from IPython.display import display, HTML, clear_output\n",
    "    progress = ipw.IntProgress(value=0, min=0, max=(fold*15)+5, step=1 , description='Processing: ')\n",
    "    display(progress)\n",
    "    \n",
    "    #ignore warnings\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore') \n",
    "\n",
    "    #defining X_train and y_train\n",
    "    data_X = X_train\n",
    "    data_y=y_train\n",
    "\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.gaussian_process.kernels import RBF\n",
    "    from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn import preprocessing as pre\n",
    "    from sklearn.pipeline import Pipeline as pipe\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import RidgeClassifier\n",
    "    from sklearn.linear_model import Lasso\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.linear_model import SGDClassifier\n",
    "    from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "    from sklearn import metrics\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import ExtraTreesClassifier\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    \n",
    "    progress.value += 1\n",
    "    \n",
    "    from scipy import stats\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.model_selection import cross_val_predict\n",
    "    from sklearn.model_selection import cross_validate\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.ensemble import BaggingClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.metrics import cohen_kappa_score\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import pandas_profiling as pd_p\n",
    "    import seaborn as sns\n",
    "    import random\n",
    "    import pandas.io.formats.style\n",
    "    \n",
    "    progress.value += 1\n",
    "    \n",
    "    lr = LogisticRegression(random_state=seed)\n",
    "    knn = KNeighborsClassifier()\n",
    "    nb = GaussianNB()\n",
    "    dt = DecisionTreeClassifier(random_state=seed)\n",
    "    svm = SGDClassifier(max_iter=1000, tol=0.001, random_state=seed)\n",
    "    rbfsvm = SVC(gamma='auto', C=1, probability=True, kernel='rbf', random_state=seed)\n",
    "    gpc = GaussianProcessClassifier(random_state=seed)\n",
    "    mlp = MLPClassifier(max_iter=500, random_state=seed)\n",
    "    ridge = RidgeClassifier(random_state=seed)\n",
    "    rf = RandomForestClassifier(n_estimators=10, random_state=seed)\n",
    "    qda = QuadraticDiscriminantAnalysis()\n",
    "    ada = AdaBoostClassifier(random_state=seed)\n",
    "    gbc = GradientBoostingClassifier(random_state=seed)\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    et = ExtraTreesClassifier(random_state=seed)\n",
    "    \n",
    "    progress.value += 1\n",
    "    \n",
    "    #blacklist models\n",
    "\n",
    "    if model_library != 'All':\n",
    "        \n",
    "        model_library = model_library\n",
    "    \n",
    "        model_names = []\n",
    "    \n",
    "        for names in model_library:\n",
    "        \n",
    "            model_names = np.append(model_names, str(names).split(\"(\")[0])\n",
    "        \n",
    "            import re \n",
    "        \n",
    "            def putSpace(input):\n",
    "                words = re.findall('[A-Z][a-z]*', input)\n",
    "                words = ' '.join(words)\n",
    "                return words  \n",
    "\n",
    "            model_names_modified = []\n",
    "            \n",
    "            for i in model_names:\n",
    "                model_names_modified.append(putSpace(i))\n",
    "\n",
    "            model_names_modified = []\n",
    "            \n",
    "            for i in model_names:\n",
    "                model_names_modified.append(putSpace(i))\n",
    "\n",
    "            model_names = model_names_modified\n",
    "\n",
    "            model_names_final = []\n",
    "            \n",
    "            for j in model_names:\n",
    "                \n",
    "                if j == 'Gaussian N B':\n",
    "                    model_names_final.append('Naive Bayes')\n",
    "                elif j == 'M L P Classifier':\n",
    "                    model_names_final.append('MLP Classifier')\n",
    "                elif j == 'S G D Classifier':\n",
    "                    model_names_final.append('SVM - Linear Kernel')\n",
    "                elif j == 'S V C':\n",
    "                    model_names_final.append('SVM - Radial Kernel')\n",
    "                else: \n",
    "                    model_names_final.append(j)\n",
    "\n",
    "                model_names = model_names_final    \n",
    "\n",
    "    else:\n",
    "        \n",
    "        if blacklist == None:\n",
    "        \n",
    "            model_library = [lr, knn, nb, dt, svm, rbfsvm, gpc, mlp, ridge, rf, qda, ada, gbc, lda, et]\n",
    "\n",
    "            model_names = []\n",
    "\n",
    "            for names in model_library:\n",
    "                model_names = np.append(model_names, str(names).split(\"(\")[0])\n",
    "\n",
    "            import re \n",
    "\n",
    "            def putSpace(input):\n",
    "                words = re.findall('[A-Z][a-z]*', input)\n",
    "                words = ' '.join(words)\n",
    "                return words  \n",
    "\n",
    "            model_names_modified = []\n",
    "            for i in model_names:\n",
    "                model_names_modified.append(putSpace(i))\n",
    "\n",
    "            model_names = model_names_modified\n",
    "\n",
    "            model_names_final = []\n",
    "            for j in model_names:\n",
    "                if j == 'Gaussian N B':\n",
    "                    model_names_final.append('Naive Bayes')\n",
    "                elif j == 'M L P Classifier':\n",
    "                    model_names_final.append('MLP Classifier')\n",
    "                elif j == 'S G D Classifier':\n",
    "                    model_names_final.append('SVM - Linear Kernel')\n",
    "                elif j == 'S V C':\n",
    "                    model_names_final.append('SVM - Radial Kernel')\n",
    "                else: \n",
    "                    model_names_final.append(j)\n",
    "\n",
    "            model_names = model_names_final\n",
    "\n",
    "        else:\n",
    "        \n",
    "            model_library_values = ['lr', 'knn', 'nb', 'dt', 'svm', 'rbfsvm', 'gpc', 'mlp', 'ridge', 'rf', 'qda', \n",
    "                        'ada', 'gbc', 'lda', 'et']\n",
    "\n",
    "            location = []\n",
    "\n",
    "            for item in blacklist:\n",
    "                location.append(model_library_values.index(item))\n",
    "\n",
    "            model_library = [lr, knn, nb, dt, svm, rbfsvm, gpc, mlp, ridge, rf, qda, ada, gbc, lda, et]\n",
    "\n",
    "            for i in location:\n",
    "                del model_library[i]\n",
    "\n",
    "            model_names = []\n",
    "\n",
    "            for names in model_library:\n",
    "                model_names = np.append(model_names, str(names).split(\"(\")[0])\n",
    "\n",
    "            import re\n",
    "\n",
    "            def putSpace(input):\n",
    "                words = re.findall('[A-Z][a-z]*', input)\n",
    "                words = ' '.join(words)\n",
    "                return words  \n",
    "\n",
    "            model_names_modified = []\n",
    "            for i in model_names:\n",
    "                model_names_modified.append(putSpace(i))\n",
    "\n",
    "            model_names = model_names_modified\n",
    "\n",
    "            model_names_final = []\n",
    "            for j in model_names:\n",
    "                if j == 'Gaussian N B':\n",
    "                    model_names_final.append('Naive Bayes')\n",
    "                elif j == 'M L P Classifier':\n",
    "                    model_names_final.append('MLP Classifier')\n",
    "                elif j == 'S G D Classifier':\n",
    "                    model_names_final.append('SVM - Linear Kernel')\n",
    "                elif j == 'S V C':\n",
    "                    model_names_final.append('SVM - Radial Kernel')\n",
    "                else: \n",
    "                    model_names_final.append(j)\n",
    "\n",
    "            model_names = model_names_final\n",
    "\n",
    "    progress.value += 1\n",
    "    \n",
    "    kf = StratifiedKFold(fold, random_state=seed)\n",
    "\n",
    "    score_acc =np.empty((0,0))\n",
    "    score_auc =np.empty((0,0))\n",
    "    score_recall =np.empty((0,0))\n",
    "    score_precision =np.empty((0,0))\n",
    "    score_f1 =np.empty((0,0))\n",
    "    score_kappa =np.empty((0,0))\n",
    "    score_acc_running = np.empty((0,0)) ##running total\n",
    "    avg_acc = np.empty((0,0))\n",
    "    avg_auc = np.empty((0,0))\n",
    "    avg_recall = np.empty((0,0))\n",
    "    avg_precision = np.empty((0,0))\n",
    "    avg_f1 = np.empty((0,0))\n",
    "    avg_kappa = np.empty((0,0))\n",
    "      \n",
    "    for model in model_library:\n",
    " \n",
    "        for train_i , test_i in kf.split(data_X,data_y):\n",
    "     \n",
    "            Xtrain,Xtest = data_X.iloc[train_i], data_X.iloc[test_i]\n",
    "            ytrain,ytest = data_y.iloc[train_i], data_y.iloc[test_i]\n",
    "        \n",
    "            if hasattr(model, 'predict_proba'):               \n",
    "        \n",
    "                model.fit(Xtrain,ytrain)\n",
    "                pred_prob = model.predict_proba(Xtest)\n",
    "                pred_prob = pred_prob[:,1]\n",
    "                pred_ = model.predict(Xtest)\n",
    "                sca = metrics.accuracy_score(ytest,pred_)\n",
    "                sc = metrics.roc_auc_score(ytest,pred_prob)\n",
    "                recall = metrics.recall_score(ytest,pred_)\n",
    "                precision = metrics.precision_score(ytest,pred_)\n",
    "                kappa = cohen_kappa_score(ytest,pred_)\n",
    "                f1 = metrics.f1_score(ytest,pred_)\n",
    "                score_acc = np.append(score_acc,sca)\n",
    "                score_auc = np.append(score_auc,sc)\n",
    "                score_recall = np.append(score_recall,recall)\n",
    "                score_precision = np.append(score_precision,precision)\n",
    "                score_f1 =np.append(score_f1,f1)\n",
    "                score_kappa =np.append(score_kappa,kappa)              \n",
    "        \n",
    "            else:        \n",
    "\n",
    "                model.fit(Xtrain,ytrain)\n",
    "                pred_prob = 0.00\n",
    "                pred_prob = 0.00\n",
    "                pred_ = model.predict(Xtest)\n",
    "                sca = metrics.accuracy_score(ytest,pred_)\n",
    "                sc = 0.00\n",
    "                recall = metrics.recall_score(ytest,pred_)\n",
    "                precision = metrics.precision_score(ytest,pred_) #change pred_prob to pred_\n",
    "                kappa = cohen_kappa_score(ytest,pred_)\n",
    "                f1 = metrics.f1_score(ytest,pred_)\n",
    "                score_acc = np.append(score_acc,sca)\n",
    "                score_auc = np.append(score_auc,sc)\n",
    "                score_recall = np.append(score_recall,recall)\n",
    "                score_precision = np.append(score_precision,precision)\n",
    "                score_f1 =np.append(score_f1,f1)\n",
    "                score_kappa =np.append(score_kappa,kappa) \n",
    "            \n",
    "            progress.value += 1\n",
    "        \n",
    "        progress.value += 1\n",
    "        \n",
    "        avg_acc = np.append(avg_acc,np.mean(score_acc))\n",
    "        avg_auc = np.append(avg_auc,np.mean(score_auc))\n",
    "        avg_recall = np.append(avg_recall,np.mean(score_recall))\n",
    "        avg_precision = np.append(avg_precision,np.mean(score_precision))\n",
    "        avg_f1 = np.append(avg_f1,np.mean(score_f1))\n",
    "        avg_kappa = np.append(avg_kappa,np.mean(score_kappa))\n",
    "        score_acc =np.empty((0,0))\n",
    "        score_auc =np.empty((0,0))\n",
    "        score_recall =np.empty((0,0))\n",
    "        score_precision =np.empty((0,0))\n",
    "        score_f1 =np.empty((0,0))\n",
    "        score_kappa =np.empty((0,0))\n",
    "  \n",
    "    progress.value += 1\n",
    "    \n",
    "    def highlight_max(s):\n",
    "        is_max = s == s.max()\n",
    "        return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "    compare_models_ = pd.DataFrame({'Model':model_names, 'Accuracy':avg_acc, 'AUC':avg_auc, \n",
    "                     'Recall':avg_recall, 'Prec.':avg_precision, \n",
    "                     'F1':avg_f1, 'Kappa': avg_kappa}).round(round).sort_values(by=[sort], \n",
    "                      ascending=False).reset_index(drop=True).style.apply(highlight_max,subset=['Accuracy','AUC','Recall',\n",
    "                      'Prec.','F1','Kappa'])\n",
    "    compare_models_ = compare_models_.set_properties(**{'text-align': 'left'})\n",
    "    compare_models_ = compare_models_.set_table_styles([dict(selector='th', props=[('text-align', 'left')])])\n",
    "    \n",
    "    progress.value += 1\n",
    "    \n",
    "    clear_output()\n",
    "\n",
    "    return compare_models_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model(estimator = None, \n",
    "               fold = 10, \n",
    "               round = 4, \n",
    "               n_iter = 10, \n",
    "               optimize = 'accuracy',\n",
    "               ensemble = False, \n",
    "               method = 'Bagging',\n",
    "               verbose = True):\n",
    "    \n",
    "      \n",
    "    \"\"\"\n",
    "    \n",
    "  Description:\n",
    "  ------------\n",
    "  This function tunes hyperparameter of a model and scores it using Stratified \n",
    "  Cross Validation. The output prints the score grid that shows Accuracy, AUC,\n",
    "  Recall, Precision, F1 and Kappa by fold (default = 10).\n",
    "\n",
    "  Function also return a trained model object that can be used for further \n",
    "  processing in pycaret or can be used to call any method available in sklearn. \n",
    "  \n",
    "  tune_model() accepts string parameter for estimator.\n",
    "  \n",
    "    Example\n",
    "    -------\n",
    "    tune_model('lr') \n",
    "    \n",
    "    This will tune the hyperparameters of Logistic Regression\n",
    "    \n",
    "    tune_model('lr', ensemble = True) \n",
    "    \n",
    "    This will tune the hyperparameters of Logistic Regression wrapped around \n",
    "    Bagging Classifier. \n",
    "    \n",
    "    \n",
    "  Parameters\n",
    "  ----------\n",
    "  \n",
    "  estimator : string, default = None\n",
    "  \n",
    "  Enter abbreviated name of the estimator class. List of estimators supported:\n",
    "  \n",
    "  Estimator                   Abbreviated String     Original Implementation \n",
    "  ---------                   ------------------     -----------------------\n",
    "  Logistic Regression         'lr'                   linear_model.LogisticRegression\n",
    "  K Nearest Neighbour         'knn'                  neighbors.KNeighborsClassifier\n",
    "  Naives Bayes                'nb'                   naive_bayes.GaussianNB\n",
    "  Decision Tree               'dt'                   tree.DecisionTreeClassifier\n",
    "  SVM (Linear)                'svm'                  linear_model.SGDClassifier\n",
    "  SVM (RBF)                   'rbfsvm'               svm.SVC\n",
    "  Gaussian Process            'gpc'                  gaussian_process.GPC\n",
    "  Multi Level Perceptron      'mlp'                  neural_network.MLPClassifier\n",
    "  Ridge Classifier            'ridge'                linear_model.RidgeClassifier\n",
    "  Random Forest               'rf'                   ensemble.RandomForestClassifier\n",
    "  Quadratic Disc. Analysis    'qda'                  discriminant_analysis.QDA \n",
    "  AdaBoost                    'ada'                  ensemble.AdaBoostClassifier\n",
    "  Gradient Boosting           'gbc'                  ensemble.GradientBoostingClassifier\n",
    "  Linear Disc. Analysis       'lda'                  discriminant_analysis.LDA \n",
    "  Extra Trees Classifier      'et'                   ensemble.ExtraTreesClassifier\n",
    "   \n",
    "  fold: integer, default = 10\n",
    "  Number of folds will determine how many folds would be done in the Kfold CV.\n",
    "  \n",
    "  round: integer, default = 4\n",
    "  The number indicates the number of decimal places metrics will be rounded to. \n",
    "\n",
    "  n_iter: integer, default = 10\n",
    "  Number of iterations within the Random Grid Search. For every iteration, \n",
    "  the model randomly selects one value from the pre-defined grid of hyperparameters.\n",
    "\n",
    "  optimize: string, default = 'accuracy'\n",
    "  Measure used to select the best model through the hyperparameter tuning.\n",
    "  The default scoring measure is 'accuracy'. Other common measures include\n",
    "  'f1', 'recall', 'precision', 'roc_auc'. Complete list available at:\n",
    "  https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "\n",
    "  ensemble: Boolean, default = False\n",
    "  True would enable ensembling of models through Bagging/Boosting method to be defined by 'method'.\n",
    "  \n",
    "  method: String, 'Bagging' or 'Boosting', default = Bagging\n",
    "  method comes into effect only when ensemble = True. Default is set to Bagging. \n",
    "\n",
    "  verbose: Boolean, default = True\n",
    "  Score grid is not printed when verbose is set to False.\n",
    "  \n",
    "  Returns:\n",
    "  --------\n",
    "  \n",
    "  score grid:   A table containing the scores of the model across the kfolds. \n",
    "  -----------   Scoring metrics used are Accuracy, AUC, Recall, Precision, F1 \n",
    "                and Kappa. Mean and standard deviation of the scores across the \n",
    "                folds is also returned.\n",
    "  \n",
    "  model:        trained model object\n",
    "  -----------\n",
    "\n",
    "  Warnings:\n",
    "  ---------\n",
    "  estimator parameter takes an abbreviated string. passing a trained model object\n",
    "  returns an error. tune_model('lr') function internally calls create_model() before\n",
    "  tuning the hyperparameters.\n",
    "\n",
    "  \n",
    "    \"\"\"\n",
    "   \n",
    "    #progress bar\n",
    "    import ipywidgets as ipw\n",
    "    from IPython.display import display, HTML, clear_output\n",
    "    progress = ipw.IntProgress(value=0, min=0, max=fold+5, step=1 , description='Processing: ')\n",
    "    display(progress)\n",
    "    \n",
    "    #check input parameter\n",
    "    import sys\n",
    "    if type(estimator) != str:   \n",
    "        print(\"Estimator is expecting abbreviated string for model to be tuned. Please see docstring complete list of models.\") \n",
    "        sys.exit('Exception Handling XXX') \n",
    "    \n",
    "    #ignore warnings\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')    \n",
    "\n",
    "    data_X = X_train\n",
    "    data_y = y_train\n",
    "\n",
    "    progress.value += 1\n",
    "    \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.gaussian_process.kernels import RBF\n",
    "    from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "    from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.model_selection import StratifiedKFold  \n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.linear_model import RidgeClassifier\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import pandas_profiling as pd_p\n",
    "    import seaborn as sns\n",
    "    from sklearn import preprocessing as pre\n",
    "    from sklearn.pipeline import Pipeline as pipe\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import Lasso\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.linear_model import SGDClassifier\n",
    "    from sklearn import metrics\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import ExtraTreesClassifier\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    from scipy import stats\n",
    "    import random\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.model_selection import cross_val_predict\n",
    "    from sklearn.model_selection import cross_validate\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.metrics import cohen_kappa_score\n",
    "    from sklearn.ensemble import BaggingClassifier\n",
    "    \n",
    "    progress.value += 1\n",
    "    \n",
    "    kf = StratifiedKFold(fold, random_state=seed)\n",
    "\n",
    "    score_auc =np.empty((0,0))\n",
    "    score_acc =np.empty((0,0))\n",
    "    score_recall =np.empty((0,0))\n",
    "    score_precision =np.empty((0,0))\n",
    "    score_f1 =np.empty((0,0))\n",
    "    score_kappa =np.empty((0,0))\n",
    "    avgs_auc =np.empty((0,0))\n",
    "    avgs_acc =np.empty((0,0))\n",
    "    avgs_recall =np.empty((0,0))\n",
    "    avgs_precision =np.empty((0,0))\n",
    "    avgs_f1 =np.empty((0,0))\n",
    "    avgs_kappa =np.empty((0,0))\n",
    "    \n",
    "    if estimator == 'knn':\n",
    "        \n",
    "        param_grid = {'n_neighbors': range(1,51),\n",
    "                 'weights' : ['uniform', 'distance'],\n",
    "                 'metric':[\"euclidean\", \"manhattan\"]\n",
    "                     }        \n",
    "        model_grid = RandomizedSearchCV(estimator=KNeighborsClassifier(), param_distributions=param_grid, \n",
    "                                        scoring=optimize, n_iter=n_iter, cv=fold, random_state=seed,\n",
    "                                       n_jobs=-1, iid=False)\n",
    "\n",
    "        model_grid.fit(X_train,y_train)\n",
    "        model = model_grid.best_estimator_\n",
    "        best_model = model_grid.best_estimator_\n",
    "        best_model_param = model_grid.best_params_\n",
    " \n",
    "    elif estimator == 'lr':\n",
    "\n",
    "        param_grid = {'C': [1,5,10,25,50,100],\n",
    "                  \"penalty\": [ 'l1', 'l2'],\n",
    "                  \"class_weight\": [\"balanced\", None]\n",
    "                     }\n",
    "        model_grid = RandomizedSearchCV(estimator=LogisticRegression(random_state=seed), \n",
    "                                        param_distributions=param_grid, scoring=optimize, n_iter=n_iter, cv=fold, \n",
    "                                        random_state=seed, iid=False,n_jobs=-1)\n",
    "        model_grid.fit(X_train,y_train)\n",
    "        model = model_grid.best_estimator_\n",
    "        best_model = model_grid.best_estimator_\n",
    "        best_model_param = model_grid.best_params_\n",
    "\n",
    "    elif estimator == 'dt':\n",
    "        \n",
    "        param_grid = {\"max_depth\": np.random.randint(3, (len(X_train.columns)*.85),4),\n",
    "                  \"max_features\": np.random.randint(3, len(X_train.columns),4),\n",
    "                  \"min_samples_leaf\": [2,3,4],\n",
    "                  \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "        model_grid = RandomizedSearchCV(estimator=DecisionTreeClassifier(random_state=seed), param_distributions=param_grid,\n",
    "                                       scoring=optimize, n_iter=n_iter, cv=fold, random_state=seed,\n",
    "                                       iid=False, n_jobs=-1)\n",
    "\n",
    "        model_grid.fit(X_train,y_train)\n",
    "        model = model_grid.best_estimator_\n",
    "        best_model = model_grid.best_estimator_\n",
    "        best_model_param = model_grid.best_params_\n",
    " \n",
    "    elif estimator == 'mlp':\n",
    "    \n",
    "        param_grid = {'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "                 'solver' : ['lbfgs', 'sgd', 'adam'],\n",
    "                 'alpha': [0.0001, 0.05],\n",
    "                 'hidden_layer_sizes': np.random.randint(5,15,5),\n",
    "                 'activation': [\"tanh\", \"identity\", \"logistic\",\"relu\"]\n",
    "                 }\n",
    "\n",
    "        model_grid = RandomizedSearchCV(estimator=MLPClassifier(max_iter=1000, random_state=seed), \n",
    "                                        param_distributions=param_grid, scoring=optimize, n_iter=n_iter, cv=fold, \n",
    "                                        random_state=seed, iid=False, n_jobs=-1)\n",
    "\n",
    "        model_grid.fit(X_train,y_train)\n",
    "        model = model_grid.best_estimator_\n",
    "        best_model = model_grid.best_estimator_\n",
    "        best_model_param = model_grid.best_params_\n",
    "    \n",
    "    elif estimator == 'gpc':\n",
    "    \n",
    "        param_grid = {\"max_iter_predict\":[100,200,300,400,500,600,700,800,900,1000]}\n",
    "\n",
    "        model_grid = RandomizedSearchCV(estimator=GaussianProcessClassifier(random_state=seed), param_distributions=param_grid,\n",
    "                                       scoring=optimize, n_iter=n_iter, cv=fold, random_state=seed,\n",
    "                                       n_jobs=-1)\n",
    "\n",
    "        model_grid.fit(X_train,y_train)\n",
    "        model = model_grid.best_estimator_\n",
    "        best_model = model_grid.best_estimator_\n",
    "        best_model_param = model_grid.best_params_    \n",
    "\n",
    "    elif estimator == 'rbfsvm':\n",
    "\n",
    "        param_grid = {'C': [.5,1,10,50,100],\n",
    "                \"class_weight\": [\"balanced\", None]}\n",
    "\n",
    "        model_grid = RandomizedSearchCV(estimator=SVC(gamma='auto', C=1, probability=True, kernel='rbf', random_state=seed), \n",
    "                                        param_distributions=param_grid, scoring=optimize, n_iter=n_iter, \n",
    "                                        cv=fold, random_state=seed, n_jobs=-1)\n",
    "\n",
    "        model_grid.fit(X_train,y_train)\n",
    "        model = model_grid.best_estimator_\n",
    "        best_model = model_grid.best_estimator_\n",
    "        best_model_param = model_grid.best_params_    \n",
    "  \n",
    "    elif estimator == 'nb':\n",
    "\n",
    "        param_grid = {'var_smoothing': [0.000000001, 0.0000001, 0.00001, 0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007,\n",
    "                                        0.008, 0.009, 0.01, 0.1, 1]}\n",
    "\n",
    "        model_grid = RandomizedSearchCV(estimator=GaussianNB(), \n",
    "                                        param_distributions=param_grid, scoring=optimize, n_iter=n_iter, \n",
    "                                        cv=fold, random_state=seed, n_jobs=-1)\n",
    " \n",
    "        model_grid.fit(X_train,y_train)\n",
    "        model = model_grid.best_estimator_\n",
    "        best_model = model_grid.best_estimator_\n",
    "        best_model_param = model_grid.best_params_        \n",
    "\n",
    "    elif estimator == 'svm':\n",
    "   \n",
    "        param_grid = {'penalty': ['l2', 'l1','elasticnet'],\n",
    "                      'l1_ratio': [0,0.1,0.15,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],\n",
    "                      'alpha': [0.0001, 0.001, 0.01, 0.0002, 0.002, 0.02, 0.0005, 0.005, 0.05],\n",
    "                      'fit_intercept': [True, False],\n",
    "                      'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "                      'eta0': [0.001, 0.01,0.05,0.1,0.2,0.3,0.4,0.5]\n",
    "                     }    \n",
    "\n",
    "        model_grid = RandomizedSearchCV(estimator=SGDClassifier(loss='hinge', random_state=seed), \n",
    "                                        param_distributions=param_grid, scoring=optimize, n_iter=n_iter, \n",
    "                                        cv=fold, random_state=seed, n_jobs=-1)\n",
    "\n",
    "        model_grid.fit(X_train,y_train)\n",
    "        model = model_grid.best_estimator_\n",
    "        best_model = model_grid.best_estimator_\n",
    "        best_model_param = model_grid.best_params_     \n",
    "\n",
    "    elif estimator == 'ridge':\n",
    "\n",
    "        param_grid = {'alpha': [0.0001,0.001,0.1,0.15,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],\n",
    "                      'fit_intercept': [True, False],\n",
    "                      'normalize': [True, False]\n",
    "                     }    \n",
    "\n",
    "        model_grid = RandomizedSearchCV(estimator=RidgeClassifier(random_state=seed), \n",
    "                                        param_distributions=param_grid, scoring=optimize, n_iter=n_iter, \n",
    "                                        cv=fold, random_state=seed, n_jobs=-1)\n",
    "\n",
    "        model_grid.fit(X_train,y_train)\n",
    "        model = model_grid.best_estimator_\n",
    "        best_model = model_grid.best_estimator_\n",
    "        best_model_param = model_grid.best_params_     \n",
    "   \n",
    "    elif estimator == 'rf':\n",
    "\n",
    "        param_grid = {'n_estimators': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "                      'criterion': ['gini', 'entropy'],\n",
    "                      'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],\n",
    "                      'min_samples_split': [2, 5, 7, 9, 10],\n",
    "                      'min_samples_leaf' : [1, 2, 4],\n",
    "                      'max_features' : ['auto', 'sqrt', 'log2'],\n",
    "                      'bootstrap': [True, False]\n",
    "                     }    \n",
    "\n",
    "        model_grid = RandomizedSearchCV(estimator=RandomForestClassifier(random_state=seed), \n",
    "                                        param_distributions=param_grid, scoring=optimize, n_iter=n_iter, \n",
    "                                        cv=fold, random_state=seed, n_jobs=-1)\n",
    "\n",
    "        model_grid.fit(X_train,y_train)\n",
    "        model = model_grid.best_estimator_\n",
    "        best_model = model_grid.best_estimator_\n",
    "        best_model_param = model_grid.best_params_     \n",
    "   \n",
    "    elif estimator == 'ada':\n",
    "\n",
    "        param_grid = {'n_estimators': [10, 40, 70, 80, 90, 100, 120, 140, 150],\n",
    "                      'learning_rate': [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],\n",
    "                      'algorithm' : [\"SAMME\", \"SAMME.R\"]\n",
    "                     }    \n",
    "\n",
    "        model_grid = RandomizedSearchCV(estimator=AdaBoostClassifier(random_state=seed), \n",
    "                                        param_distributions=param_grid, scoring=optimize, n_iter=n_iter, \n",
    "                                        cv=fold, random_state=seed, n_jobs=-1)\n",
    "\n",
    "        model_grid.fit(X_train,y_train)\n",
    "        model = model_grid.best_estimator_\n",
    "        best_model = model_grid.best_estimator_\n",
    "        best_model_param = model_grid.best_params_   \n",
    "\n",
    "    elif estimator == 'gbc':\n",
    "\n",
    "        param_grid = {'loss': ['deviance', 'exponential'],\n",
    "                      'n_estimators': [10, 40, 70, 80, 90, 100, 120, 140, 150],\n",
    "                      'learning_rate': [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],\n",
    "                      'subsample' : [0.1,0.3,0.5,0.7,0.9,1],\n",
    "                      'min_samples_split' : [2,4,5,7,9,10],\n",
    "                      'min_samples_leaf' : [1,2,3,4,5],\n",
    "                      'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],\n",
    "                      'max_features' : ['auto', 'sqrt', 'log2']\n",
    "                     }    \n",
    "\n",
    "        model_grid = RandomizedSearchCV(estimator=GradientBoostingClassifier(random_state=seed), \n",
    "                                        param_distributions=param_grid, scoring=optimize, n_iter=n_iter, \n",
    "                                        cv=fold, random_state=seed, n_jobs=-1)\n",
    "\n",
    "        model_grid.fit(X_train,y_train)\n",
    "        model = model_grid.best_estimator_\n",
    "        best_model = model_grid.best_estimator_\n",
    "        best_model_param = model_grid.best_params_   \n",
    "\n",
    "    elif estimator == 'qda':\n",
    "\n",
    "        param_grid = {'reg_param': [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]}    \n",
    "\n",
    "        model_grid = RandomizedSearchCV(estimator=QuadraticDiscriminantAnalysis(), \n",
    "                                        param_distributions=param_grid, scoring=optimize, n_iter=n_iter, \n",
    "                                        cv=fold, random_state=seed, n_jobs=-1)\n",
    "\n",
    "        model_grid.fit(X_train,y_train)\n",
    "        model = model_grid.best_estimator_\n",
    "        best_model = model_grid.best_estimator_\n",
    "        best_model_param = model_grid.best_params_      \n",
    "\n",
    "    elif estimator == 'lda':\n",
    "\n",
    "        param_grid = {'solver' : ['lsqr', 'eigen'],\n",
    "                      'shrinkage': [0.0001, 0.001, 0.01, 0.0005, 0.005, 0.05, 0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "                     }    \n",
    "\n",
    "        model_grid = RandomizedSearchCV(estimator=LinearDiscriminantAnalysis(), \n",
    "                                        param_distributions=param_grid, scoring=optimize, n_iter=n_iter, \n",
    "                                        cv=fold, random_state=seed, n_jobs=-1)\n",
    "\n",
    "        model_grid.fit(X_train,y_train)\n",
    "        model = model_grid.best_estimator_\n",
    "        best_model = model_grid.best_estimator_\n",
    "        best_model_param = model_grid.best_params_        \n",
    "\n",
    "    elif estimator == 'et':\n",
    "\n",
    "        param_grid = {'n_estimators': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "                      'criterion': ['gini', 'entropy'],\n",
    "                      'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],\n",
    "                      'min_samples_split': [2, 5, 7, 9, 10],\n",
    "                      'min_samples_leaf' : [1, 2, 4],\n",
    "                      'max_features' : ['auto', 'sqrt', 'log2'],\n",
    "                      'bootstrap': [True, False]\n",
    "                     }    \n",
    "\n",
    "        model_grid = RandomizedSearchCV(estimator=ExtraTreesClassifier(random_state=seed), \n",
    "                                        param_distributions=param_grid, scoring=optimize, n_iter=n_iter, \n",
    "                                        cv=fold, random_state=seed, n_jobs=-1)\n",
    "\n",
    "        model_grid.fit(X_train,y_train)\n",
    "        model = model_grid.best_estimator_\n",
    "        best_model = model_grid.best_estimator_\n",
    "        best_model_param = model_grid.best_params_          \n",
    "    \n",
    "    progress.value += 1\n",
    "    \n",
    "    if estimator == 'dt' and ensemble == True and method == 'Bagging':\n",
    "    \n",
    "    #when using normal BaggingClassifier() DT estimator raise's an exception for max_features parameter. Hence a separate \n",
    "    #call has been made for estimator='dt' and method = 'Bagging' where max_features has been removed from param_grid_dt.\n",
    "    \n",
    "        param_grid = {'n_estimators': [10,15,20,25,30],\n",
    "                     'max_samples': [0.3,0.5,0.6,0.7,0.8,0.9],\n",
    "                     'max_features':[0.3,0.5,0.6,0.7,0.8,0.9],\n",
    "                     'bootstrap': [True, False],\n",
    "                     'bootstrap_features': [True, False],\n",
    "                     }\n",
    "\n",
    "        param_grid_dt = {\"max_depth\": np.random.randint(3, (len(X_train.columns)*.85),4),\n",
    "                      \"min_samples_leaf\": [2,3,4],\n",
    "                      \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "\n",
    "        model_grid = RandomizedSearchCV(estimator=DecisionTreeClassifier(random_state=seed), param_distributions=param_grid_dt,\n",
    "                                       scoring=optimize, n_iter=n_iter, cv=fold, random_state=seed,\n",
    "                                       iid=False, n_jobs=-1)\n",
    "\n",
    "        model_grid.fit(X_train,y_train)\n",
    "        model = model_grid.best_estimator_\n",
    "        best_model = model_grid.best_estimator_\n",
    "        best_model_param = model_grid.best_params_\n",
    "\n",
    "        best_model = BaggingClassifier(best_model, random_state=seed)\n",
    "\n",
    "        model_grid = RandomizedSearchCV(estimator=best_model, \n",
    "                                        param_distributions=param_grid, scoring=optimize, n_iter=n_iter, \n",
    "                                        cv=fold, random_state=seed, iid=False, n_jobs=-1)\n",
    "\n",
    "        model_grid.fit(X_train,y_train)\n",
    "        model = model_grid.best_estimator_\n",
    "        best_model = model_grid.best_estimator_\n",
    "        best_model_param = model_grid.best_params_    \n",
    "  \n",
    "        progress.value += 1\n",
    "    \n",
    "    elif ensemble and method == 'Bagging':\n",
    "    \n",
    "        param_grid = {'n_estimators': [10,15,20,25,30],\n",
    "                     'max_samples': [0.3,0.5,0.6,0.7,0.8,0.9],\n",
    "                     'max_features':[0.3,0.5,0.6,0.7,0.8,0.9],\n",
    "                     'bootstrap': [True, False],\n",
    "                     'bootstrap_features': [True, False],\n",
    "                     }\n",
    "\n",
    "        best_model = BaggingClassifier(best_model, random_state=seed)\n",
    "\n",
    "        model_grid = RandomizedSearchCV(estimator=best_model, \n",
    "                                        param_distributions=param_grid, scoring=optimize, n_iter=n_iter, \n",
    "                                        cv=fold, random_state=seed, iid=False, n_jobs=-1)\n",
    "\n",
    "        model_grid.fit(X_train,y_train)\n",
    "        model = model_grid.best_estimator_\n",
    "        best_model = model_grid.best_estimator_\n",
    "        best_model_param = model_grid.best_params_    \n",
    "     \n",
    "    elif ensemble and method =='Boosting':\n",
    "        \n",
    "        param_grid = {'n_estimators': [25,35,50,60,70,75],\n",
    "                     'learning_rate': [1,0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2],\n",
    "                     }        \n",
    "\n",
    "        best_model = AdaBoostClassifier(best_model, random_state=seed)\n",
    "\n",
    "        model_grid = RandomizedSearchCV(estimator=best_model, \n",
    "                                        param_distributions=param_grid, scoring=optimize, n_iter=n_iter, \n",
    "                                        cv=fold, random_state=seed, iid=False, n_jobs=-1)\n",
    "\n",
    "    progress.value += 1\n",
    "    \n",
    "    for train_i , test_i in kf.split(data_X,data_y):\n",
    "    \n",
    "        Xtrain,Xtest = data_X.iloc[train_i], data_X.iloc[test_i]\n",
    "        ytrain,ytest = data_y.iloc[train_i], data_y.iloc[test_i]\n",
    "    \n",
    "        if hasattr(best_model, 'predict_proba'):  \n",
    "        \n",
    "            model.fit(Xtrain,ytrain)\n",
    "            pred_prob = model.predict_proba(Xtest)\n",
    "            pred_prob = pred_prob[:,1]\n",
    "            pred_ = model.predict(Xtest)\n",
    "            sca = metrics.accuracy_score(ytest,pred_)\n",
    "            sc = metrics.roc_auc_score(ytest,pred_prob)\n",
    "            recall = metrics.recall_score(ytest,pred_)\n",
    "            precision = metrics.precision_score(ytest,pred_)\n",
    "            kappa = cohen_kappa_score(ytest,pred_)\n",
    "            f1 = metrics.f1_score(ytest,pred_)\n",
    "            score_acc = np.append(score_acc,sca)\n",
    "            score_auc = np.append(score_auc,sc)\n",
    "            score_recall = np.append(score_recall,recall)\n",
    "            score_precision = np.append(score_precision,precision)\n",
    "            score_f1 =np.append(score_f1,f1)\n",
    "            score_kappa =np.append(score_kappa,kappa)\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            model.fit(Xtrain,ytrain)\n",
    "            pred_prob = 0.00\n",
    "            pred_prob = 0.00\n",
    "            pred_ = model.predict(Xtest)\n",
    "            sca = metrics.accuracy_score(ytest,pred_)\n",
    "            sc = 0.00\n",
    "            recall = metrics.recall_score(ytest,pred_)\n",
    "            precision = metrics.precision_score(ytest,pred_) #change pred_prob to pred_\n",
    "            kappa = cohen_kappa_score(ytest,pred_)\n",
    "            f1 = metrics.f1_score(ytest,pred_)\n",
    "            score_acc = np.append(score_acc,sca)\n",
    "            score_auc = np.append(score_auc,sc)\n",
    "            score_recall = np.append(score_recall,recall)\n",
    "            score_precision = np.append(score_precision,precision)\n",
    "            score_f1 =np.append(score_f1,f1)\n",
    "            score_kappa =np.append(score_kappa,kappa) \n",
    "    \n",
    "    progress.value += 1\n",
    "    \n",
    "    mean_acc=np.mean(score_acc)\n",
    "    mean_auc=np.mean(score_auc)\n",
    "    mean_recall=np.mean(score_recall)\n",
    "    mean_precision=np.mean(score_precision)\n",
    "    mean_f1=np.mean(score_f1)\n",
    "    mean_kappa=np.mean(score_kappa)\n",
    "    std_acc=np.std(score_acc)\n",
    "    std_auc=np.std(score_auc)\n",
    "    std_recall=np.std(score_recall)\n",
    "    std_precision=np.std(score_precision)\n",
    "    std_f1=np.std(score_f1)\n",
    "    std_kappa=np.std(score_kappa)\n",
    "\n",
    "    avgs_acc = np.append(avgs_acc, mean_acc)\n",
    "    avgs_acc = np.append(avgs_acc, std_acc) \n",
    "    avgs_auc = np.append(avgs_auc, mean_auc)\n",
    "    avgs_auc = np.append(avgs_auc, std_auc)\n",
    "    avgs_recall = np.append(avgs_recall, mean_recall)\n",
    "    avgs_recall = np.append(avgs_recall, std_recall)\n",
    "    avgs_precision = np.append(avgs_precision, mean_precision)\n",
    "    avgs_precision = np.append(avgs_precision, std_precision)\n",
    "    avgs_f1 = np.append(avgs_f1, mean_f1)\n",
    "    avgs_f1 = np.append(avgs_f1, std_f1)\n",
    "    avgs_kappa = np.append(avgs_kappa, mean_kappa)\n",
    "    avgs_kappa = np.append(avgs_kappa, std_kappa)\n",
    "\n",
    "    progress.value += 1\n",
    "    \n",
    "    model_results = pd.DataFrame({'Accuracy': score_acc, 'AUC': score_auc, 'Recall' : score_recall, 'Prec.' : score_precision , \n",
    "                     'F1' : score_f1, 'Kappa' : score_kappa})\n",
    "    model_avgs = pd.DataFrame({'Accuracy': avgs_acc, 'AUC': avgs_auc, 'Recall' : avgs_recall, 'Prec.' : avgs_precision , \n",
    "                     'F1' : avgs_f1, 'Kappa' : avgs_kappa},index=['Mean', 'SD'])\n",
    "\n",
    "    model_results = model_results.append(model_avgs)\n",
    "    model_results = model_results.round(round)\n",
    "\n",
    "    progress.value += 1\n",
    "    \n",
    "    if verbose:\n",
    "        clear_output()\n",
    "        display(model_results)\n",
    "        return best_model\n",
    "    else:\n",
    "        clear_output()\n",
    "        return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blend_models(estimator_list = 'All', \n",
    "                 fold = 10, \n",
    "                 round = 4, \n",
    "                 method = 'hard'):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "  Description:\n",
    "  ------------\n",
    "  This function creates a Soft Voting / Majority Rule classifier for list of estimators\n",
    "  provided or for all estimators in model library and scores it using Stratified Cross \n",
    "  Validation. The output prints the score grid that shows Accuracy, AUC, Recall, \n",
    "  Precision, F1 and Kappa by fold (default = 10). \n",
    "\n",
    "  Function also return a trained model object that can be used for further \n",
    "  processing in pycaret or can be used to call any method available in sklearn. \n",
    "  \n",
    "    Example:\n",
    "    --------\n",
    "    \n",
    "    blend_models() \n",
    "    \n",
    "    This will result in VotingClassifier for all models in library.   \n",
    "    ** All other parameters are optional.\n",
    "    \n",
    "    For specific models, you can use:\n",
    "    \n",
    "    lr = create_model( 'lr' )\n",
    "    rf = create_model( 'rf' )\n",
    "    \n",
    "    blend_models( [ lr, rf ] )\n",
    "    \n",
    "    This will result in VotingClassifier of lr and rf.\n",
    "    \n",
    "  Parameters\n",
    "  ----------\n",
    "  \n",
    "  estimator_list : string ('All') or list of object, default = 'All'\n",
    "\n",
    "  fold: integer, default = 10\n",
    "  Number of folds will determine how many folds would be done in the Kfold CV.\n",
    "  \n",
    "  round: integer, default = 4\n",
    "  The number of decimal places metrics will be rounded to. \n",
    "\n",
    "  method: string, default = 'hard'\n",
    "  \n",
    "  If ‘hard’, uses predicted class labels for majority rule voting. \n",
    "  Else if ‘soft’, predicts the class label based on the argmax of the sums \n",
    "  of the predicted probabilities, which is recommended for an ensemble of \n",
    "  well-calibrated classifiers. When estimator_list is set as 'All'. \n",
    "  Method is forced to be 'hard'. \n",
    "  \n",
    "  Returns:\n",
    "  --------\n",
    "  \n",
    "  score grid:   A table containing the scores of the model across the kfolds. \n",
    "  -----------   Scoring metrics used are Accuracy, AUC, Recall, Precision, F1 \n",
    "                and Kappa. Mean and standard deviation of the scores across the \n",
    "                folds is also returned.\n",
    "  \n",
    "  model:        trained model object\n",
    "  -----------\n",
    "\n",
    "  Warnings:\n",
    "  ---------\n",
    "  None\n",
    "  \n",
    "  \n",
    "    \"\"\"\n",
    "    \n",
    "    #progress bar\n",
    "    import ipywidgets as ipw\n",
    "    from IPython.display import display, HTML, clear_output\n",
    "    progress = ipw.IntProgress(value=0, min=0, max=fold+3, step=1 , description='Processing: ')\n",
    "    display(progress)\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import sys\n",
    "    from sklearn import metrics\n",
    "    \n",
    "    #defining X_train and y_train called from setup() into variable data_X and data_y to be used in cross validation   \n",
    "    data_X = X_train\n",
    "    data_y = y_train\n",
    "\n",
    "    #ignore warnings\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore') \n",
    "\n",
    "    #general imports\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    import sys #for exception handling  \n",
    "    from sklearn import metrics\n",
    "    from sklearn.model_selection import StratifiedKFold  \n",
    "    from sklearn.ensemble import VotingClassifier\n",
    "    \n",
    "    progress.value += 1\n",
    "    \n",
    "    score_auc =np.empty((0,0))\n",
    "    score_acc =np.empty((0,0))\n",
    "    score_recall =np.empty((0,0))\n",
    "    score_precision =np.empty((0,0))\n",
    "    score_f1 =np.empty((0,0))\n",
    "    score_kappa =np.empty((0,0))\n",
    "    avgs_auc =np.empty((0,0))\n",
    "    avgs_acc =np.empty((0,0))\n",
    "    avgs_recall =np.empty((0,0))\n",
    "    avgs_precision =np.empty((0,0))\n",
    "    avgs_f1 =np.empty((0,0))\n",
    "    avgs_kappa =np.empty((0,0))\n",
    "    avg_acc = np.empty((0,0))\n",
    "    avg_auc = np.empty((0,0))\n",
    "    avg_recall = np.empty((0,0))\n",
    "    avg_precision = np.empty((0,0))\n",
    "    avg_f1 = np.empty((0,0))\n",
    "    avg_kappa = np.empty((0,0))\n",
    "\n",
    "    kf = StratifiedKFold(fold, random_state=seed)\n",
    "        \n",
    "    if estimator_list == 'All':\n",
    "\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        from sklearn.naive_bayes import GaussianNB\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        from sklearn.linear_model import SGDClassifier\n",
    "        from sklearn.svm import SVC\n",
    "        from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "        from sklearn.neural_network import MLPClassifier\n",
    "        from sklearn.linear_model import RidgeClassifier\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "        from sklearn.ensemble import AdaBoostClassifier\n",
    "        from sklearn.ensemble import GradientBoostingClassifier    \n",
    "        from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "        from sklearn.ensemble import ExtraTreesClassifier\n",
    "        from sklearn.ensemble import BaggingClassifier     \n",
    "\n",
    "        lr = LogisticRegression(random_state=seed)\n",
    "        knn = KNeighborsClassifier()\n",
    "        nb = GaussianNB()\n",
    "        dt = DecisionTreeClassifier(random_state=seed)\n",
    "        svm = SGDClassifier(max_iter=1000, tol=0.001, random_state=seed)\n",
    "        rbfsvm = SVC(gamma='auto', C=1, probability=True, kernel='rbf', random_state=seed)\n",
    "        gpc = GaussianProcessClassifier(random_state=seed)\n",
    "        mlp = MLPClassifier(max_iter=500, random_state=seed)\n",
    "        ridge = RidgeClassifier(random_state=seed)\n",
    "        rf = RandomForestClassifier(n_estimators=10, random_state=seed)\n",
    "        qda = QuadraticDiscriminantAnalysis()\n",
    "        ada = AdaBoostClassifier(random_state=seed)\n",
    "        gbc = GradientBoostingClassifier(random_state=seed)\n",
    "        lda = LinearDiscriminantAnalysis()\n",
    "        et = ExtraTreesClassifier(random_state=seed)  \n",
    "\n",
    "        progress.value += 1\n",
    "        \n",
    "        estimator_list = [lr,knn,nb,dt,svm,rbfsvm,gpc,mlp,ridge,rf,qda,ada,gbc,lda,et]\n",
    "        voting = 'hard'\n",
    "\n",
    "    else:\n",
    "\n",
    "        estimator_list = estimator_list\n",
    "        voting = method  \n",
    "        \n",
    "        progress.value += 1\n",
    "        \n",
    "    model_names = []\n",
    "\n",
    "    for names in estimator_list:\n",
    "\n",
    "        model_names = np.append(model_names, str(names).split(\"(\")[0])\n",
    "\n",
    "    def putSpace(input):\n",
    "        words = re.findall('[A-Z][a-z]*', input)\n",
    "        words = ' '.join(words)\n",
    "        return words  \n",
    "\n",
    "    model_names_modified = []\n",
    "    \n",
    "    for i in model_names:\n",
    "        \n",
    "        model_names_modified.append(putSpace(i))\n",
    "        model_names = model_names_modified\n",
    "\n",
    "    global model_names_final\n",
    "    \n",
    "    model_names_final = []\n",
    "  \n",
    "    for j in model_names_modified:\n",
    "\n",
    "        if j == 'Gaussian N B':\n",
    "            model_names_final.append('Naive Bayes')\n",
    "\n",
    "        elif j == 'M L P Classifier':\n",
    "            model_names_final.append('MLP Classifier')\n",
    "\n",
    "        elif j == 'S G D Classifier':\n",
    "            model_names_final.append('SVM - Linear Kernel')\n",
    "\n",
    "        elif j == 'S V C':\n",
    "            model_names_final.append('SVM - Radial Kernel')\n",
    "\n",
    "        else: \n",
    "            model_names_final.append(j)\n",
    "            model_names = model_names_final\n",
    "            #estimator_list = estimator_list\n",
    "\n",
    "            estimator_list_ = zip(model_names, estimator_list)\n",
    "            estimator_list_ = set(estimator_list_)\n",
    "            estimator_list_ = list(estimator_list_)\n",
    "    \n",
    "        model = VotingClassifier(estimators=estimator_list_, voting=voting, n_jobs=-1)\n",
    "    \n",
    "    progress.value += 1\n",
    "    \n",
    "    for train_i , test_i in kf.split(data_X,data_y):\n",
    "    \n",
    "        Xtrain,Xtest = data_X.iloc[train_i], data_X.iloc[test_i]\n",
    "        ytrain,ytest = data_y.iloc[train_i], data_y.iloc[test_i]    \n",
    "    \n",
    "        if voting == 'hard':\n",
    "        \n",
    "            model.fit(Xtrain,ytrain)\n",
    "            pred_prob = 0.0\n",
    "            pred_prob = 0.0\n",
    "            pred_ = model.predict(Xtest)\n",
    "            sca = metrics.accuracy_score(ytest,pred_)\n",
    "            sc = 0.0\n",
    "            recall = metrics.recall_score(ytest,pred_)\n",
    "            precision = metrics.precision_score(ytest,pred_)\n",
    "            kappa = metrics.cohen_kappa_score(ytest,pred_)\n",
    "            f1 = metrics.f1_score(ytest,pred_)\n",
    "            score_acc = np.append(score_acc,sca)\n",
    "            score_auc = np.append(score_auc,sc)\n",
    "            score_recall = np.append(score_recall,recall)\n",
    "            score_precision = np.append(score_precision,precision)\n",
    "            score_f1 =np.append(score_f1,f1)\n",
    "            score_kappa =np.append(score_kappa,kappa)\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            model.fit(Xtrain,ytrain)\n",
    "            pred_prob = model.predict_proba(Xtest)\n",
    "            pred_prob = pred_prob[:,1]\n",
    "            pred_ = model.predict(Xtest)\n",
    "            sca = metrics.accuracy_score(ytest,pred_)\n",
    "            sc = metrics.roc_auc_score(ytest,pred_prob)\n",
    "            recall = metrics.recall_score(ytest,pred_)\n",
    "            precision = metrics.precision_score(ytest,pred_)\n",
    "            kappa = metrics.cohen_kappa_score(ytest,pred_)\n",
    "            f1 = metrics.f1_score(ytest,pred_)\n",
    "            score_acc = np.append(score_acc,sca)\n",
    "            score_auc = np.append(score_auc,sc)\n",
    "            score_recall = np.append(score_recall,recall)\n",
    "            score_precision = np.append(score_precision,precision)\n",
    "            score_f1 =np.append(score_f1,f1)\n",
    "            score_kappa =np.append(score_kappa,kappa)\n",
    "    \n",
    "    progress.value += 1\n",
    "    \n",
    "    mean_acc=np.mean(score_acc)\n",
    "    mean_auc=np.mean(score_auc)\n",
    "    mean_recall=np.mean(score_recall)\n",
    "    mean_precision=np.mean(score_precision)\n",
    "    mean_f1=np.mean(score_f1)\n",
    "    mean_kappa=np.mean(score_kappa)\n",
    "    std_acc=np.std(score_acc)\n",
    "    std_auc=np.std(score_auc)\n",
    "    std_recall=np.std(score_recall)\n",
    "    std_precision=np.std(score_precision)\n",
    "    std_f1=np.std(score_f1)\n",
    "    std_kappa=np.std(score_kappa)\n",
    "\n",
    "    avgs_acc = np.append(avgs_acc, mean_acc)\n",
    "    avgs_acc = np.append(avgs_acc, std_acc) \n",
    "    avgs_auc = np.append(avgs_auc, mean_auc)\n",
    "    avgs_auc = np.append(avgs_auc, std_auc)\n",
    "    avgs_recall = np.append(avgs_recall, mean_recall)\n",
    "    avgs_recall = np.append(avgs_recall, std_recall)\n",
    "    avgs_precision = np.append(avgs_precision, mean_precision)\n",
    "    avgs_precision = np.append(avgs_precision, std_precision)\n",
    "    avgs_f1 = np.append(avgs_f1, mean_f1)\n",
    "    avgs_f1 = np.append(avgs_f1, std_f1)\n",
    "    avgs_kappa = np.append(avgs_kappa, mean_kappa)\n",
    "    avgs_kappa = np.append(avgs_kappa, std_kappa)\n",
    "    \n",
    "    progress.value += 1\n",
    "    \n",
    "    model_results = pd.DataFrame({'Accuracy': score_acc, 'AUC': score_auc, 'Recall' : score_recall, 'Prec.' : score_precision , \n",
    "                     'F1' : score_f1, 'Kappa' : score_kappa})\n",
    "    model_avgs = pd.DataFrame({'Accuracy': avgs_acc, 'AUC': avgs_auc, 'Recall' : avgs_recall, 'Prec.' : avgs_precision , \n",
    "                     'F1' : avgs_f1, 'Kappa' : avgs_kappa},index=['Mean', 'SD'])\n",
    "\n",
    "    model_results = model_results.append(model_avgs)\n",
    "    model_results = model_results.round(round)\n",
    "    \n",
    "    progress.value += 1\n",
    "    \n",
    "    clear_output()\n",
    "    display(model_results)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_models(estimator_list, \n",
    "                 meta_model = None, \n",
    "                 fold = 10,\n",
    "                 round = 4, \n",
    "                 method = 'hard', \n",
    "                 restack = False, \n",
    "                 plot = False):\n",
    "    \n",
    "    \"\"\"\n",
    "     \n",
    "  Description:\n",
    "  ------------\n",
    "  This function creates a meta model and scores it using Stratified Cross Validation,\n",
    "  the prediction from base level models passed as estimator_list parameter is used\n",
    "  as input feature for meta model. Restacking parameter control the ability to expose\n",
    "  raw features to meta model when set to True (default = False). \n",
    "\n",
    "  The output prints the score grid that shows Accuracy, AUC, Recall, Precision, \n",
    "  F1 and Kappa by fold (default = 10). Function returns a container which is the \n",
    "  list of all models. \n",
    "  \n",
    "  This is an original implementation of pycaret.\n",
    "  \n",
    "    Example:\n",
    "    --------\n",
    "    \n",
    "    nb = create_model('nb')\n",
    "    rf = create_model('rf')\n",
    "    ada = create_model('ada')\n",
    "    ridge = create_model('ridge')\n",
    "    knn = create_model('knn')\n",
    "    \n",
    "    stack_models( [ nb, rf, ada, ridge, knn ] )\n",
    "    \n",
    "    This will result in creation of meta model that will use the predictions of \n",
    "    all the models provided as an input feature of meta model By default meta model \n",
    "    is Logistic Regression but can be changed with meta_model param.\n",
    "    \n",
    "  Parameters\n",
    "  ----------\n",
    "  \n",
    "  estimator_list : list of object\n",
    "  \n",
    "  meta_model : object, default = None\n",
    "  if set to None, Logistic Regression is used as a meta model.\n",
    "\n",
    "  fold: integer, default = 10\n",
    "  Number of folds will determine how many folds would be done in the Kfold CV.\n",
    "  \n",
    "  round: integer, default = 4\n",
    "  The number of decimal places metrics will be rounded to. \n",
    "\n",
    "  method: string, default = 'hard'\n",
    "  'hard', uses predicted class labels as input to meta model. \n",
    "  'soft', uses predicted probabilities as input to meta model.\n",
    "  \n",
    "  restack: Boolean, default = False\n",
    "  When restack is set to True, it will expose raw data to meta model.\n",
    "  \n",
    "  plot: Boolean, default = False\n",
    "  When plot is set to True, it will return the correlation plot of prediction\n",
    "  from all base models provided in estimator_list.\n",
    "  \n",
    "  Returns:\n",
    "  --------\n",
    "  \n",
    "  score grid:   A table containing the scores of the model across the kfolds. \n",
    "  -----------   Scoring metrics used are Accuracy, AUC, Recall, Precision, F1 \n",
    "                and Kappa. Mean and standard deviation of the scores across the \n",
    "                folds is also returned.\n",
    "  \n",
    "  model:        trained model object\n",
    "  -----------\n",
    "  \n",
    "  Warnings:\n",
    "  ---------\n",
    "  When estimator doesn't support 'predict_proba' (for example: ridge) and method is \n",
    "  forced to 'soft', stack_models() will return an error. \n",
    "   \n",
    "  \n",
    "  \"\"\"\n",
    "    \n",
    "    #dependencies\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn import metrics\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.model_selection import cross_val_predict\n",
    "    import sys\n",
    "    \n",
    "    #Capturing the method of stacking required by user. method='soft' means 'predict_proba' else 'predict'\n",
    "    \n",
    "    if method == 'soft':\n",
    "        predict_method = 'predict_proba'\n",
    "    elif method == 'hard':\n",
    "        predict_method = 'predict'\n",
    "    \n",
    "    #Defining meta model. Logistic Regression hardcoded for now\n",
    "    \n",
    "    if meta_model == None:\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        meta_model = LogisticRegression()\n",
    "    else:\n",
    "        meta_model = meta_model\n",
    "    \n",
    "    #defining model_library model names\n",
    "    \n",
    "    model_names = np.zeros(0)\n",
    "    for item in estimator_list:\n",
    "        model_names = np.append(model_names, str(item).split(\"(\")[0])\n",
    "    \n",
    "    ##########################\n",
    "    ##########################\n",
    "    ##########################\n",
    "    \n",
    "    base_array = np.zeros((0,0))\n",
    "    base_prediction = pd.DataFrame(y_train)\n",
    "    base_prediction = base_prediction.reset_index(drop=True)\n",
    "    \n",
    "    for model in estimator_list:\n",
    "        base_array = cross_val_predict(model,X_train,y_train,cv=fold, method=predict_method)\n",
    "        if method == 'soft':\n",
    "            base_array = base_array[:,1]\n",
    "        elif method == 'hard':\n",
    "            base_array = base_array\n",
    "        base_array_df = pd.DataFrame(base_array)\n",
    "        base_prediction = pd.concat([base_prediction,base_array_df],axis=1)\n",
    "        base_array = np.empty((0,0))\n",
    "        \n",
    "    #defining column names now\n",
    "    target_col_name = np.array(base_prediction.columns[0])\n",
    "    model_names = np.append(target_col_name, model_names)\n",
    "    base_prediction.columns = model_names #defining colum names now\n",
    "    \n",
    "    #defining data_X and data_y dataframe to be used in next stage.\n",
    "    \n",
    "    if restack:\n",
    "        data_X_ = X_train\n",
    "        data_X_ = data_X_.reset_index(drop=True)\n",
    "        data_X = base_prediction.drop(base_prediction.columns[0],axis=1)\n",
    "        data_X = pd.concat([data_X_,data_X],axis=1)\n",
    "        \n",
    "    elif restack == False:\n",
    "        data_X = base_prediction.drop(base_prediction.columns[0],axis=1)\n",
    "        \n",
    "    data_y = base_prediction[base_prediction.columns[0]]\n",
    "    \n",
    "    #Correlation matrix of base_prediction\n",
    "    base_prediction_cor = base_prediction.drop(base_prediction.columns[0],axis=1)\n",
    "    base_prediction_cor = base_prediction_cor.corr()\n",
    "    \n",
    "    #Meta Modeling Starts Here\n",
    "    \n",
    "    model = meta_model #this defines model to be used below as model = meta_model (as captured above)\n",
    "\n",
    "    kf = StratifiedKFold(fold, random_state=seed) #capturing fold requested by user\n",
    "\n",
    "    score_auc =np.empty((0,0))\n",
    "    score_acc =np.empty((0,0))\n",
    "    score_recall =np.empty((0,0))\n",
    "    score_precision =np.empty((0,0))\n",
    "    score_f1 =np.empty((0,0))\n",
    "    score_kappa =np.empty((0,0))\n",
    "    avgs_auc =np.empty((0,0))\n",
    "    avgs_acc =np.empty((0,0))\n",
    "    avgs_recall =np.empty((0,0))\n",
    "    avgs_precision =np.empty((0,0))\n",
    "    avgs_f1 =np.empty((0,0))\n",
    "    avgs_kappa =np.empty((0,0))\n",
    "    \n",
    "    for train_i , test_i in kf.split(data_X,data_y):\n",
    "        Xtrain,Xtest = data_X.iloc[train_i], data_X.iloc[test_i]\n",
    "        ytrain,ytest = data_y.iloc[train_i], data_y.iloc[test_i]\n",
    "\n",
    "        model.fit(Xtrain,ytrain)\n",
    "        pred_prob = model.predict_proba(Xtest)\n",
    "        pred_prob = pred_prob[:,1]\n",
    "        pred_ = model.predict(Xtest)\n",
    "        sca = metrics.accuracy_score(ytest,pred_)\n",
    "        sc = metrics.roc_auc_score(ytest,pred_prob)\n",
    "        recall = metrics.recall_score(ytest,pred_)\n",
    "        precision = metrics.average_precision_score(ytest,pred_prob)\n",
    "        kappa = metrics.cohen_kappa_score(ytest,pred_)\n",
    "        f1 = metrics.f1_score(ytest,pred_)\n",
    "        score_acc = np.append(score_acc,sca)\n",
    "        score_auc = np.append(score_auc,sc)\n",
    "        score_recall = np.append(score_recall,recall)\n",
    "        score_precision = np.append(score_precision,precision)\n",
    "        score_f1 =np.append(score_f1,f1)\n",
    "        score_kappa =np.append(score_kappa,kappa)\n",
    "     \n",
    "    mean_acc=np.mean(score_acc)\n",
    "    mean_auc=np.mean(score_auc)\n",
    "    mean_recall=np.mean(score_recall)\n",
    "    mean_precision=np.mean(score_precision)\n",
    "    mean_f1=np.mean(score_f1)\n",
    "    mean_kappa=np.mean(score_kappa)\n",
    "    std_acc=np.std(score_acc)\n",
    "    std_auc=np.std(score_auc)\n",
    "    std_recall=np.std(score_recall)\n",
    "    std_precision=np.std(score_precision)\n",
    "    std_f1=np.std(score_f1)\n",
    "    std_kappa=np.std(score_kappa)\n",
    "    \n",
    "    avgs_acc = np.append(avgs_acc, mean_acc)\n",
    "    avgs_acc = np.append(avgs_acc, std_acc) \n",
    "    avgs_auc = np.append(avgs_auc, mean_auc)\n",
    "    avgs_auc = np.append(avgs_auc, std_auc)\n",
    "    avgs_recall = np.append(avgs_recall, mean_recall)\n",
    "    avgs_recall = np.append(avgs_recall, std_recall)\n",
    "    avgs_precision = np.append(avgs_precision, mean_precision)\n",
    "    avgs_precision = np.append(avgs_precision, std_precision)\n",
    "    avgs_f1 = np.append(avgs_f1, mean_f1)\n",
    "    avgs_f1 = np.append(avgs_f1, std_f1)\n",
    "    avgs_kappa = np.append(avgs_kappa, mean_kappa)\n",
    "    avgs_kappa = np.append(avgs_kappa, std_kappa)\n",
    "      \n",
    "    model_results = pd.DataFrame({'Accuracy': score_acc, 'AUC': score_auc, 'Recall' : score_recall, 'Prec.' : score_precision , \n",
    "                     'F1' : score_f1, 'Kappa' : score_kappa})\n",
    "    model_avgs = pd.DataFrame({'Accuracy': avgs_acc, 'AUC': avgs_auc, 'Recall' : avgs_recall, 'Prec.' : avgs_precision , \n",
    "                     'F1' : avgs_f1, 'Kappa' : avgs_kappa},index=['Mean', 'SD'])\n",
    "  \n",
    "    model_results = model_results.append(model_avgs)\n",
    "    model_results = model_results.round(round)  \n",
    "    \n",
    "    models = []\n",
    "    for i in estimator_list:\n",
    "        models.append(i)\n",
    "    \n",
    "    models.append(meta_model)\n",
    "    \n",
    "    if plot:\n",
    "        ax = sns.heatmap(base_prediction_cor, vmin=-0.5, vmax=1, center=0,cmap='magma', square=True, annot=True, \n",
    "                         linewidths=1)\n",
    "    \n",
    "    else:\n",
    "        display(model_results)\n",
    "        return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stacknet(estimator_list,\n",
    "                    meta_model = None,\n",
    "                    fold = 10,\n",
    "                    round = 4,\n",
    "                    method = 'hard',\n",
    "                    restack = False):\n",
    "    \"\"\"\n",
    "     \n",
    "  Description:\n",
    "  ------------\n",
    "  This function creates a sequential stack net using cross validated predictions at\n",
    "  each layer. The final score grid is predictions from meta model using Stratified \n",
    "  Cross Validation. Base level models can be passed as estimator_list parameter, the\n",
    "  layers can be organized as a sub list within the estimator_list object. Restacking \n",
    "  parameter control the ability to expose raw features to meta model when set to True. \n",
    "  \n",
    "    Example:\n",
    "    --------\n",
    "    \n",
    "    nb = create_model( 'nb' )\n",
    "    rf = create_model( 'rf' )\n",
    "    ada = create_model( 'ada' )\n",
    "    ridge = create_model( 'ridge' )\n",
    "    knn = create_model( 'knn' )\n",
    "    \n",
    "    create_stacknet( [ [ nb, rf ], [ ada, ridge, knn] ] )\n",
    "    \n",
    "    This will result in stacking of models in multiple layers. The first layer \n",
    "    contains nb and rf, the predictions of which is used by models in second layer\n",
    "    to produce predictions which is used by meta model to generate final predictions.\n",
    "    By default meta model is Logistic Regression but can be changed with meta_model.\n",
    "    \n",
    "  Parameters\n",
    "  ----------\n",
    "  \n",
    "  estimator_list : nested list of object\n",
    "  \n",
    "  meta_model : object, default = None\n",
    "  if set to None, Logistic Regression is used as a meta model.\n",
    "\n",
    "  fold: integer, default = 10\n",
    "  Number of folds will determine how many folds would be done in the Kfold CV.\n",
    "  \n",
    "  round: integer, default = 4\n",
    "  The number indicates the number of decimal places metrics will be rounded to. \n",
    "\n",
    "  method: string, default = 'hard'\n",
    "  'hard', uses predicted class labels as input to meta model. \n",
    "  'soft', uses predicted probabilities as input to meta model.\n",
    "  \n",
    "  restack: Boolean, default = False\n",
    "  When restack is set to True, it will expose raw data to meta model.\n",
    "  \n",
    "  Attributes\n",
    "  ----------\n",
    "  All original attributes available in sklearn for a given estimator.\n",
    "  \n",
    "  Returns:\n",
    "  --------\n",
    "  \n",
    "  score grid:   A table containing the scores of the model across the kfolds. \n",
    "  -----------   Scoring metrics used are Accuracy, AUC, Recall, Precision, F1 \n",
    "                and Kappa. Mean and standard deviation of the scores across the \n",
    "                folds is also returned.\n",
    "  \n",
    "  model:        trained model object\n",
    "  -----------\n",
    "  \n",
    "  Warnings:\n",
    "  ---------\n",
    "  When estimator doesn't support 'predict_proba' (for example: ridge) and method is \n",
    "  forced to 'soft', stack_models() will return an error. \n",
    "  \n",
    "    \"\"\"\n",
    "\n",
    "    #dependencies\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn import metrics\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.model_selection import cross_val_predict\n",
    "    import sys\n",
    "    \n",
    "    #global base_array_df\n",
    "    \n",
    "    base_level = estimator_list[0]\n",
    "    inter_level = estimator_list[1:]\n",
    "    data_X = X_train\n",
    "    data_y = y_train\n",
    "    \n",
    "    #defining meta model\n",
    "    \n",
    "    if meta_model == None:\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        meta_model = LogisticRegression()\n",
    "    else:\n",
    "        meta_model = meta_model\n",
    "    \n",
    "    #Capturing the method of stacking required by user. method='soft' means 'predict_proba' else 'predict'\n",
    "    \n",
    "    if method == 'soft':\n",
    "        predict_method = 'predict_proba'\n",
    "    elif method == 'hard':\n",
    "        predict_method = 'predict'\n",
    "        \n",
    "        \n",
    "    base_array = np.zeros((0,0))\n",
    "    base_array_df = pd.DataFrame()\n",
    "    base_prediction = pd.DataFrame(y_train)\n",
    "    base_prediction = base_prediction.reset_index(drop=True)\n",
    "    \n",
    "    for model in base_level:\n",
    "                     \n",
    "        base_array = cross_val_predict(model,X_train,y_train,cv=fold, method=predict_method)\n",
    "        if method == 'soft':\n",
    "            base_array = base_array[:,1]\n",
    "        elif method == 'hard':\n",
    "            base_array = base_array\n",
    "        base_array = pd.DataFrame(base_array)\n",
    "        base_array_df = pd.concat([base_array_df, base_array], axis=1)\n",
    "        base_array = np.empty((0,0))  \n",
    "        \n",
    "    for level in inter_level:\n",
    "        \n",
    "        for model in level:\n",
    "            \n",
    "            base_array = cross_val_predict(model,base_array_df,base_prediction,cv=fold, method=predict_method)\n",
    "            if method == 'soft':\n",
    "                base_array = base_array[:,1]\n",
    "            elif method == 'hard':\n",
    "                base_array = base_array\n",
    "            base_array = pd.DataFrame(base_array)\n",
    "            base_array_df = pd.concat([base_array, base_array_df], axis=1)\n",
    "            base_array = np.empty((0,0))\n",
    "        \n",
    "        if restack == False:\n",
    "            base_array_df = base_array_df.iloc[:,:len(level)]\n",
    "        else:\n",
    "            base_array_df = base_array_df\n",
    "    \n",
    "    model = meta_model\n",
    "    \n",
    "    kf = StratifiedKFold(fold, random_state=seed) #capturing fold requested by user\n",
    "\n",
    "    score_auc =np.empty((0,0))\n",
    "    score_acc =np.empty((0,0))\n",
    "    score_recall =np.empty((0,0))\n",
    "    score_precision =np.empty((0,0))\n",
    "    score_f1 =np.empty((0,0))\n",
    "    score_kappa =np.empty((0,0))\n",
    "    avgs_auc =np.empty((0,0))\n",
    "    avgs_acc =np.empty((0,0))\n",
    "    avgs_recall =np.empty((0,0))\n",
    "    avgs_precision =np.empty((0,0))\n",
    "    avgs_f1 =np.empty((0,0))\n",
    "    avgs_kappa =np.empty((0,0))\n",
    "    \n",
    "    for train_i , test_i in kf.split(data_X,data_y):\n",
    "        \n",
    "        Xtrain,Xtest = data_X.iloc[train_i], data_X.iloc[test_i]\n",
    "        ytrain,ytest = data_y.iloc[train_i], data_y.iloc[test_i]\n",
    "\n",
    "        model.fit(Xtrain,ytrain)\n",
    "        pred_prob = model.predict_proba(Xtest)\n",
    "        pred_prob = pred_prob[:,1]\n",
    "        pred_ = model.predict(Xtest)\n",
    "        sca = metrics.accuracy_score(ytest,pred_)\n",
    "        sc = metrics.roc_auc_score(ytest,pred_prob)\n",
    "        recall = metrics.recall_score(ytest,pred_)\n",
    "        precision = metrics.average_precision_score(ytest,pred_prob)\n",
    "        kappa = metrics.cohen_kappa_score(ytest,pred_)\n",
    "        f1 = metrics.f1_score(ytest,pred_)\n",
    "        score_acc = np.append(score_acc,sca)\n",
    "        score_auc = np.append(score_auc,sc)\n",
    "        score_recall = np.append(score_recall,recall)\n",
    "        score_precision = np.append(score_precision,precision)\n",
    "        score_f1 =np.append(score_f1,f1)\n",
    "        score_kappa =np.append(score_kappa,kappa)\n",
    "     \n",
    "    mean_acc=np.mean(score_acc)\n",
    "    mean_auc=np.mean(score_auc)\n",
    "    mean_recall=np.mean(score_recall)\n",
    "    mean_precision=np.mean(score_precision)\n",
    "    mean_f1=np.mean(score_f1)\n",
    "    mean_kappa=np.mean(score_kappa)\n",
    "    std_acc=np.std(score_acc)\n",
    "    std_auc=np.std(score_auc)\n",
    "    std_recall=np.std(score_recall)\n",
    "    std_precision=np.std(score_precision)\n",
    "    std_f1=np.std(score_f1)\n",
    "    std_kappa=np.std(score_kappa)\n",
    "    \n",
    "    avgs_acc = np.append(avgs_acc, mean_acc)\n",
    "    avgs_acc = np.append(avgs_acc, std_acc) \n",
    "    avgs_auc = np.append(avgs_auc, mean_auc)\n",
    "    avgs_auc = np.append(avgs_auc, std_auc)\n",
    "    avgs_recall = np.append(avgs_recall, mean_recall)\n",
    "    avgs_recall = np.append(avgs_recall, std_recall)\n",
    "    avgs_precision = np.append(avgs_precision, mean_precision)\n",
    "    avgs_precision = np.append(avgs_precision, std_precision)\n",
    "    avgs_f1 = np.append(avgs_f1, mean_f1)\n",
    "    avgs_f1 = np.append(avgs_f1, std_f1)\n",
    "    avgs_kappa = np.append(avgs_kappa, mean_kappa)\n",
    "    avgs_kappa = np.append(avgs_kappa, std_kappa)\n",
    "      \n",
    "    model_results = pd.DataFrame({'Accuracy': score_acc, 'AUC': score_auc, 'Recall' : score_recall, 'Prec.' : score_precision , \n",
    "                     'F1' : score_f1, 'Kappa' : score_kappa})\n",
    "    model_avgs = pd.DataFrame({'Accuracy': avgs_acc, 'AUC': avgs_auc, 'Recall' : avgs_recall, 'Prec.' : avgs_precision , \n",
    "                     'F1' : avgs_f1, 'Kappa' : avgs_kappa},index=['Mean', 'SD'])\n",
    "  \n",
    "    model_results = model_results.append(model_avgs)\n",
    "    model_results = model_results.round(round)      \n",
    "    \n",
    "    display(model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_model(estimator,\n",
    "                   type = 'summary',\n",
    "                   feature = None, \n",
    "                   observation = 'All'):\n",
    "    \n",
    "    \"\"\"\n",
    "      \n",
    "  Description:\n",
    "  ------------\n",
    "  This function takes a trained model object and returns the interpretation plot on\n",
    "  test set. This function only supports tree based algorithm. \n",
    "  \n",
    "  This function is implemented based on original implementation in package 'shap'.\n",
    "  SHAP (SHapley Additive exPlanations) is a unified approach to explain the output \n",
    "  of any machine learning model. SHAP connects game theory with local explanations.\n",
    "  \n",
    "  For more information : https://shap.readthedocs.io/en/latest/\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    \n",
    "    dt = create_model('dt')\n",
    "    interpret_model(dt)\n",
    "    \n",
    "    This will return the summary interpretation plot of Decision Tree model.\n",
    "  \n",
    "  Parameters\n",
    "  ----------\n",
    "  \n",
    "  estimator : object, default=none\n",
    "  \n",
    "  A trained tree based model object should be passed as an estimator. \n",
    "  Model must be created using create_model() or tune_model() in pycaret or using \n",
    "  any other package that returns sklearn object.\n",
    "  \n",
    "  type : string, default = 'summary'\n",
    "  other available options are 'dependence' and 'prediction'.\n",
    "  \n",
    "  feature: string, default = None\n",
    "  This parameter is only needed when type = 'dependence'. By default feature is set\n",
    "  to None which means the first column of dataset will be used as a variable. \n",
    "  To change feature param must be passed. \n",
    "  \n",
    "  observation: integer or string (when set to 'All'), default = 'All'\n",
    "  This parameter is only needed when type = 'prediction'. By default the plot \n",
    "  will  return the analysis for all observations with option to select the feature \n",
    "  on x and y axis through drop down interactivity. For analysis of individual\n",
    "  observation, observation parameter must be passed with index value of \n",
    "  observation in test set. \n",
    "\n",
    "  Returns:\n",
    "  --------\n",
    "  \n",
    "  Visual Plot:  Returns the visual plot.\n",
    "                Returns the interactive JS plot when type = 'prediction'.\n",
    "              \n",
    "  Warnings:\n",
    "  ---------\n",
    "  None    \n",
    "    \n",
    "    \"\"\"\n",
    "    model = estimator\n",
    "    model_name = str(model).split(\"(\")[0]\n",
    "    \n",
    "    #dependencies\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import shap\n",
    "    import sys\n",
    "    #shap.initjs()\n",
    "    \n",
    "    #allowed models\n",
    "    allowed_models = ['RandomForestClassifier',\n",
    "                      'DecisionTreeClassifier',\n",
    "                      'ExtraTreesClassifier',\n",
    "                      'GradientBoostingClassifier']\n",
    "    \n",
    "    #defining type of classifier\n",
    "    type1 = ['RandomForestClassifier','DecisionTreeClassifier','ExtraTreesClassifier']\n",
    "    type2 = ['GradientBoostingClassifier']\n",
    "    \n",
    "    #chcecking if model passed is acceptable or not\n",
    "    if model_name not in allowed_models:\n",
    "        sys.exit('Not Allowed')\n",
    "    \n",
    "    if type == 'summary':\n",
    "        \n",
    "        if model_name in type1:\n",
    "        \n",
    "            explainer = shap.TreeExplainer(model)\n",
    "            shap_values = explainer.shap_values(X_test)\n",
    "            shap.summary_plot(shap_values, X_test)\n",
    "            \n",
    "        elif model_name in type2:\n",
    "            \n",
    "            explainer = shap.TreeExplainer(model)\n",
    "            shap_values = explainer.shap_values(X_test)\n",
    "            shap.summary_plot(shap_values, X_test)\n",
    "                              \n",
    "    elif type == 'dependence':\n",
    "        \n",
    "        if feature == None:\n",
    "            \n",
    "            dependence = X_test.columns[0]\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            dependence = feature\n",
    "        \n",
    "        if model_name in type1:\n",
    "                \n",
    "            explainer = shap.TreeExplainer(model)\n",
    "            shap_values = explainer.shap_values(X_test)\n",
    "            shap.dependence_plot(dependence, shap_values[1], X_test)\n",
    "        \n",
    "        elif model_name in type2:\n",
    "            \n",
    "            explainer = shap.TreeExplainer(model)\n",
    "            shap_values = explainer.shap_values(X_test) \n",
    "            shap.dependence_plot(dependence, shap_values, X_test)\n",
    "        \n",
    "    elif type == 'prediction':\n",
    "        \n",
    "        if model_name in type1:\n",
    "            \n",
    "            if observation == 'All':\n",
    "                \n",
    "                explainer = shap.TreeExplainer(model)\n",
    "                shap_values = explainer.shap_values(X_test)\n",
    "                shap.initjs()\n",
    "                return shap.force_plot(explainer.expected_value[1], shap_values[1], X_test)\n",
    "            \n",
    "            else: \n",
    "                \n",
    "                row_to_show = observation\n",
    "                data_for_prediction = X_test.iloc[row_to_show]\n",
    "                explainer = shap.TreeExplainer(model)\n",
    "                shap_values = explainer.shap_values(data_for_prediction)\n",
    "                shap.initjs()\n",
    "                return shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)        \n",
    "\n",
    "            \n",
    "        elif model_name in type2:\n",
    "\n",
    "            if observation == 'All':\n",
    "                \n",
    "                explainer = shap.TreeExplainer(model)\n",
    "                shap_values = explainer.shap_values(X_test)\n",
    "                shap.initjs()\n",
    "                return shap.force_plot(explainer.expected_value, shap_values, X_test)\n",
    "            \n",
    "            else: \n",
    "                \n",
    "                explainer = shap.TreeExplainer(model)\n",
    "                shap_values = explainer.shap_values(X_test)\n",
    "                shap.initjs()\n",
    "                return shap.force_plot(explainer.expected_value, shap_values[0,:], X_test.iloc[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automl(qualifier = 5,\n",
    "           target_metric = 'Accuracy',\n",
    "           fold = 10, \n",
    "           round = 4):\n",
    "    \n",
    "    \"\"\"\n",
    "      \n",
    "  Description:\n",
    "  ------------\n",
    "  This function is an original implementation of pycaret. It sequentially creates\n",
    "  various model and apply different techniques for Ensembling and Stacking. It returns\n",
    "  the best model based on 'target_metric' parameter defined. To limit the processing\n",
    "  time, 'qualifier' param can be reduced (by default = 5).  \n",
    "  \n",
    "    Example:\n",
    "    --------\n",
    "    \n",
    "    automl = automl()\n",
    "    \n",
    "    ** All parameters are optional\n",
    "    \n",
    "  Parameters\n",
    "  ----------\n",
    "  \n",
    "  qualifier : integer, default = None\n",
    "  Number of top models considered for further processing to return the best model.\n",
    "  Higher number will result in longer process times.\n",
    "  \n",
    "  target_metric : String, default = 'Accuracy'\n",
    "  Metric to use for qualifying models and tuning the hyperparameters.\n",
    "\n",
    "  fold: integer, default = 10\n",
    "  Number of folds will determine how many folds would be done in the Kfold CV.\n",
    "  \n",
    "  round: integer, default = 4\n",
    "  The number indicates the number of decimal places metrics will be rounded to. \n",
    "\n",
    "  Attributes\n",
    "  ----------\n",
    "  All original attributes available in sklearn for a given estimator.\n",
    "  \n",
    "  Returns:\n",
    "  --------\n",
    "  \n",
    "  score grid:   A table containing the averaged Kfold scores of all the models\n",
    "  -----------   Scoring metrics used are Accuracy, AUC, Recall, Precision, F1 \n",
    "                and Kappa. \n",
    "  \n",
    "  model:        trained model object (best model selected using target metric param)\n",
    "  -----------\n",
    "  \n",
    "  Warnings:\n",
    "  ---------\n",
    "  None\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #base dependencies\n",
    "    from IPython.display import clear_output\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import random\n",
    "    import sys\n",
    "    \n",
    "    #master collector\n",
    "    #This is being used for appending throughout the process 1/N\n",
    "    global master, master_results\n",
    "    master = []\n",
    "    master_results = pd.DataFrame(columns=['Model', 'Accuracy','AUC','Recall', 'Prec.', 'F1', 'Kappa'])\n",
    "    #master_display = master_results\n",
    "    \n",
    "    #progress bar\n",
    "    import ipywidgets as ipw\n",
    "    progress = ipw.IntProgress(value=0, min=0, max=12, step=1 , description='Processing: ')\n",
    "    display(progress)\n",
    "    display(master_results)\n",
    "    \n",
    "    #automl parameters to be used in this function\n",
    "    top_n = qualifier #top_n candidates for processing\n",
    "    \n",
    "    if target_metric == 'Accuracy':\n",
    "        optimize = target_metric.lower()\n",
    "        sort = 'Accuracy'\n",
    "        \n",
    "    elif target_metric == 'AUC':\n",
    "        optimize = 'roc_auc'\n",
    "        sort = 'AUC'     \n",
    "        \n",
    "    elif target_metric == 'Recall':\n",
    "        optimize = target_metric.lower()\n",
    "        sort = 'Recall'        \n",
    "\n",
    "    elif target_metric == 'Precision':\n",
    "        optimize = target_metric.lower()\n",
    "        sort = 'Prec.'\n",
    "   \n",
    "    elif target_metric == 'F1':\n",
    "        optimize = target_metric.lower()\n",
    "        sort = 'F1'\n",
    "        \n",
    "    elif target_metric == 'Kappa':\n",
    "        optimize = 'roc_auc'\n",
    "        sort = 'Kappa'\n",
    "        \n",
    "    n_iter = 10 #number of iteration for tuning\n",
    "    \n",
    "    #ignore warnings\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore') \n",
    "\n",
    "    #defining X_train and y_train\n",
    "    data_X = X_train\n",
    "    data_y=y_train\n",
    "    \n",
    "    #sklearn dependencies\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.gaussian_process.kernels import RBF\n",
    "    from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.linear_model import RidgeClassifier\n",
    "    from sklearn.linear_model import Lasso\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.linear_model import SGDClassifier\n",
    "    from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import ExtraTreesClassifier    \n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    \n",
    "    #sklearn ensembling dependencies\n",
    "    from sklearn.ensemble import BaggingClassifier\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.ensemble import VotingClassifier\n",
    "    \n",
    "    #other imports from sklearn\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    from sklearn.model_selection import cross_val_predict\n",
    "    from sklearn import metrics\n",
    "    \n",
    "    #create sklearn model objects\n",
    "    lr = LogisticRegression(random_state=seed)\n",
    "    knn = KNeighborsClassifier()\n",
    "    nb = GaussianNB()\n",
    "    dt = DecisionTreeClassifier(random_state=seed)\n",
    "    svm = SGDClassifier(max_iter=1000, tol=0.001, random_state=seed)\n",
    "    rbfsvm = SVC(gamma='auto', C=1, probability=True, kernel='rbf', random_state=seed)\n",
    "    gpc = GaussianProcessClassifier(random_state=seed)\n",
    "    mlp = MLPClassifier(max_iter=500, random_state=seed)\n",
    "    ridge = RidgeClassifier(random_state=seed)\n",
    "    rf = RandomForestClassifier(n_estimators=10, random_state=seed)\n",
    "    qda = QuadraticDiscriminantAnalysis()\n",
    "    ada = AdaBoostClassifier(random_state=seed)\n",
    "    gbc = GradientBoostingClassifier(random_state=seed)\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    et = ExtraTreesClassifier(random_state=seed)\n",
    "    \n",
    "    #defining model library \n",
    "    model_library = [lr, knn, nb, dt, svm, rbfsvm, gpc, mlp, ridge, rf, qda, ada, gbc, lda, et]\n",
    "\n",
    "    #defining model names\n",
    "    model_names = []\n",
    "\n",
    "    for names in model_library:\n",
    "        model_names = np.append(model_names, str(names).split(\"(\")[0])\n",
    "    \n",
    "    \n",
    "    progress.value += 1\n",
    "    \n",
    "    '''\n",
    "    Step 1 - Run all the models in model library.\n",
    "    This function is equivalent to compare_models() without any blacklist model\n",
    "\n",
    "    '''\n",
    "    #cross validation\n",
    "    kf = StratifiedKFold(fold, random_state=seed)\n",
    "\n",
    "    score_acc =np.empty((0,0))\n",
    "    score_auc =np.empty((0,0))\n",
    "    score_recall =np.empty((0,0))\n",
    "    score_precision =np.empty((0,0))\n",
    "    score_f1 =np.empty((0,0))\n",
    "    score_kappa =np.empty((0,0))\n",
    "    score_acc_running = np.empty((0,0)) ##running total\n",
    "    avg_acc = np.empty((0,0))\n",
    "    avg_auc = np.empty((0,0))\n",
    "    avg_recall = np.empty((0,0))\n",
    "    avg_precision = np.empty((0,0))\n",
    "    avg_f1 = np.empty((0,0))\n",
    "    avg_kappa = np.empty((0,0))\n",
    "      \n",
    "    for model in model_library:\n",
    " \n",
    "        for train_i , test_i in kf.split(data_X,data_y):\n",
    "     \n",
    "            Xtrain,Xtest = data_X.iloc[train_i], data_X.iloc[test_i]\n",
    "            ytrain,ytest = data_y.iloc[train_i], data_y.iloc[test_i]\n",
    "        \n",
    "            if hasattr(model, 'predict_proba'):               \n",
    "        \n",
    "                model.fit(Xtrain,ytrain)\n",
    "                pred_prob = model.predict_proba(Xtest)\n",
    "                pred_prob = pred_prob[:,1]\n",
    "                pred_ = model.predict(Xtest)\n",
    "                sca = metrics.accuracy_score(ytest,pred_)\n",
    "                sc = metrics.roc_auc_score(ytest,pred_prob)\n",
    "                recall = metrics.recall_score(ytest,pred_)\n",
    "                precision = metrics.precision_score(ytest,pred_)\n",
    "                kappa = metrics.cohen_kappa_score(ytest,pred_)\n",
    "                f1 = metrics.f1_score(ytest,pred_)\n",
    "                score_acc = np.append(score_acc,sca)\n",
    "                score_auc = np.append(score_auc,sc)\n",
    "                score_recall = np.append(score_recall,recall)\n",
    "                score_precision = np.append(score_precision,precision)\n",
    "                score_f1 =np.append(score_f1,f1)\n",
    "                score_kappa =np.append(score_kappa,kappa)              \n",
    "        \n",
    "            else:        \n",
    "\n",
    "                model.fit(Xtrain,ytrain)\n",
    "                pred_prob = 0.00\n",
    "                pred_prob = 0.00\n",
    "                pred_ = model.predict(Xtest)\n",
    "                sca = metrics.accuracy_score(ytest,pred_)\n",
    "                sc = 0.00\n",
    "                recall = metrics.recall_score(ytest,pred_)\n",
    "                precision = metrics.precision_score(ytest,pred_) #change pred_prob to pred_\n",
    "                kappa = metrics.cohen_kappa_score(ytest,pred_)\n",
    "                f1 = metrics.f1_score(ytest,pred_)\n",
    "                score_acc = np.append(score_acc,sca)\n",
    "                score_auc = np.append(score_auc,sc)\n",
    "                score_recall = np.append(score_recall,recall)\n",
    "                score_precision = np.append(score_precision,precision)\n",
    "                score_f1 =np.append(score_f1,f1)\n",
    "                score_kappa =np.append(score_kappa,kappa) \n",
    "        \n",
    "        avg_acc = np.append(avg_acc,np.mean(score_acc))\n",
    "        avg_auc = np.append(avg_auc,np.mean(score_auc))\n",
    "        avg_recall = np.append(avg_recall,np.mean(score_recall))\n",
    "        avg_precision = np.append(avg_precision,np.mean(score_precision))\n",
    "        avg_f1 = np.append(avg_f1,np.mean(score_f1))\n",
    "        avg_kappa = np.append(avg_kappa,np.mean(score_kappa))\n",
    "        score_acc =np.empty((0,0))\n",
    "        score_auc =np.empty((0,0))\n",
    "        score_recall =np.empty((0,0))\n",
    "        score_precision =np.empty((0,0))\n",
    "        score_f1 =np.empty((0,0))\n",
    "        score_kappa =np.empty((0,0))\n",
    "\n",
    "    compare_models_ = pd.DataFrame({'Model':model_names, 'Accuracy':avg_acc, 'AUC':avg_auc, \n",
    "                     'Recall':avg_recall, 'Prec.':avg_precision, \n",
    "                     'F1':avg_f1, 'Kappa': avg_kappa})\n",
    "    \n",
    "    compare_models_ = compare_models_.sort_values(by=sort, ascending=False).reset_index(drop=True)\n",
    "    compare_models_ = compare_models_.round(round)\n",
    "    \n",
    "    top_n_model_names = list(compare_models_.iloc[0:top_n]['Model'])  #DO NOT DELETE - IT IS USED BELOW\n",
    "    top_n_model_results = compare_models_[:top_n] #DO NOT DELETE - IT IS USED - IT IS USED TO APPEND TO MASTER_RESULTS\n",
    "    master_results = master_results.append(top_n_model_results)\n",
    "    \n",
    "    progress.value += 1\n",
    "    clear_output()\n",
    "    master_display = master_results.sort_values(by=sort,ascending=False)\n",
    "    master_display.reset_index(drop=True, inplace=True)\n",
    "    display(progress)\n",
    "    display(master_display)\n",
    "    \n",
    "    '''\n",
    "\n",
    "    The section below is still part of Step 1. The purpose of this chunk is to \n",
    "    take the name string from 'top_n_model_names' and create a model that is being\n",
    "    appended to master list. Models are re-created (In future, re-creation must be\n",
    "    replaced by already created object for efficiency purpose).\n",
    "    \n",
    "    '''\n",
    "    top_n_models = []\n",
    "    \n",
    "    for i in top_n_model_names:\n",
    "        \n",
    "        if i == 'LinearDiscriminantAnalysis':\n",
    "            \n",
    "            model = LinearDiscriminantAnalysis()\n",
    "            top_n_models.append(model)\n",
    "            \n",
    "        elif i == 'LogisticRegression':\n",
    "            \n",
    "            model = LogisticRegression(random_state=seed)\n",
    "            top_n_models.append(model)\n",
    "            \n",
    "        elif i == 'GradientBoostingClassifier':\n",
    "            \n",
    "            model = GradientBoostingClassifier(random_state=seed)\n",
    "            top_n_models.append(model)\n",
    "            \n",
    "        elif i == 'AdaBoostClassifier':\n",
    "            \n",
    "            model =  AdaBoostClassifier(random_state=seed)           \n",
    "            top_n_models.append(model)\n",
    "            \n",
    "        elif i == 'MLPClassifier':\n",
    "            \n",
    "            model = MLPClassifier(max_iter=500, random_state=seed)\n",
    "            top_n_models.append(model)\n",
    "            \n",
    "        elif i == 'RandomForestClassifier':\n",
    "            \n",
    "            model = RandomForestClassifier(n_estimators=10, random_state=seed)\n",
    "            top_n_models.append(model)\n",
    "            \n",
    "        elif i == 'GaussianNB':\n",
    "            \n",
    "            model = GaussianNB()\n",
    "            top_n_models.append(model)\n",
    "            \n",
    "        elif i == 'DecisionTreeClassifier':\n",
    "            \n",
    "            model = DecisionTreeClassifier(random_state=seed)\n",
    "            top_n_models.append(model)\n",
    "            \n",
    "        elif i == 'ExtraTreesClassifier':\n",
    "            \n",
    "            model = ExtraTreesClassifier(random_state=seed)\n",
    "            top_n_models.append(model)\n",
    "            \n",
    "        elif i == 'SVC':\n",
    "            \n",
    "            model = SVC(gamma='auto', C=1, probability=True, kernel='rbf', random_state=seed)\n",
    "            top_n_models.append(model)\n",
    "            \n",
    "        elif i == 'KNeighborsClassifier':\n",
    "            \n",
    "            model = KNeighborsClassifier()\n",
    "            top_n_models.append(model)\n",
    "            \n",
    "        elif i == 'GaussianProcessClassifier':\n",
    "            \n",
    "            model = GaussianProcessClassifier(random_state=seed)\n",
    "            top_n_models.append(model)\n",
    "            \n",
    "        elif i == 'QuadraticDiscriminantAnalysis':\n",
    "            \n",
    "            model = QuadraticDiscriminantAnalysis()\n",
    "            top_n_models.append(model)\n",
    "            \n",
    "        elif i == 'SGDClassifier':\n",
    "            \n",
    "            model = SGDClassifier(max_iter=1000, tol=0.001, random_state=seed)\n",
    "            top_n_models.append(model)\n",
    "            \n",
    "        elif i == 'RidgeClassifier':\n",
    "            \n",
    "            model = RidgeClassifier(random_state=seed)\n",
    "            top_n_models.append(model)\n",
    "    \n",
    "    master.append(top_n_models) #appending top_n models to master list\n",
    "    \n",
    "    progress.value += 1\n",
    "    clear_output()\n",
    "    master_display = master_results.sort_values(by=sort,ascending=False)\n",
    "    master_display.reset_index(drop=True, inplace=True)\n",
    "    display(progress)\n",
    "    display(master_display)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Step 2 - Create Ensemble Bagging using BaggingClassifier() from sklearn for all the \n",
    "    models in 'top_n_models' param defined above. Number of models at this stage in \n",
    "    'top_n_models' param is equal to # of models in 'master' param.\n",
    "    \n",
    "    This function is equivalent to ensemble_model().\n",
    "    \n",
    "    '''    \n",
    "\n",
    "    top_n_bagged_models = []\n",
    "    top_n_bagged_model_results = pd.DataFrame(columns=['Model', 'Accuracy','AUC','Recall', 'Prec.', 'F1', 'Kappa'])\n",
    "    \n",
    "    #defining names\n",
    "    bagging_model_names = []\n",
    "    for i in top_n_model_names:\n",
    "        s = 'Bagging ' + i\n",
    "        bagging_model_names.append(s)\n",
    "    \n",
    "    #counter for naming\n",
    "    name_counter = 0 \n",
    "    \n",
    "    for i in top_n_models:\n",
    "       \n",
    "        #from sklearn.ensemble import BaggingClassifier\n",
    "        model = BaggingClassifier(i,bootstrap=True,n_estimators=10, random_state=seed)\n",
    "        top_n_bagged_models.append(model)\n",
    "    \n",
    "        #setting cross validation\n",
    "        kf = StratifiedKFold(fold, random_state=seed)\n",
    "\n",
    "        score_auc =np.empty((0,0))\n",
    "        score_acc =np.empty((0,0))\n",
    "        score_recall =np.empty((0,0))\n",
    "        score_precision =np.empty((0,0))\n",
    "        score_f1 =np.empty((0,0))\n",
    "        score_kappa =np.empty((0,0))\n",
    "        avgs_auc =np.empty((0,0))\n",
    "        avgs_acc =np.empty((0,0))\n",
    "        avgs_recall =np.empty((0,0))\n",
    "        avgs_precision =np.empty((0,0))\n",
    "        avgs_f1 =np.empty((0,0))\n",
    "        avgs_kappa =np.empty((0,0))\n",
    "        \n",
    "        for train_i , test_i in kf.split(data_X,data_y):\n",
    "    \n",
    "            Xtrain,Xtest = data_X.iloc[train_i], data_X.iloc[test_i]\n",
    "            ytrain,ytest = data_y.iloc[train_i], data_y.iloc[test_i]\n",
    "        \n",
    "            if hasattr(model, 'predict_proba'):\n",
    "        \n",
    "                model.fit(Xtrain,ytrain)\n",
    "                pred_prob = model.predict_proba(Xtest)\n",
    "                pred_prob = pred_prob[:,1]\n",
    "                pred_ = model.predict(Xtest)\n",
    "                sca = metrics.accuracy_score(ytest,pred_)\n",
    "                sc = metrics.roc_auc_score(ytest,pred_prob)\n",
    "                recall = metrics.recall_score(ytest,pred_)\n",
    "                precision = metrics.precision_score(ytest,pred_)\n",
    "                kappa = metrics.cohen_kappa_score(ytest,pred_)\n",
    "                f1 = metrics.f1_score(ytest,pred_)\n",
    "                score_acc = np.append(score_acc,sca)\n",
    "                score_auc = np.append(score_auc,sc)\n",
    "                score_recall = np.append(score_recall,recall)\n",
    "                score_precision = np.append(score_precision,precision)\n",
    "                score_f1 =np.append(score_f1,f1)\n",
    "                score_kappa =np.append(score_kappa,kappa)\n",
    "\n",
    "            else:\n",
    "            \n",
    "                model.fit(Xtrain,ytrain)\n",
    "                pred_prob = 0.00\n",
    "                pred_prob = 0.00\n",
    "                pred_ = model.predict(Xtest)\n",
    "                sca = metrics.accuracy_score(ytest,pred_)\n",
    "                sc = 0.00\n",
    "                recall = metrics.recall_score(ytest,pred_)\n",
    "                precision = metrics.precision_score(ytest,pred_) #change pred_prob to pred_\n",
    "                kappa = metrics.cohen_kappa_score(ytest,pred_)\n",
    "                f1 = metrics.f1_score(ytest,pred_)\n",
    "                score_acc = np.append(score_acc,sca)\n",
    "                score_auc = np.append(score_auc,sc)\n",
    "                score_recall = np.append(score_recall,recall)\n",
    "                score_precision = np.append(score_precision,precision)\n",
    "                score_f1 =np.append(score_f1,f1)\n",
    "                score_kappa =np.append(score_kappa,kappa) \n",
    "       \n",
    "        mean_acc=np.mean(score_acc)\n",
    "        mean_auc=np.mean(score_auc)\n",
    "        mean_recall=np.mean(score_recall)\n",
    "        mean_precision=np.mean(score_precision)\n",
    "        mean_f1=np.mean(score_f1)\n",
    "        mean_kappa=np.mean(score_kappa)\n",
    "\n",
    "        avgs_acc = np.append(avgs_acc, mean_acc)\n",
    "        avgs_auc = np.append(avgs_auc, mean_auc)\n",
    "        avgs_recall = np.append(avgs_recall, mean_recall)\n",
    "        avgs_precision = np.append(avgs_precision, mean_precision)\n",
    "        avgs_f1 = np.append(avgs_f1, mean_f1)\n",
    "        avgs_kappa = np.append(avgs_kappa, mean_kappa)\n",
    "        \n",
    "        #model_name = 'Bagging' + str(i).split(\"(\")[0]\n",
    "        model_results = pd.DataFrame({'Model': bagging_model_names[name_counter], 'Accuracy': avgs_acc, 'AUC': avgs_auc, \n",
    "                                      'Recall' : avgs_recall, 'Prec.' : avgs_precision , 'F1' : avgs_f1, \n",
    "                                      'Kappa' : avgs_kappa}).reset_index(drop=True)\n",
    "        model_results = model_results.round(round)\n",
    "        name_counter += 1\n",
    "        top_n_bagged_model_results = pd.concat([top_n_bagged_model_results, model_results],ignore_index=True)\n",
    "        \n",
    "    master_results = master_results.append(top_n_bagged_model_results)\n",
    "    master.append(top_n_bagged_models) \n",
    "    \n",
    "    progress.value += 1\n",
    "    clear_output()\n",
    "    master_display = master_results.sort_values(by=sort,ascending=False)\n",
    "    master_display.reset_index(drop=True, inplace=True)\n",
    "    display(progress)\n",
    "    display(master_display)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Step 3 - Create Ensemble Boosting using AdaBoostClassifier() from sklearn for all the \n",
    "    models in 'top_n_models' param defined above. \n",
    "    \n",
    "    This function is equivalent to ensemble_model(method = 'Boosting').\n",
    "    \n",
    "    '''        \n",
    "    \n",
    "    top_n_boosted_models = []\n",
    "    top_n_boosted_model_results = pd.DataFrame(columns=['Model','Accuracy','AUC','Recall', 'Prec.', 'F1', 'Kappa'])\n",
    "    \n",
    "    boosting_model_names = []\n",
    "    for i in top_n_model_names:\n",
    "        s = 'Boosting ' + i\n",
    "        boosting_model_names.append(s)\n",
    "     \n",
    "    #counter for naming\n",
    "    name_counter = 0 \n",
    "        \n",
    "    for i in top_n_models:\n",
    "       \n",
    "        if hasattr(i,'predict_proba') and hasattr(i,'class_weight'):\n",
    "            model = AdaBoostClassifier(i, random_state=seed)\n",
    "            top_n_boosted_models.append(model)\n",
    "            \n",
    "        else:\n",
    "            model = i\n",
    "            top_n_boosted_models.append(model)\n",
    "    \n",
    "        #setting cross validation\n",
    "        kf = StratifiedKFold(fold, random_state=seed)\n",
    "\n",
    "        score_auc =np.empty((0,0))\n",
    "        score_acc =np.empty((0,0))\n",
    "        score_recall =np.empty((0,0))\n",
    "        score_precision =np.empty((0,0))\n",
    "        score_f1 =np.empty((0,0))\n",
    "        score_kappa =np.empty((0,0))\n",
    "        avgs_auc =np.empty((0,0))\n",
    "        avgs_acc =np.empty((0,0))\n",
    "        avgs_recall =np.empty((0,0))\n",
    "        avgs_precision =np.empty((0,0))\n",
    "        avgs_f1 =np.empty((0,0))\n",
    "        avgs_kappa =np.empty((0,0))\n",
    "        \n",
    "        for train_i , test_i in kf.split(data_X,data_y):\n",
    "    \n",
    "            Xtrain,Xtest = data_X.iloc[train_i], data_X.iloc[test_i]\n",
    "            ytrain,ytest = data_y.iloc[train_i], data_y.iloc[test_i]\n",
    "        \n",
    "            if hasattr(model, 'predict_proba'):\n",
    "        \n",
    "                model.fit(Xtrain,ytrain)\n",
    "                pred_prob = model.predict_proba(Xtest)\n",
    "                pred_prob = pred_prob[:,1]\n",
    "                pred_ = model.predict(Xtest)\n",
    "                sca = metrics.accuracy_score(ytest,pred_)\n",
    "                sc = metrics.roc_auc_score(ytest,pred_prob)\n",
    "                recall = metrics.recall_score(ytest,pred_)\n",
    "                precision = metrics.precision_score(ytest,pred_)\n",
    "                kappa = metrics.cohen_kappa_score(ytest,pred_)\n",
    "                f1 = metrics.f1_score(ytest,pred_)\n",
    "                score_acc = np.append(score_acc,sca)\n",
    "                score_auc = np.append(score_auc,sc)\n",
    "                score_recall = np.append(score_recall,recall)\n",
    "                score_precision = np.append(score_precision,precision)\n",
    "                score_f1 =np.append(score_f1,f1)\n",
    "                score_kappa =np.append(score_kappa,kappa)\n",
    "\n",
    "            else:\n",
    "            \n",
    "                model.fit(Xtrain,ytrain)\n",
    "                pred_prob = 0.00\n",
    "                pred_prob = 0.00\n",
    "                pred_ = model.predict(Xtest)\n",
    "                sca = metrics.accuracy_score(ytest,pred_)\n",
    "                sc = 0.00\n",
    "                recall = metrics.recall_score(ytest,pred_)\n",
    "                precision = metrics.precision_score(ytest,pred_) #change pred_prob to pred_\n",
    "                kappa = metrics.cohen_kappa_score(ytest,pred_)\n",
    "                f1 = metrics.f1_score(ytest,pred_)\n",
    "                score_acc = np.append(score_acc,sca)\n",
    "                score_auc = np.append(score_auc,sc)\n",
    "                score_recall = np.append(score_recall,recall)\n",
    "                score_precision = np.append(score_precision,precision)\n",
    "                score_f1 =np.append(score_f1,f1)\n",
    "                score_kappa =np.append(score_kappa,kappa) \n",
    "       \n",
    "        mean_acc=np.mean(score_acc)\n",
    "        mean_auc=np.mean(score_auc)\n",
    "        mean_recall=np.mean(score_recall)\n",
    "        mean_precision=np.mean(score_precision)\n",
    "        mean_f1=np.mean(score_f1)\n",
    "        mean_kappa=np.mean(score_kappa)\n",
    "\n",
    "        avgs_acc = np.append(avgs_acc, mean_acc)\n",
    "        avgs_auc = np.append(avgs_auc, mean_auc)\n",
    "        avgs_recall = np.append(avgs_recall, mean_recall)\n",
    "        avgs_precision = np.append(avgs_precision, mean_precision)\n",
    "        avgs_f1 = np.append(avgs_f1, mean_f1)\n",
    "        avgs_kappa = np.append(avgs_kappa, mean_kappa)\n",
    "        \n",
    "        #model_name = 'Boosting' + str(i).split(\"(\")[0]\n",
    "        model_results = pd.DataFrame({'Model': boosting_model_names[name_counter],'Accuracy': avgs_acc, \n",
    "                                      'AUC': avgs_auc, 'Recall' : avgs_recall, 'Prec.' : avgs_precision, \n",
    "                                      'F1' : avgs_f1, 'Kappa' : avgs_kappa}).reset_index(drop=True)\n",
    "        model_results = model_results.round(round)\n",
    "        name_counter += 1\n",
    "        top_n_boosted_model_results = pd.concat([top_n_boosted_model_results, model_results],ignore_index=True)\n",
    "        \n",
    "    master_results = master_results.append(top_n_boosted_model_results)\n",
    "    master.append(top_n_boosted_models)\n",
    "    \n",
    "    progress.value += 1\n",
    "    clear_output()\n",
    "    master_display = master_results.sort_values(by=sort,ascending=False)\n",
    "    master_display.reset_index(drop=True, inplace=True)\n",
    "    display(progress)\n",
    "    display(master_display) \n",
    "    \n",
    "    '''\n",
    "\n",
    "    Step 4 - Tune all models in 'top_n_models' param defined in Step 1 above.\n",
    "    This function is equivalent to tune_model().\n",
    "\n",
    "\n",
    "    '''           \n",
    "    \n",
    "    #4.1 Store tuned model objects in the list 'top_n_tuned_models'\n",
    "    \n",
    "    top_n_tuned_models = []\n",
    "    \n",
    "    for i in top_n_model_names:\n",
    "        \n",
    "        if i == 'RidgeClassifier':\n",
    "            \n",
    "            param_grid = {'alpha': [0.0001,0.001,0.1,0.15,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],\n",
    "                          'fit_intercept': [True, False],\n",
    "                          'normalize': [True, False]\n",
    "                         }    \n",
    "\n",
    "            model_grid = RandomizedSearchCV(estimator=RidgeClassifier(random_state=seed), \n",
    "                                            param_distributions=param_grid, scoring=optimize, n_iter=n_iter, \n",
    "                                            cv=fold, random_state=seed, n_jobs=-1)\n",
    "\n",
    "            model_grid.fit(X_train,y_train)\n",
    "            model = model_grid.best_estimator_\n",
    "            best_model = model_grid.best_estimator_\n",
    "            best_model_param = model_grid.best_params_\n",
    "            top_n_tuned_models.append(best_model)\n",
    "        \n",
    "        elif i == 'LogisticRegression':\n",
    "            \n",
    "            param_grid = {'C': [1,5,10,25,50,100],\n",
    "                      \"penalty\": [ 'l1', 'l2'],\n",
    "                      \"class_weight\": [\"balanced\", None]\n",
    "                         }\n",
    "            model_grid = RandomizedSearchCV(estimator=LogisticRegression(random_state=seed), \n",
    "                                            param_distributions=param_grid, scoring=optimize, n_iter=n_iter, cv=fold, \n",
    "                                            random_state=seed, iid=False,n_jobs=-1)\n",
    "            model_grid.fit(X_train,y_train)\n",
    "            model = model_grid.best_estimator_\n",
    "            best_model = model_grid.best_estimator_\n",
    "            best_model_param = model_grid.best_params_\n",
    "            top_n_tuned_models.append(best_model)\n",
    "        \n",
    "        elif i == 'GradientBoostingClassifier':\n",
    "            \n",
    "            param_grid = {'loss': ['deviance', 'exponential'],\n",
    "                          'n_estimators': [10, 40, 70, 80, 90, 100, 120, 140, 150],\n",
    "                          'learning_rate': [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],\n",
    "                          'subsample' : [0.1,0.3,0.5,0.7,0.9,1],\n",
    "                          'min_samples_split' : [2,4,5,7,9,10],\n",
    "                          'min_samples_leaf' : [1,2,3,4,5],\n",
    "                          'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],\n",
    "                          'max_features' : ['auto', 'sqrt', 'log2']\n",
    "                         }    \n",
    "\n",
    "            model_grid = RandomizedSearchCV(estimator=GradientBoostingClassifier(random_state=seed), \n",
    "                                            param_distributions=param_grid, scoring=optimize, n_iter=n_iter, \n",
    "                                            cv=fold, random_state=seed, n_jobs=-1)\n",
    "\n",
    "            model_grid.fit(X_train,y_train)\n",
    "            model = model_grid.best_estimator_\n",
    "            best_model = model_grid.best_estimator_\n",
    "            best_model_param = model_grid.best_params_\n",
    "            top_n_tuned_models.append(best_model)\n",
    "        \n",
    "        elif i == 'LinearDiscriminantAnalysis':\n",
    "            \n",
    "            param_grid = {'solver' : ['lsqr', 'eigen'],\n",
    "                          'shrinkage': [0.0001, 0.001, 0.01, 0.0005, 0.005, 0.05, 0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "                         }    \n",
    "\n",
    "            model_grid = RandomizedSearchCV(estimator=LinearDiscriminantAnalysis(), \n",
    "                                            param_distributions=param_grid, scoring=optimize, n_iter=n_iter, \n",
    "                                            cv=fold, random_state=seed, n_jobs=-1)\n",
    "\n",
    "            model_grid.fit(X_train,y_train)\n",
    "            model = model_grid.best_estimator_\n",
    "            best_model = model_grid.best_estimator_\n",
    "            best_model_param = model_grid.best_params_\n",
    "            top_n_tuned_models.append(best_model)\n",
    "        \n",
    "        elif i == 'AdaBoostClassifier':\n",
    "            \n",
    "            param_grid = {'n_estimators': [10, 40, 70, 80, 90, 100, 120, 140, 150],\n",
    "                          'learning_rate': [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],\n",
    "                          'algorithm' : [\"SAMME\", \"SAMME.R\"]\n",
    "                         }    \n",
    "\n",
    "            model_grid = RandomizedSearchCV(estimator=AdaBoostClassifier(random_state=seed), \n",
    "                                            param_distributions=param_grid, scoring=optimize, n_iter=n_iter, \n",
    "                                            cv=fold, random_state=seed, n_jobs=-1)\n",
    "\n",
    "            model_grid.fit(X_train,y_train)\n",
    "            model = model_grid.best_estimator_\n",
    "            best_model = model_grid.best_estimator_\n",
    "            best_model_param = model_grid.best_params_\n",
    "            top_n_tuned_models.append(best_model)\n",
    "        \n",
    "        elif i == 'RandomForestClassifier':\n",
    "            \n",
    "            param_grid = {'n_estimators': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "                          'criterion': ['gini', 'entropy'],\n",
    "                          'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],\n",
    "                          'min_samples_split': [2, 5, 7, 9, 10],\n",
    "                          'min_samples_leaf' : [1, 2, 4],\n",
    "                          'max_features' : ['auto', 'sqrt', 'log2'],\n",
    "                          'bootstrap': [True, False]\n",
    "                         }    \n",
    "\n",
    "            model_grid = RandomizedSearchCV(estimator=RandomForestClassifier(random_state=seed), \n",
    "                                            param_distributions=param_grid, scoring=optimize, n_iter=n_iter, \n",
    "                                            cv=fold, random_state=seed, n_jobs=-1)\n",
    "\n",
    "            model_grid.fit(X_train,y_train)\n",
    "            model = model_grid.best_estimator_\n",
    "            best_model = model_grid.best_estimator_\n",
    "            best_model_param = model_grid.best_params_\n",
    "            top_n_tuned_models.append(best_model)\n",
    "        \n",
    "        elif i == 'DecisionTreeClassifier':\n",
    "            \n",
    "            param_grid = {\"max_depth\": np.random.randint(3, (len(X_train.columns)*.85),4),\n",
    "                      \"max_features\": np.random.randint(3, len(X_train.columns),4),\n",
    "                      \"min_samples_leaf\": [2,3,4],\n",
    "                      \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "            model_grid = RandomizedSearchCV(estimator=DecisionTreeClassifier(random_state=seed), param_distributions=param_grid,\n",
    "                                           scoring=optimize, n_iter=n_iter, cv=fold, random_state=seed,\n",
    "                                           iid=False, n_jobs=-1)\n",
    "\n",
    "            model_grid.fit(X_train,y_train)\n",
    "            model = model_grid.best_estimator_\n",
    "            best_model = model_grid.best_estimator_\n",
    "            best_model_param = model_grid.best_params_\n",
    "            top_n_tuned_models.append(best_model)\n",
    "        \n",
    "        elif i == 'MLPClassifier':\n",
    "            \n",
    "            param_grid = {'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "                     'solver' : ['lbfgs', 'sgd', 'adam'],\n",
    "                     'alpha': [0.0001, 0.05],\n",
    "                     'hidden_layer_sizes': np.random.randint(5,15,5),\n",
    "                     'activation': [\"tanh\", \"identity\", \"logistic\",\"relu\"]\n",
    "                     }\n",
    "\n",
    "            model_grid = RandomizedSearchCV(estimator=MLPClassifier(max_iter=1000, random_state=seed), \n",
    "                                            param_distributions=param_grid, scoring=optimize, n_iter=n_iter, cv=fold, \n",
    "                                            random_state=seed, iid=False, n_jobs=-1)\n",
    "\n",
    "            model_grid.fit(X_train,y_train)\n",
    "            model = model_grid.best_estimator_\n",
    "            best_model = model_grid.best_estimator_\n",
    "            best_model_param = model_grid.best_params_\n",
    "            top_n_tuned_models.append(best_model)\n",
    "        \n",
    "        elif i == 'ExtraTreesClassifier':\n",
    "            \n",
    "\n",
    "            param_grid = {'n_estimators': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "                          'criterion': ['gini', 'entropy'],\n",
    "                          'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],\n",
    "                          'min_samples_split': [2, 5, 7, 9, 10],\n",
    "                          'min_samples_leaf' : [1, 2, 4],\n",
    "                          'max_features' : ['auto', 'sqrt', 'log2'],\n",
    "                          'bootstrap': [True, False]\n",
    "                         }    \n",
    "\n",
    "            model_grid = RandomizedSearchCV(estimator=ExtraTreesClassifier(random_state=seed), \n",
    "                                            param_distributions=param_grid, scoring=optimize, n_iter=n_iter, \n",
    "                                            cv=fold, random_state=seed, n_jobs=-1)\n",
    "\n",
    "            model_grid.fit(X_train,y_train)\n",
    "            model = model_grid.best_estimator_\n",
    "            best_model = model_grid.best_estimator_\n",
    "            best_model_param = model_grid.best_params_\n",
    "            top_n_tuned_models.append(best_model)\n",
    "        \n",
    "        elif i == 'GaussianProcessClassifier':\n",
    "            \n",
    "            param_grid = {\"max_iter_predict\":[100,200,300,400,500,600,700,800,900,1000]}\n",
    "\n",
    "            model_grid = RandomizedSearchCV(estimator=GaussianProcessClassifier(random_state=seed), param_distributions=param_grid,\n",
    "                                           scoring=optimize, n_iter=n_iter, cv=fold, random_state=seed,\n",
    "                                           n_jobs=-1)\n",
    "\n",
    "            model_grid.fit(X_train,y_train)\n",
    "            model = model_grid.best_estimator_\n",
    "            best_model = model_grid.best_estimator_\n",
    "            best_model_param = model_grid.best_params_\n",
    "            top_n_tuned_models.append(best_model)\n",
    "        \n",
    "        elif i == 'KNeighborsClassifier':\n",
    "            \n",
    "            param_grid = {'n_neighbors': range(1,51),\n",
    "                     'weights' : ['uniform', 'distance'],\n",
    "                     'metric':[\"euclidean\", \"manhattan\"]\n",
    "                         }        \n",
    "            model_grid = RandomizedSearchCV(estimator=KNeighborsClassifier(), param_distributions=param_grid, \n",
    "                                            scoring=optimize, n_iter=n_iter, cv=fold, random_state=seed,\n",
    "                                           n_jobs=-1, iid=False)\n",
    "\n",
    "            model_grid.fit(X_train,y_train)\n",
    "            model = model_grid.best_estimator_\n",
    "            best_model = model_grid.best_estimator_\n",
    "            best_model_param = model_grid.best_params_\n",
    "            top_n_tuned_models.append(best_model)\n",
    "        \n",
    "        elif i == 'GaussianNB':\n",
    "            \n",
    "            param_grid = {'var_smoothing': [0.000000001, 0.0000001, 0.00001, 0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007,\n",
    "                                            0.008, 0.009, 0.01, 0.1, 1]}\n",
    "\n",
    "            model_grid = RandomizedSearchCV(estimator=GaussianNB(), \n",
    "                                            param_distributions=param_grid, scoring=optimize, n_iter=n_iter, \n",
    "                                            cv=fold, random_state=seed, n_jobs=-1)\n",
    "\n",
    "            model_grid.fit(X_train,y_train)\n",
    "            model = model_grid.best_estimator_\n",
    "            best_model = model_grid.best_estimator_\n",
    "            best_model_param = model_grid.best_params_\n",
    "            top_n_tuned_models.append(best_model)\n",
    "        \n",
    "        elif i == 'SVC':\n",
    "            \n",
    "            param_grid = {'C': [.5,1,10,50,100],\n",
    "                    \"class_weight\": [\"balanced\", None]}\n",
    "\n",
    "            model_grid = RandomizedSearchCV(estimator=SVC(gamma='auto', C=1, probability=True, kernel='rbf', random_state=seed), \n",
    "                                            param_distributions=param_grid, scoring=optimize, n_iter=n_iter, \n",
    "                                            cv=fold, random_state=seed, n_jobs=-1)\n",
    "\n",
    "            model_grid.fit(X_train,y_train)\n",
    "            model = model_grid.best_estimator_\n",
    "            best_model = model_grid.best_estimator_\n",
    "            best_model_param = model_grid.best_params_\n",
    "            top_n_tuned_models.append(best_model)\n",
    "        \n",
    "        elif i == 'QuadraticDiscriminantAnalysis':\n",
    "            \n",
    "            param_grid = {'reg_param': [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]}    \n",
    "\n",
    "            model_grid = RandomizedSearchCV(estimator=QuadraticDiscriminantAnalysis(), \n",
    "                                            param_distributions=param_grid, scoring=optimize, n_iter=n_iter, \n",
    "                                            cv=fold, random_state=seed, n_jobs=-1)\n",
    "\n",
    "            model_grid.fit(X_train,y_train)\n",
    "            model = model_grid.best_estimator_\n",
    "            best_model = model_grid.best_estimator_\n",
    "            best_model_param = model_grid.best_params_\n",
    "            top_n_tuned_models.append(best_model)\n",
    "        \n",
    "        elif i == 'SGDClassifier':\n",
    "            \n",
    "            param_grid = {'penalty': ['l2', 'l1','elasticnet'],\n",
    "                          'l1_ratio': [0,0.1,0.15,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],\n",
    "                          'alpha': [0.0001, 0.001, 0.01, 0.0002, 0.002, 0.02, 0.0005, 0.005, 0.05],\n",
    "                          'fit_intercept': [True, False],\n",
    "                          'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "                          'eta0': [0.001, 0.01,0.05,0.1,0.2,0.3,0.4,0.5]\n",
    "                         }    \n",
    "\n",
    "            model_grid = RandomizedSearchCV(estimator=SGDClassifier(loss='hinge', random_state=seed), \n",
    "                                            param_distributions=param_grid, scoring=optimize, n_iter=n_iter, \n",
    "                                            cv=fold, random_state=seed, n_jobs=-1)\n",
    "\n",
    "            model_grid.fit(X_train,y_train)\n",
    "            model = model_grid.best_estimator_\n",
    "            best_model = model_grid.best_estimator_\n",
    "            best_model_param = model_grid.best_params_\n",
    "            top_n_tuned_models.append(best_model)\n",
    "    \n",
    "    master.append(top_n_tuned_models)\n",
    "    \n",
    "    progress.value += 1\n",
    "    clear_output()\n",
    "    master_display = master_results.sort_values(by=sort,ascending=False)\n",
    "    master_display.reset_index(drop=True, inplace=True)\n",
    "    display(progress)\n",
    "    display(master_display)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    This section below is still continued from Step 4. In the part above tuned model\n",
    "    object is stored in the list. In the part below the CV results are generated using\n",
    "    stored objects in above step.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    tuning_model_names = []\n",
    "    top_n_tuned_model_results = pd.DataFrame(columns=['Model', 'Accuracy','AUC','Recall', 'Prec.', 'F1', 'Kappa'])\n",
    "    \n",
    "    for i in top_n_model_names:\n",
    "        s = 'Tuning ' + i\n",
    "        tuning_model_names.append(s)\n",
    "    \n",
    "    #defining name counter\n",
    "    name_counter = 0\n",
    "    \n",
    "    for i in top_n_tuned_models:\n",
    "        model = i\n",
    "    \n",
    "        #setting cross validation\n",
    "        kf = StratifiedKFold(fold, random_state=seed)\n",
    "\n",
    "        score_auc =np.empty((0,0))\n",
    "        score_acc =np.empty((0,0))\n",
    "        score_recall =np.empty((0,0))\n",
    "        score_precision =np.empty((0,0))\n",
    "        score_f1 =np.empty((0,0))\n",
    "        score_kappa =np.empty((0,0))\n",
    "        avgs_auc =np.empty((0,0))\n",
    "        avgs_acc =np.empty((0,0))\n",
    "        avgs_recall =np.empty((0,0))\n",
    "        avgs_precision =np.empty((0,0))\n",
    "        avgs_f1 =np.empty((0,0))\n",
    "        avgs_kappa =np.empty((0,0))\n",
    "        \n",
    "        for train_i , test_i in kf.split(data_X,data_y):\n",
    "    \n",
    "            Xtrain,Xtest = data_X.iloc[train_i], data_X.iloc[test_i]\n",
    "            ytrain,ytest = data_y.iloc[train_i], data_y.iloc[test_i]\n",
    "        \n",
    "            if hasattr(model, 'predict_proba'):\n",
    "        \n",
    "                model.fit(Xtrain,ytrain)\n",
    "                pred_prob = model.predict_proba(Xtest)\n",
    "                pred_prob = pred_prob[:,1]\n",
    "                pred_ = model.predict(Xtest)\n",
    "                sca = metrics.accuracy_score(ytest,pred_)\n",
    "                sc = metrics.roc_auc_score(ytest,pred_prob)\n",
    "                recall = metrics.recall_score(ytest,pred_)\n",
    "                precision = metrics.precision_score(ytest,pred_)\n",
    "                kappa = metrics.cohen_kappa_score(ytest,pred_)\n",
    "                f1 = metrics.f1_score(ytest,pred_)\n",
    "                score_acc = np.append(score_acc,sca)\n",
    "                score_auc = np.append(score_auc,sc)\n",
    "                score_recall = np.append(score_recall,recall)\n",
    "                score_precision = np.append(score_precision,precision)\n",
    "                score_f1 =np.append(score_f1,f1)\n",
    "                score_kappa =np.append(score_kappa,kappa)\n",
    "\n",
    "            else:\n",
    "            \n",
    "                model.fit(Xtrain,ytrain)\n",
    "                pred_prob = 0.00\n",
    "                pred_prob = 0.00\n",
    "                pred_ = model.predict(Xtest)\n",
    "                sca = metrics.accuracy_score(ytest,pred_)\n",
    "                sc = 0.00\n",
    "                recall = metrics.recall_score(ytest,pred_)\n",
    "                precision = metrics.precision_score(ytest,pred_) #change pred_prob to pred_\n",
    "                kappa = metrics.cohen_kappa_score(ytest,pred_)\n",
    "                f1 = metrics.f1_score(ytest,pred_)\n",
    "                score_acc = np.append(score_acc,sca)\n",
    "                score_auc = np.append(score_auc,sc)\n",
    "                score_recall = np.append(score_recall,recall)\n",
    "                score_precision = np.append(score_precision,precision)\n",
    "                score_f1 =np.append(score_f1,f1)\n",
    "                score_kappa =np.append(score_kappa,kappa) \n",
    "       \n",
    "        mean_acc=np.mean(score_acc)\n",
    "        mean_auc=np.mean(score_auc)\n",
    "        mean_recall=np.mean(score_recall)\n",
    "        mean_precision=np.mean(score_precision)\n",
    "        mean_f1=np.mean(score_f1)\n",
    "        mean_kappa=np.mean(score_kappa)\n",
    "\n",
    "        avgs_acc = np.append(avgs_acc, mean_acc)\n",
    "        avgs_auc = np.append(avgs_auc, mean_auc)\n",
    "        avgs_recall = np.append(avgs_recall, mean_recall)\n",
    "        avgs_precision = np.append(avgs_precision, mean_precision)\n",
    "        avgs_f1 = np.append(avgs_f1, mean_f1)\n",
    "        avgs_kappa = np.append(avgs_kappa, mean_kappa)\n",
    "        \n",
    "        #model_name = 'Tuned' + str(i).split(\"(\")[0]\n",
    "        model_results = pd.DataFrame({'Model': tuning_model_names[name_counter], 'Accuracy': avgs_acc, \n",
    "                                      'AUC': avgs_auc, 'Recall' : avgs_recall, 'Prec.' : avgs_precision, \n",
    "                                      'F1' : avgs_f1, 'Kappa' : avgs_kappa}).reset_index(drop=True)\n",
    "        model_results = model_results.round(round)\n",
    "        name_counter += 1\n",
    "        top_n_tuned_model_results = pd.concat([top_n_tuned_model_results, model_results],ignore_index=True)\n",
    "        \n",
    "    master_results = master_results.append(top_n_tuned_model_results)\n",
    "    \n",
    "    progress.value += 1\n",
    "    clear_output()\n",
    "    master_display = master_results.sort_values(by=sort,ascending=False)\n",
    "    master_display.reset_index(drop=True, inplace=True)\n",
    "    display(progress)\n",
    "    display(master_display)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Unpacking Master into master_unpack so it can be used for sampling \n",
    "    for VotingClassifier and Stacking in Step 5 and Step 6 below. Note that\n",
    "    master_unpack is not the most updated list by the end of code as the \n",
    "    models created in Step 5 and 6 below are not unpacked into master_unpack.\n",
    "    Last part of code used object 'master_final' to unpack all the models from\n",
    "    object 'master'.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    master_unpack = []\n",
    "    for i in master:\n",
    "        for k in i:\n",
    "            master_unpack.append(k)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    This is the loop created for random sampling index numbers in master_unpack list\n",
    "    for models that can be used in VotingClassifier in Step 5 below. Same sampling i.e.\n",
    "    variable mix and mix_names is used in Stacking in Step 6 below.\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    count_while = 0\n",
    "    \n",
    "    mix = []\n",
    "    mix_names = []\n",
    "    while count_while < top_n:\n",
    "        sub_list = []\n",
    "        sub_list_names = []\n",
    "        generator = random.sample(range(len(master_results)), random.randint(3,len(master_results)))\n",
    "        for r in generator:\n",
    "            sub_list.append(master_unpack[r])\n",
    "            sub_list_names.append(master_results.iloc[r]['Model'])\n",
    "        mix.append(sub_list)\n",
    "        mix_names.append(sub_list_names)\n",
    "        count_while += 1\n",
    "    \n",
    "    progress.value += 1\n",
    "    \n",
    "    '''\n",
    "\n",
    "    Step 5 - Using mix and mix_names created above, build voting classifier n # of times.\n",
    "    This is equivalent to blend_models()\n",
    "\n",
    "    '''    \n",
    "    \n",
    "    top_n_voting_models = []\n",
    "    top_n_voting_model_results = pd.DataFrame(columns=['Model', 'Accuracy','AUC','Recall', 'Prec.', 'F1', 'Kappa'])\n",
    "    \n",
    "    for i,j in zip(mix,mix_names):\n",
    "        \n",
    "        estimator_list = zip(j, i)\n",
    "        estimator_list = list(estimator_list)    \n",
    "        model = VotingClassifier(estimators=estimator_list, voting='hard', n_jobs=-1)\n",
    "        top_n_voting_models.append(model)\n",
    "    \n",
    "        #setting cross validation\n",
    "        kf = StratifiedKFold(fold, random_state=seed)\n",
    "\n",
    "        score_auc =np.empty((0,0))\n",
    "        score_acc =np.empty((0,0))\n",
    "        score_recall =np.empty((0,0))\n",
    "        score_precision =np.empty((0,0))\n",
    "        score_f1 =np.empty((0,0))\n",
    "        score_kappa =np.empty((0,0))\n",
    "        avgs_auc =np.empty((0,0))\n",
    "        avgs_acc =np.empty((0,0))\n",
    "        avgs_recall =np.empty((0,0))\n",
    "        avgs_precision =np.empty((0,0))\n",
    "        avgs_f1 =np.empty((0,0))\n",
    "        avgs_kappa =np.empty((0,0))\n",
    "        \n",
    "        for train_i , test_i in kf.split(data_X,data_y):\n",
    "    \n",
    "            Xtrain,Xtest = data_X.iloc[train_i], data_X.iloc[test_i]\n",
    "            ytrain,ytest = data_y.iloc[train_i], data_y.iloc[test_i]\n",
    "            \n",
    "            model.fit(Xtrain,ytrain)\n",
    "            pred_prob = 0.00\n",
    "            pred_prob = 0.00\n",
    "            pred_ = model.predict(Xtest)\n",
    "            sca = metrics.accuracy_score(ytest,pred_)\n",
    "            sc = 0.00\n",
    "            recall = metrics.recall_score(ytest,pred_)\n",
    "            precision = metrics.precision_score(ytest,pred_) #change pred_prob to pred_\n",
    "            kappa = metrics.cohen_kappa_score(ytest,pred_)\n",
    "            f1 = metrics.f1_score(ytest,pred_)\n",
    "            score_acc = np.append(score_acc,sca)\n",
    "            score_auc = np.append(score_auc,sc)\n",
    "            score_recall = np.append(score_recall,recall)\n",
    "            score_precision = np.append(score_precision,precision)\n",
    "            score_f1 =np.append(score_f1,f1)\n",
    "            score_kappa =np.append(score_kappa,kappa) \n",
    "       \n",
    "        mean_acc=np.mean(score_acc)\n",
    "        mean_auc=np.mean(score_auc)\n",
    "        mean_recall=np.mean(score_recall)\n",
    "        mean_precision=np.mean(score_precision)\n",
    "        mean_f1=np.mean(score_f1)\n",
    "        mean_kappa=np.mean(score_kappa)\n",
    "\n",
    "        avgs_acc = np.append(avgs_acc, mean_acc)\n",
    "        avgs_auc = np.append(avgs_auc, mean_auc)\n",
    "        avgs_recall = np.append(avgs_recall, mean_recall)\n",
    "        avgs_precision = np.append(avgs_precision, mean_precision)\n",
    "        avgs_f1 = np.append(avgs_f1, mean_f1)\n",
    "        avgs_kappa = np.append(avgs_kappa, mean_kappa)\n",
    "\n",
    "        model_results = pd.DataFrame({'Model': 'Voting Classifier', 'Accuracy': avgs_acc, 'AUC': avgs_auc, \n",
    "                                      'Recall' : avgs_recall, 'Prec.' : avgs_precision , 'F1' : avgs_f1, \n",
    "                                      'Kappa' : avgs_kappa}).reset_index(drop=True)\n",
    "        model_results = model_results.round(round)\n",
    "        top_n_voting_model_results = pd.concat([top_n_voting_model_results, model_results],ignore_index=True)\n",
    "        \n",
    "    master_results = master_results.append(top_n_voting_model_results)\n",
    "    master_results = master_results.reset_index(drop=True)\n",
    "    master.append(top_n_voting_models)\n",
    "    \n",
    "    progress.value += 1\n",
    "    clear_output()\n",
    "    master_display = master_results.sort_values(by=sort,ascending=False)\n",
    "    master_display.reset_index(drop=True, inplace=True)\n",
    "    display(progress)\n",
    "    display(master_display) \n",
    "    \n",
    "    '''\n",
    "\n",
    "    Step 6 - Stacking for all the models using same sample as above that are stored in\n",
    "    mix and mix_names. \n",
    "    \n",
    "    This is equivalent to stack_models()\n",
    "\n",
    "\n",
    "    '''    \n",
    "    \n",
    "    top_n_stacking_models = []\n",
    "    top_n_stacking_model_results = pd.DataFrame(columns=['Model', 'Accuracy','AUC','Recall', 'Prec.', 'F1', 'Kappa'])\n",
    "    \n",
    "    meta_model = LogisticRegression()\n",
    "    \n",
    "    for i in mix:\n",
    "        \n",
    "        estimator_list = i\n",
    "        top_n_stacking_models.append(i)\n",
    "        \n",
    "        #defining model_library model names\n",
    "        model_names = np.zeros(0)\n",
    "        for item in estimator_list:\n",
    "            model_names = np.append(model_names, str(item).split(\"(\")[0])\n",
    "    \n",
    "        base_array = np.zeros((0,0))\n",
    "        base_prediction = pd.DataFrame(y_train)\n",
    "        base_prediction = base_prediction.reset_index(drop=True)\n",
    "    \n",
    "        for model in estimator_list:\n",
    "            base_array = cross_val_predict(model,X_train,y_train,cv=fold, method='predict')\n",
    "            base_array = base_array\n",
    "            base_array_df = pd.DataFrame(base_array)\n",
    "            base_prediction = pd.concat([base_prediction,base_array_df],axis=1)\n",
    "            base_array = np.empty((0,0))\n",
    "        \n",
    "        #defining column names now\n",
    "        target_col_name = np.array(base_prediction.columns[0])\n",
    "        model_names = np.append(target_col_name, model_names)\n",
    "        base_prediction.columns = model_names #defining colum names now\n",
    "        data_X = base_prediction.drop(base_prediction.columns[0],axis=1)\n",
    "        data_y = base_prediction[base_prediction.columns[0]]\n",
    "\n",
    "        #Meta Modeling Starts Here\n",
    "\n",
    "        model = meta_model \n",
    "        \n",
    "        kf = StratifiedKFold(fold, random_state=seed)\n",
    "\n",
    "        score_auc =np.empty((0,0))\n",
    "        score_acc =np.empty((0,0))\n",
    "        score_recall =np.empty((0,0))\n",
    "        score_precision =np.empty((0,0))\n",
    "        score_f1 =np.empty((0,0))\n",
    "        score_kappa =np.empty((0,0))\n",
    "        avgs_auc =np.empty((0,0))\n",
    "        avgs_acc =np.empty((0,0))\n",
    "        avgs_recall =np.empty((0,0))\n",
    "        avgs_precision =np.empty((0,0))\n",
    "        avgs_f1 =np.empty((0,0))\n",
    "        avgs_kappa =np.empty((0,0))\n",
    "\n",
    "        for train_i , test_i in kf.split(data_X,data_y):\n",
    "            Xtrain,Xtest = data_X.iloc[train_i], data_X.iloc[test_i]\n",
    "            ytrain,ytest = data_y.iloc[train_i], data_y.iloc[test_i]\n",
    "\n",
    "            model.fit(Xtrain,ytrain)\n",
    "            pred_prob = model.predict_proba(Xtest)\n",
    "            pred_prob = pred_prob[:,1]\n",
    "            pred_ = model.predict(Xtest)\n",
    "            sca = metrics.accuracy_score(ytest,pred_)\n",
    "            sc = metrics.roc_auc_score(ytest,pred_prob)\n",
    "            recall = metrics.recall_score(ytest,pred_)\n",
    "            precision = metrics.average_precision_score(ytest,pred_prob)\n",
    "            kappa = metrics.cohen_kappa_score(ytest,pred_)\n",
    "            f1 = metrics.f1_score(ytest,pred_)\n",
    "            score_acc = np.append(score_acc,sca)\n",
    "            score_auc = np.append(score_auc,sc)\n",
    "            score_recall = np.append(score_recall,recall)\n",
    "            score_precision = np.append(score_precision,precision)\n",
    "            score_f1 =np.append(score_f1,f1)\n",
    "            score_kappa =np.append(score_kappa,kappa)\n",
    "\n",
    "        mean_acc=np.mean(score_acc)\n",
    "        mean_auc=np.mean(score_auc)\n",
    "        mean_recall=np.mean(score_recall)\n",
    "        mean_precision=np.mean(score_precision)\n",
    "        mean_f1=np.mean(score_f1)\n",
    "        mean_kappa=np.mean(score_kappa)\n",
    "        std_acc=np.std(score_acc)\n",
    "        std_auc=np.std(score_auc)\n",
    "        std_recall=np.std(score_recall)\n",
    "        std_precision=np.std(score_precision)\n",
    "        std_f1=np.std(score_f1)\n",
    "        std_kappa=np.std(score_kappa)\n",
    "\n",
    "        avgs_acc = np.append(avgs_acc, mean_acc)\n",
    "        avgs_auc = np.append(avgs_auc, mean_auc)\n",
    "        avgs_recall = np.append(avgs_recall, mean_recall)\n",
    "        avgs_precision = np.append(avgs_precision, mean_precision)\n",
    "        avgs_f1 = np.append(avgs_f1, mean_f1)\n",
    "        avgs_kappa = np.append(avgs_kappa, mean_kappa)\n",
    "\n",
    "        model_results = pd.DataFrame({'Model': 'Stacking Classifier',  'Accuracy': avgs_acc, 'AUC': avgs_auc, \n",
    "                                      'Recall' : avgs_recall, 'Prec.' : avgs_precision , 'F1' : avgs_f1, \n",
    "                                      'Kappa' : avgs_kappa})\n",
    "        top_n_stacking_model_results = pd.concat([top_n_stacking_model_results, model_results],ignore_index=True)\n",
    "        top_n_stacking_model_results = top_n_stacking_model_results.round(round)  \n",
    "\n",
    "\n",
    "    master_results = master_results.append(top_n_stacking_model_results)\n",
    "    master_results = master_results.reset_index(drop=True)\n",
    "    master.append(top_n_stacking_models)\n",
    "    \n",
    "    progress.value += 1\n",
    "    clear_output()\n",
    "    master_display = master_results.sort_values(by=sort,ascending=False)\n",
    "    master_display.reset_index(drop=True, inplace=True)\n",
    "    display(progress)\n",
    "    display(master_display)\n",
    "    \n",
    "    '''\n",
    "\n",
    "    Step 7 - Unpacking final master list stored in object 'master'. The one unpacked\n",
    "    before step 4 was used for sampling in Step 5 and 6.\n",
    "    \n",
    "    THIS IS THE FINAL UNPACKING.\n",
    "    \n",
    "    ''' \n",
    "    #global master_final\n",
    "    master_final = []\n",
    "    for i in master:\n",
    "        for k in i:\n",
    "            master_final.append(k)\n",
    "    \n",
    "    #renaming\n",
    "    master = master_final\n",
    "    del(master_final) #remove master_final\n",
    "    \n",
    "    progress.value += 1\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Step 8 - This is the final step in which master_results is sorted based on defined metric\n",
    "    to get the index of best model so that master can return the final best model.\n",
    "    also master_results is sorted and index is reset before display.\n",
    "    \n",
    "    ''' \n",
    "    best_model_position = master_results.sort_values(by=sort,ascending=False).index[0]\n",
    "    best_model = master[best_model_position]\n",
    "    \n",
    "    master_results_sorted = master_results.sort_values(by=sort,ascending=False)\n",
    "    master_results_sorted.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    progress.value += 1\n",
    "    \n",
    "    clear_output()\n",
    "    \n",
    "    display(master_results_sorted)\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work in Progress / Future Release "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(data_X=X_train, n=3):\n",
    "    global X_train\n",
    "    drop_list = var_imp_array_top_n[0:n]\n",
    "    X_train.drop(drop_list, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_name):\n",
    "    from sklearn.externals import joblib\n",
    "    model_name = model_name + '.pkl'\n",
    "    joblib.dump(model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    from sklearn.externals import joblib\n",
    "    model_name = model_name + '.pkl'\n",
    "    return joblib.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules now Available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0. compare_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0. create_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0. plot_model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0. tune_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0. ensemble_model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0 blend_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.0. stack_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.0. create_stacknet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.0. save_model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.0 load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.0. optimize_model (Future Release)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.0. predict_stacknet (Future Release)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.0. calibrate_model (Future Release)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
