{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BOSTON HOUSING DATA\n",
    "###################\n",
    "data = pd.read_csv('Boston.csv')\n",
    "data = data.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(data, \n",
    "          target, \n",
    "          split=0.7):\n",
    "    \n",
    "  from sklearn.model_selection import train_test_split\n",
    "  X = data.drop(target,axis=1)\n",
    "  y = data[target]\n",
    "  global X_train, X_test, y_train, y_test, seed\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1-split)\n",
    "  import random\n",
    "  seed = random.randint(150,900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup(data, 'medv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(estimator = None, \n",
    "                 fold = 10, \n",
    "                 round = 4,  \n",
    "                 verbose = True):\n",
    "  \n",
    "  #defining X_train and y_train    \n",
    "  data_X = X_train\n",
    "  data_y = y_train\n",
    "  \n",
    "  #ignore co-linearity warnings for qda and lda \n",
    "  import warnings\n",
    "  warnings.filterwarnings('ignore') \n",
    "  \n",
    "  import numpy as np\n",
    "  import pandas as pd\n",
    "  from scipy import stats\n",
    "  import random\n",
    "  from sklearn.model_selection import KFold\n",
    "  from sklearn.metrics import mean_absolute_error\n",
    "  from sklearn.metrics import mean_squared_error\n",
    "  from sklearn.metrics import mean_squared_log_error\n",
    "  from sklearn.metrics import max_error\n",
    "  from sklearn.metrics import r2_score\n",
    "    \n",
    "  kf = KFold(fold, random_state=seed)\n",
    "\n",
    "  score_mae =np.empty((0,0))\n",
    "  score_mse =np.empty((0,0))\n",
    "  score_rmse =np.empty((0,0))\n",
    "  score_r2 =np.empty((0,0))\n",
    "  score_max_error =np.empty((0,0))\n",
    "  avgs_mae =np.empty((0,0))\n",
    "  avgs_mse =np.empty((0,0))\n",
    "  avgs_rmse =np.empty((0,0))\n",
    "  avgs_r2 =np.empty((0,0))\n",
    "  avgs_max_error =np.empty((0,0))\n",
    "    \n",
    "  if estimator == None:\n",
    "    print(\"Please enter your custom model as on object or choose from model library. If you have previously defined the estimator, the output is generated using the same estimator\") \n",
    "  elif estimator == 'lr':\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    model = LinearRegression()\n",
    "    full_name = 'Linear Regression'\n",
    "  elif estimator == 'lasso':\n",
    "    from sklearn.linear_model import Lasso\n",
    "    model = Lasso(random_state=seed)\n",
    "    full_name = 'Lasso Regression'\n",
    "  elif estimator == 'ridge':\n",
    "    from sklearn.linear_model import Ridge\n",
    "    model = Ridge(random_state=seed)\n",
    "    full_name = 'Ridge Regression'\n",
    "  elif estimator == 'en':\n",
    "    from sklearn.linear_model import ElasticNet\n",
    "    model = ElasticNet(random_state=seed)\n",
    "    full_name = 'Elastic Net'\n",
    "  elif estimator == 'lars':\n",
    "    from sklearn.linear_model import Lars\n",
    "    model = Lars()\n",
    "    full_name = 'Least Angle Regression'\n",
    "  elif estimator == 'llars':\n",
    "    from sklearn.linear_model import LassoLars\n",
    "    model = LassoLars()\n",
    "    full_name = 'Lasso Least Angle Regression'\n",
    "  elif estimator == 'omp':\n",
    "    from sklearn.linear_model import OrthogonalMatchingPursuit\n",
    "    model = OrthogonalMatchingPursuit()\n",
    "    full_name = 'Orthogonal Matching Pursuit'\n",
    "  elif estimator == 'br':\n",
    "    from sklearn.linear_model import BayesianRidge\n",
    "    model = BayesianRidge()\n",
    "    full_name = 'Bayesian Ridge Regression'    \n",
    "  elif estimator == 'ard':\n",
    "    from sklearn.linear_model import ARDRegression\n",
    "    model = ARDRegression()\n",
    "    full_name = 'Automatic Relevance Determination'        \n",
    "  elif estimator == 'par':\n",
    "    from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "    model = PassiveAggressiveRegressor(random_state=seed)\n",
    "    full_name = 'Passive Aggressive Regressor'    \n",
    "  elif estimator == 'ransac':\n",
    "    from sklearn.linear_model import RANSACRegressor\n",
    "    model = RANSACRegressor(random_state=seed)\n",
    "    full_name = 'Random Sample Consensus'   \n",
    "  elif estimator == 'tr':\n",
    "    from sklearn.linear_model import TheilSenRegressor\n",
    "    model = TheilSenRegressor(random_state=seed)\n",
    "    full_name = 'TheilSen Regressor'     \n",
    "  elif estimator == 'huber':\n",
    "    from sklearn.linear_model import HuberRegressor\n",
    "    model = HuberRegressor()\n",
    "    full_name = 'Huber Regressor'   \n",
    "  elif estimator == 'kr':\n",
    "    from sklearn.kernel_ridge import KernelRidge\n",
    "    model = KernelRidge()\n",
    "    full_name = 'Kernel Ridge'    \n",
    "  elif estimator == 'svm':\n",
    "    from sklearn.svm import SVR\n",
    "    model = SVR()\n",
    "    full_name = 'Support Vector Regression'          \n",
    "  elif estimator == 'knn':\n",
    "    from sklearn.neighbors import KNeighborsRegressor\n",
    "    model = KNeighborsRegressor()\n",
    "    full_name = 'Nearest Neighbors Regression'      \n",
    "  elif estimator == 'dt':\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    model = DecisionTreeRegressor(random_state=seed)\n",
    "    full_name = 'Decision Tree Regressor'\n",
    "  elif estimator == 'rf':\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    model = RandomForestRegressor(random_state=seed)\n",
    "    full_name = 'Random Forest Regressor'\n",
    "  elif estimator == 'et':\n",
    "    from sklearn.ensemble import ExtraTreesRegressor\n",
    "    model = ExtraTreesRegressor(random_state=seed)\n",
    "    full_name = 'Extra Trees Regressor'    \n",
    "  elif estimator == 'ada':\n",
    "    from sklearn.ensemble import AdaBoostRegressor\n",
    "    model = AdaBoostRegressor(random_state=seed)\n",
    "    full_name = 'AdaBoost Regressor'    \n",
    "  elif estimator == 'gbr':\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    model = GradientBoostingRegressor(random_state=seed)\n",
    "    full_name = 'Gradient Boosting Regressor'       \n",
    "  elif estimator == 'mlp':\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "    model = MLPRegressor(random_state=seed)\n",
    "    full_name = 'MLP Regressor'\n",
    "  else:\n",
    "    model = estimator\n",
    "    full_name = str(model).split(\"(\")[0]\n",
    "     \n",
    "  for train_i , test_i in kf.split(data_X,data_y):\n",
    "    \n",
    "    Xtrain,Xtest = data_X.iloc[train_i], data_X.iloc[test_i]\n",
    "    ytrain,ytest = data_y.iloc[train_i], data_y.iloc[test_i]\n",
    "    model.fit(Xtrain,ytrain)\n",
    "    pred_ = model.predict(Xtest)\n",
    "    mae = mean_absolute_error(ytest,pred_)\n",
    "    mse = mean_squared_error(ytest,pred_)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(ytest,pred_)\n",
    "    max_error_ = max_error(ytest,pred_)\n",
    "    score_mae = np.append(score_mae,mae)\n",
    "    score_mse = np.append(score_mse,mse)\n",
    "    score_rmse = np.append(score_rmse,rmse)\n",
    "    score_r2 =np.append(score_r2,r2)\n",
    "    score_max_error = np.append(score_max_error,max_error_)\n",
    "       \n",
    "  mean_mae=np.mean(score_mae)\n",
    "  mean_mse=np.mean(score_mse)\n",
    "  mean_rmse=np.mean(score_rmse)\n",
    "  mean_r2=np.mean(score_r2)\n",
    "  mean_max_error=np.mean(score_max_error)\n",
    "  std_mae=np.std(score_mae)\n",
    "  std_mse=np.std(score_mse)\n",
    "  std_rmse=np.std(score_rmse)\n",
    "  std_r2=np.std(score_r2)\n",
    "  std_max_error=np.std(score_max_error)\n",
    "    \n",
    "  avgs_mae = np.append(avgs_mae, mean_mae)\n",
    "  avgs_mae = np.append(avgs_mae, std_mae) \n",
    "  avgs_mse = np.append(avgs_mse, mean_mse)\n",
    "  avgs_mse = np.append(avgs_mse, std_mse)\n",
    "  avgs_rmse = np.append(avgs_rmse, mean_rmse)\n",
    "  avgs_rmse = np.append(avgs_rmse, std_rmse)\n",
    "  avgs_r2 = np.append(avgs_r2, mean_r2)\n",
    "  avgs_r2 = np.append(avgs_r2, std_r2)\n",
    "  avgs_max_error = np.append(avgs_max_error, mean_max_error)\n",
    "  avgs_max_error = np.append(avgs_max_error, std_max_error)\n",
    "    \n",
    "  model_results = pd.DataFrame({'MAE': score_mae, 'MSE': score_mse, 'RMSE' : score_rmse, \n",
    "                                'R2' : score_r2, 'ME' : score_max_error})\n",
    "  #model_results_unpivot = pd.melt(model_results,value_vars=['Mean Absolute Error', 'AUC', 'Recall', 'Prec.', 'F1', 'Kappa'])\n",
    "  #model_results_unpivot.columns = ['Metric', 'Measure']\n",
    "  model_avgs = pd.DataFrame({'MAE': avgs_mae, 'MSE': avgs_mse, 'RMSE' : avgs_rmse, 'R2' : avgs_r2,\n",
    "                             'ME' : avgs_max_error},index=['Mean', 'SD'])\n",
    "    \n",
    "  model_results = model_results.append(model_avgs)\n",
    "  model_results = model_results.round(round)  \n",
    "  model_results = model_results.style.set_table_styles([ dict(selector='th', props=[('text-align', 'center')] ) ])\n",
    "\n",
    "  if verbose:\n",
    "    display(model_results)\n",
    "    return model\n",
    "  else:\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(model_library = None, \n",
    "                   fold = 10, \n",
    "                   round = 4, \n",
    "                   sort = 'MAE', \n",
    "                   blacklist = None):\n",
    "  \n",
    "  #ignore warnings\n",
    "  import warnings\n",
    "  warnings.filterwarnings('ignore') \n",
    "    \n",
    "  #defining X_train and y_train\n",
    "  data_X = X_train\n",
    "  data_y=y_train\n",
    "\n",
    "  from sklearn.linear_model import LinearRegression\n",
    "  from sklearn.linear_model import Ridge\n",
    "  from sklearn.linear_model import Lasso\n",
    "  from sklearn.linear_model import ElasticNet\n",
    "  from sklearn.linear_model import Lars\n",
    "  from sklearn.linear_model import LassoLars\n",
    "  from sklearn.linear_model import OrthogonalMatchingPursuit\n",
    "  from sklearn.linear_model import BayesianRidge\n",
    "  from sklearn.linear_model import ARDRegression\n",
    "  from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "  from sklearn.linear_model import RANSACRegressor\n",
    "  from sklearn.linear_model import TheilSenRegressor\n",
    "  from sklearn.linear_model import HuberRegressor\n",
    "  from sklearn.kernel_ridge import KernelRidge\n",
    "  from sklearn.svm import SVR\n",
    "  from sklearn.neighbors import KNeighborsRegressor\n",
    "  from sklearn.tree import DecisionTreeRegressor\n",
    "  from sklearn.ensemble import RandomForestRegressor\n",
    "  from sklearn.ensemble import ExtraTreesRegressor\n",
    "  from sklearn.ensemble import AdaBoostRegressor\n",
    "  from sklearn.ensemble import GradientBoostingRegressor\n",
    "  from sklearn.neural_network import MLPRegressor \n",
    "  from sklearn.model_selection import KFold\n",
    "  from sklearn.model_selection import cross_val_score\n",
    "  from sklearn.model_selection import cross_val_predict\n",
    "  from sklearn.model_selection import cross_validate\n",
    "  from sklearn.model_selection import KFold\n",
    "  from sklearn.metrics import mean_absolute_error\n",
    "  from sklearn.metrics import mean_squared_error\n",
    "  from sklearn.metrics import mean_squared_log_error\n",
    "  from sklearn.metrics import max_error\n",
    "  from sklearn.metrics import r2_score\n",
    "  import numpy as np\n",
    "  import pandas as pd\n",
    "  import random\n",
    "  import pandas.io.formats.style\n",
    "\n",
    "  lr = LinearRegression()\n",
    "  lasso = Lasso(random_state=seed)\n",
    "  ridge = Ridge(random_state=seed)\n",
    "  en = ElasticNet(random_state=seed)\n",
    "  lars = Lars()\n",
    "  llars = LassoLars()\n",
    "  omp = OrthogonalMatchingPursuit()\n",
    "  br = BayesianRidge()\n",
    "  ard = ARDRegression()\n",
    "  par = PassiveAggressiveRegressor(random_state=seed)\n",
    "  ransac = RANSACRegressor(random_state=seed)\n",
    "  tr = TheilSenRegressor(random_state=seed)\n",
    "  huber = HuberRegressor()\n",
    "  kr = KernelRidge()\n",
    "  svr = SVR()\n",
    "  knn = KNeighborsRegressor()\n",
    "  dt = DecisionTreeRegressor(random_state=seed)\n",
    "  rf = RandomForestRegressor(random_state=seed)\n",
    "  et = ExtraTreesRegressor(random_state=seed)\n",
    "  ada = AdaBoostRegressor(random_state=seed)\n",
    "  gbr = GradientBoostingRegressor(random_state=seed)\n",
    "  mlp = MLPRegressor(random_state=seed)  \n",
    "  \n",
    "  #blacklist models\n",
    "\n",
    "  if model_library != None:\n",
    "    \n",
    "    model_library = model_library\n",
    "    \n",
    "    model_names = []\n",
    "    \n",
    "    for names in model_library:\n",
    "        \n",
    "        model_names = np.append(model_names, str(names).split(\"(\")[0])\n",
    "        \n",
    "        import re \n",
    "        \n",
    "        def putSpace(input):\n",
    "            words = re.findall('[A-Z][a-z]*', input)\n",
    "            words = ' '.join(words)\n",
    "            return words  \n",
    "\n",
    "        model_names_modified = []\n",
    "        for i in model_names:\n",
    "            model_names_modified.append(putSpace(i))\n",
    "            \n",
    "        model_names_modified = []\n",
    "        for i in model_names:\n",
    "            model_names_modified.append(putSpace(i))\n",
    "\n",
    "        model_names = model_names_modified\n",
    "\n",
    "        model_names_final = []\n",
    "        for j in model_names:\n",
    "            if j == 'A R D Regression':\n",
    "                model_names_final.append('Automatic Relevance Determination')\n",
    "            elif j == 'M L P Regressor':\n",
    "                model_names_final.append('MLP Regressor')\n",
    "            elif j == 'R A N S A C Regressor':\n",
    "                model_names_final.append('RANSAC Regressor')\n",
    "            elif j == 'S V R':\n",
    "                model_names_final.append('Support Vector Regressor')\n",
    "            elif j == 'Lars':\n",
    "                model_names_final.append('Least Angle Regression')                \n",
    "            else: \n",
    "                model_names_final.append(j)\n",
    "\n",
    "        model_names = model_names_final    \n",
    "    \n",
    "  else:\n",
    "        \n",
    "    if blacklist == None:\n",
    "        \n",
    "        model_library = [lr, lasso, ridge, en, lars, llars, omp, br, ard, par, ransac, tr, huber, kr, svr, knn, \n",
    "                        dt, rf, et, ada, gbr, mlp]\n",
    "    \n",
    "        model_names = []\n",
    "    \n",
    "        for names in model_library:\n",
    "            model_names = np.append(model_names, str(names).split(\"(\")[0])\n",
    "    \n",
    "        import re \n",
    "\n",
    "        def putSpace(input):\n",
    "            words = re.findall('[A-Z][a-z]*', input)\n",
    "            words = ' '.join(words)\n",
    "            return words  \n",
    "\n",
    "        model_names_modified = []\n",
    "        for i in model_names:\n",
    "            model_names_modified.append(putSpace(i))\n",
    "\n",
    "        model_names = model_names_modified\n",
    "\n",
    "        model_names_final = []\n",
    "        for j in model_names:\n",
    "            \n",
    "            if j == 'A R D Regression':\n",
    "                model_names_final.append('Automatic Relevance Determination')\n",
    "            elif j == 'M L P Regressor':\n",
    "                model_names_final.append('MLP Regressor')\n",
    "            elif j == 'R A N S A C Regressor':\n",
    "                model_names_final.append('RANSAC Regressor')\n",
    "            elif j == 'S V R':\n",
    "                model_names_final.append('Support Vector Regressor')\n",
    "            elif j == 'Lars':\n",
    "                model_names_final.append('Least Angle Regression')  \n",
    "            else: \n",
    "                model_names_final.append(j)\n",
    "\n",
    "        model_names = model_names_final\n",
    "\n",
    "    else:\n",
    "        \n",
    "        model_library_values = ['lr', 'lasso', 'ridge', 'en', 'lars', 'llars', 'omp', 'br', 'ard', 'par', 'ransac', 'tr',\n",
    "                                'huber', 'kr', 'svr', 'knn', 'dt', 'rf', 'et', 'ada', 'gbr', 'mlp']\n",
    "\n",
    "        location = []\n",
    "\n",
    "        for item in blacklist:\n",
    "            location.append(model_library_values.index(item))\n",
    "\n",
    "        model_library = [lr, lasso, ridge, en, lars, llars, omp, br, ard, par, ransac, tr, huber, kr, svr, knn, \n",
    "                        dt, rf, et, ada, gbr, mlp]\n",
    "\n",
    "        for i in location:\n",
    "            del model_library[i]\n",
    "\n",
    "        model_names = []\n",
    "\n",
    "        for names in model_library:\n",
    "            model_names = np.append(model_names, str(names).split(\"(\")[0])\n",
    "\n",
    "        import re\n",
    "\n",
    "        def putSpace(input):\n",
    "            words = re.findall('[A-Z][a-z]*', input)\n",
    "            words = ' '.join(words)\n",
    "            return words  \n",
    "\n",
    "        model_names_modified = []\n",
    "        for i in model_names:\n",
    "            model_names_modified.append(putSpace(i))\n",
    "\n",
    "        model_names = model_names_modified\n",
    "\n",
    "        model_names_final = []\n",
    "        for j in model_names:\n",
    "            if j == 'Gaussian N B':\n",
    "                model_names_final.append('Naive Bayes')\n",
    "            elif j == 'M L P Classifier':\n",
    "                model_names_final.append('MLP Classifier')\n",
    "            elif j == 'S G D Classifier':\n",
    "                model_names_final.append('SVM - Linear Kernel')\n",
    "            elif j == 'S V C':\n",
    "                model_names_final.append('SVM - Radial Kernel')\n",
    "            else: \n",
    "                model_names_final.append(j)\n",
    "\n",
    "        model_names = model_names_final\n",
    "\n",
    "  kf = KFold(fold, random_state=seed)\n",
    "  \n",
    "  score_mae =np.empty((0,0))\n",
    "  score_mse =np.empty((0,0))\n",
    "  score_rmse =np.empty((0,0))\n",
    "  score_r2 =np.empty((0,0))\n",
    "  score_max_error =np.empty((0,0))\n",
    "  #avgs_mae =np.empty((0,0))\n",
    "  #avgs_mse =np.empty((0,0))\n",
    "  #avgs_rmse =np.empty((0,0))\n",
    "  #avgs_r2 =np.empty((0,0))\n",
    "  #avgs_max_error =np.empty((0,0))\n",
    "    \n",
    "  avg_mae = np.empty((0,0))\n",
    "  avg_mse = np.empty((0,0))\n",
    "  avg_rmse = np.empty((0,0))\n",
    "  avg_r2 = np.empty((0,0))\n",
    "  avg_max_error = np.empty((0,0))\n",
    "  #avg_kappa = np.empty((0,0))\n",
    "      \n",
    "  for model in model_library:\n",
    " \n",
    "    for train_i , test_i in kf.split(data_X,data_y):\n",
    "     \n",
    "        Xtrain,Xtest = data_X.iloc[train_i], data_X.iloc[test_i]\n",
    "        ytrain,ytest = data_y.iloc[train_i], data_y.iloc[test_i]\n",
    "        \n",
    "        model.fit(Xtrain,ytrain)\n",
    "        pred_ = model.predict(Xtest)\n",
    "        mae = mean_absolute_error(ytest,pred_)\n",
    "        mse = mean_squared_error(ytest,pred_)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(ytest,pred_)\n",
    "        max_error_ = max_error(ytest,pred_)\n",
    "        score_mae = np.append(score_mae,mae)\n",
    "        score_mse = np.append(score_mse,mse)\n",
    "        score_rmse = np.append(score_rmse,rmse)\n",
    "        score_r2 =np.append(score_r2,r2)\n",
    "        score_max_error = np.append(score_max_error,max_error_)\n",
    "        \n",
    "    avg_mae = np.append(avg_mae,np.mean(score_mae))\n",
    "    avg_mse = np.append(avg_mse,np.mean(score_mse))\n",
    "    avg_rmse = np.append(avg_rmse,np.mean(score_rmse))\n",
    "    avg_r2 = np.append(avg_r2,np.mean(score_r2))\n",
    "    avg_max_error = np.append(avg_max_error,np.mean(score_max_error))\n",
    "    \n",
    "    score_mae =np.empty((0,0))\n",
    "    score_mse =np.empty((0,0))\n",
    "    score_rmse =np.empty((0,0))\n",
    "    score_r2 =np.empty((0,0))\n",
    "    score_max_error =np.empty((0,0))\n",
    "    #score_kappa =np.empty((0,0))\n",
    "  \n",
    "  def highlight_min(s):\n",
    "    is_min = s == s.min()\n",
    "    return ['background-color: yellow' if v else '' for v in is_min]\n",
    "\n",
    "  def highlight_max(s):\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "  compare_models_ = pd.DataFrame({'Model':model_names, 'MAE':avg_mae, 'MSE':avg_mse, \n",
    "                     'RMSE':avg_rmse, 'R2':avg_r2, \n",
    "                     'ME':avg_max_error}).round(round).sort_values(by=[sort], \n",
    "                      ascending=True).reset_index(drop=True).style.apply(highlight_min,subset=['MAE','MSE','RMSE','ME']) #.style.apply(highlight_max, subset='R2')\n",
    "  compare_models_ = compare_models_.set_properties(**{'text-align': 'left'})\n",
    "  compare_models_ = compare_models_.set_table_styles([dict(selector='th', props=[('text-align', 'left')])])\n",
    "                  \n",
    "  return compare_models_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56 th {\n",
       "          text-align: left;\n",
       "    }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row0_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row0_col1 {\n",
       "            background-color:  yellow;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row0_col2 {\n",
       "            background-color:  yellow;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row0_col3 {\n",
       "            background-color:  yellow;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row0_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row0_col5 {\n",
       "            background-color:  yellow;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row1_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row1_col1 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row1_col2 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row1_col3 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row1_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row1_col5 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row2_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row2_col1 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row2_col2 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row2_col3 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row2_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row2_col5 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row3_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row3_col1 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row3_col2 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row3_col3 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row3_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row3_col5 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row4_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row4_col1 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row4_col2 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row4_col3 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row4_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row4_col5 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row5_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row5_col1 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row5_col2 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row5_col3 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row5_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row5_col5 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row6_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row6_col1 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row6_col2 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row6_col3 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row6_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row6_col5 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row7_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row7_col1 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row7_col2 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row7_col3 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row7_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row7_col5 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row8_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row8_col1 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row8_col2 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row8_col3 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row8_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row8_col5 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row9_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row9_col1 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row9_col2 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row9_col3 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row9_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row9_col5 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row10_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row10_col1 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row10_col2 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row10_col3 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row10_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row10_col5 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row11_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row11_col1 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row11_col2 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row11_col3 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row11_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row11_col5 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row12_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row12_col1 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row12_col2 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row12_col3 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row12_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row12_col5 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row13_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row13_col1 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row13_col2 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row13_col3 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row13_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row13_col5 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row14_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row14_col1 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row14_col2 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row14_col3 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row14_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row14_col5 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row15_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row15_col1 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row15_col2 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row15_col3 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row15_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row15_col5 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row16_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row16_col1 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row16_col2 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row16_col3 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row16_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row16_col5 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row17_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row17_col1 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row17_col2 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row17_col3 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row17_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row17_col5 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row18_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row18_col1 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row18_col2 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row18_col3 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row18_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row18_col5 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row19_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row19_col1 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row19_col2 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row19_col3 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row19_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row19_col5 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row20_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row20_col1 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row20_col2 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row20_col3 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row20_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row20_col5 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row21_col0 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row21_col1 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row21_col2 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row21_col3 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row21_col4 {\n",
       "            text-align:  left;\n",
       "        }    #T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row21_col5 {\n",
       "            : ;\n",
       "            text-align:  left;\n",
       "        }</style><table id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Model</th>        <th class=\"col_heading level0 col1\" >MAE</th>        <th class=\"col_heading level0 col2\" >MSE</th>        <th class=\"col_heading level0 col3\" >RMSE</th>        <th class=\"col_heading level0 col4\" >R2</th>        <th class=\"col_heading level0 col5\" >ME</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row0_col0\" class=\"data row0 col0\" >Gradient Boosting Regressor</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row0_col1\" class=\"data row0 col1\" >2.26</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row0_col2\" class=\"data row0 col2\" >10.82</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row0_col3\" class=\"data row0 col3\" >3.22</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row0_col4\" class=\"data row0 col4\" >0.84</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row0_col5\" class=\"data row0 col5\" >11.24</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row1_col0\" class=\"data row1 col0\" >Extra Trees Regressor</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row1_col1\" class=\"data row1 col1\" >2.46</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row1_col2\" class=\"data row1 col2\" >13.59</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row1_col3\" class=\"data row1 col3\" >3.59</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row1_col4\" class=\"data row1 col4\" >0.82</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row1_col5\" class=\"data row1 col5\" >12.66</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row2_col0\" class=\"data row2 col0\" >Random Forest Regressor</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row2_col1\" class=\"data row2 col1\" >2.58</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row2_col2\" class=\"data row2 col2\" >14.93</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row2_col3\" class=\"data row2 col3\" >3.77</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row2_col4\" class=\"data row2 col4\" >0.79</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row2_col5\" class=\"data row2 col5\" >13.17</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row3_col0\" class=\"data row3 col0\" >Ada Boost Regressor</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row3_col1\" class=\"data row3 col1\" >2.91</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row3_col2\" class=\"data row3 col2\" >17.49</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row3_col3\" class=\"data row3 col3\" >4.1</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row3_col4\" class=\"data row3 col4\" >0.76</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row3_col5\" class=\"data row3 col5\" >13.43</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row4_col0\" class=\"data row4 col0\" >Decision Tree Regressor</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row4_col1\" class=\"data row4 col1\" >3.3</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row4_col2\" class=\"data row4 col2\" >23.12</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row4_col3\" class=\"data row4 col3\" >4.71</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row4_col4\" class=\"data row4 col4\" >0.67</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row4_col5\" class=\"data row4 col5\" >16.01</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row5_col0\" class=\"data row5 col0\" >Theil Sen Regressor</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row5_col1\" class=\"data row5 col1\" >3.33</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row5_col2\" class=\"data row5 col2\" >26.8</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row5_col3\" class=\"data row5 col3\" >5.01</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row5_col4\" class=\"data row5 col4\" >0.63</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row5_col5\" class=\"data row5 col5\" >17.68</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row6_col0\" class=\"data row6 col0\" >Kernel Ridge</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row6_col1\" class=\"data row6 col1\" >3.38</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row6_col2\" class=\"data row6 col2\" >27.32</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row6_col3\" class=\"data row6 col3\" >5.07</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row6_col4\" class=\"data row6 col4\" >0.62</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row6_col5\" class=\"data row6 col5\" >18.22</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row7_col0\" class=\"data row7 col0\" >Ridge</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row7_col1\" class=\"data row7 col1\" >3.4</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row7_col2\" class=\"data row7 col2\" >25.75</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row7_col3\" class=\"data row7 col3\" >4.95</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row7_col4\" class=\"data row7 col4\" >0.65</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row7_col5\" class=\"data row7 col5\" >16.83</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row8_col0\" class=\"data row8 col0\" >Linear Regression</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row8_col1\" class=\"data row8 col1\" >3.44</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row8_col2\" class=\"data row8 col2\" >25.5</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row8_col3\" class=\"data row8 col3\" >4.95</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row8_col4\" class=\"data row8 col4\" >0.65</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row8_col5\" class=\"data row8 col5\" >16.68</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row9_col0\" class=\"data row9 col0\" >Bayesian Ridge</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row9_col1\" class=\"data row9 col1\" >3.45</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row9_col2\" class=\"data row9 col2\" >26.46</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row9_col3\" class=\"data row9 col3\" >5.02</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row9_col4\" class=\"data row9 col4\" >0.64</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row9_col5\" class=\"data row9 col5\" >16.81</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row10_col0\" class=\"data row10 col0\" >Least Angle Regression</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row10_col1\" class=\"data row10 col1\" >3.52</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row10_col2\" class=\"data row10 col2\" >26.61</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row10_col3\" class=\"data row10 col3\" >5.06</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row10_col4\" class=\"data row10 col4\" >0.63</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row10_col5\" class=\"data row10 col5\" >16.92</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row11_col0\" class=\"data row11 col0\" >Automatic Relevance Determination</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row11_col1\" class=\"data row11 col1\" >3.54</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row11_col2\" class=\"data row11 col2\" >26.32</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row11_col3\" class=\"data row11 col3\" >5.03</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row11_col4\" class=\"data row11 col4\" >0.64</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row11_col5\" class=\"data row11 col5\" >16.56</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row12_col0\" class=\"data row12 col0\" >Huber Regressor</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row12_col1\" class=\"data row12 col1\" >3.7</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row12_col2\" class=\"data row12 col2\" >33.01</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row12_col3\" class=\"data row12 col3\" >5.52</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row12_col4\" class=\"data row12 col4\" >0.57</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row12_col5\" class=\"data row12 col5\" >18.6</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row13_col0\" class=\"data row13 col0\" >Elastic Net</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row13_col1\" class=\"data row13 col1\" >3.76</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row13_col2\" class=\"data row13 col2\" >29.77</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row13_col3\" class=\"data row13 col3\" >5.35</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row13_col4\" class=\"data row13 col4\" >0.61</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row13_col5\" class=\"data row13 col5\" >16.72</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row14_col0\" class=\"data row14 col0\" >Lasso</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row14_col1\" class=\"data row14 col1\" >3.83</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row14_col2\" class=\"data row14 col2\" >30.98</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row14_col3\" class=\"data row14 col3\" >5.45</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row14_col4\" class=\"data row14 col4\" >0.59</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row14_col5\" class=\"data row14 col5\" >17.02</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row15_col0\" class=\"data row15 col0\" >MLP Regressor</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row15_col1\" class=\"data row15 col1\" >4.36</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row15_col2\" class=\"data row15 col2\" >37.38</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row15_col3\" class=\"data row15 col3\" >6</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row15_col4\" class=\"data row15 col4\" >0.51</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row15_col5\" class=\"data row15 col5\" >18.49</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row16_col0\" class=\"data row16 col0\" >RANSAC Regressor</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row16_col1\" class=\"data row16 col1\" >4.42</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row16_col2\" class=\"data row16 col2\" >50.35</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row16_col3\" class=\"data row16 col3\" >6.7</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row16_col4\" class=\"data row16 col4\" >0.36</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row16_col5\" class=\"data row16 col5\" >23.9</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row17_col0\" class=\"data row17 col0\" >Orthogonal Matching Pursuit</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row17_col1\" class=\"data row17 col1\" >4.62</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row17_col2\" class=\"data row17 col2\" >42.11</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row17_col3\" class=\"data row17 col3\" >6.42</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row17_col4\" class=\"data row17 col4\" >0.41</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row17_col5\" class=\"data row17 col5\" >19.81</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row18_col0\" class=\"data row18 col0\" >K Neighbors Regressor</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row18_col1\" class=\"data row18 col1\" >4.65</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row18_col2\" class=\"data row18 col2\" >46.3</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row18_col3\" class=\"data row18 col3\" >6.73</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row18_col4\" class=\"data row18 col4\" >0.36</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row18_col5\" class=\"data row18 col5\" >21.08</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row19_col0\" class=\"data row19 col0\" >Support Vector Regressor</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row19_col1\" class=\"data row19 col1\" >6.27</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row19_col2\" class=\"data row19 col2\" >77.96</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row19_col3\" class=\"data row19 col3\" >8.75</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row19_col4\" class=\"data row19 col4\" >-0.03</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row19_col5\" class=\"data row19 col5\" >24.52</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row20_col0\" class=\"data row20 col0\" >Lasso Lars</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row20_col1\" class=\"data row20 col1\" >6.47</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row20_col2\" class=\"data row20 col2\" >78.23</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row20_col3\" class=\"data row20 col3\" >8.78</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row20_col4\" class=\"data row20 col4\" >-0.04</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row20_col5\" class=\"data row20 col5\" >23.59</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row21_col0\" class=\"data row21 col0\" >Passive Aggressive Regressor</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row21_col1\" class=\"data row21 col1\" >7.43</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row21_col2\" class=\"data row21 col2\" >99.63</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row21_col3\" class=\"data row21 col3\" >9.56</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row21_col4\" class=\"data row21 col4\" >-0.29</td>\n",
       "                        <td id=\"T_6275f0e8_e4b2_11e9_9759_a0a4c5954f56row21_col5\" class=\"data row21 col5\" >24.18</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x17c31601208>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_models(round=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_model(estimator,\n",
    "                   method = 'Bagging', \n",
    "                   fold = 10,\n",
    "                   n_estimators = 10,\n",
    "                   round = 4,  \n",
    "                   verbose = True):\n",
    "    \n",
    "    #defining X_train and y_train    \n",
    "    data_X = X_train\n",
    "    data_y = y_train\n",
    "  \n",
    "    #ignore co-linearity warnings for qda and lda \n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore') \n",
    "    \n",
    "    #defining estimator as model\n",
    "    model = estimator\n",
    "     \n",
    "    if method == 'Bagging':\n",
    "        from sklearn.ensemble import BaggingClassifier\n",
    "        model = BaggingClassifier(model,bootstrap=True,n_estimators=n_estimators, random_state=seed)\n",
    "        \n",
    "    else:\n",
    "        from sklearn.ensemble import AdaBoostClassifier\n",
    "        model = AdaBoostClassifier(model, random_state=seed)\n",
    "    \n",
    "    kf = StratifiedKFold(fold, random_state=seed)\n",
    "    \n",
    "    score_auc =np.empty((0,0))\n",
    "    score_acc =np.empty((0,0))\n",
    "    score_recall =np.empty((0,0))\n",
    "    score_precision =np.empty((0,0))\n",
    "    score_f1 =np.empty((0,0))\n",
    "    score_kappa =np.empty((0,0))\n",
    "    avgs_auc =np.empty((0,0))\n",
    "    avgs_acc =np.empty((0,0))\n",
    "    avgs_recall =np.empty((0,0))\n",
    "    avgs_precision =np.empty((0,0))\n",
    "    avgs_f1 =np.empty((0,0))\n",
    "    avgs_kappa =np.empty((0,0))\n",
    "    \n",
    "    for train_i , test_i in kf.split(data_X,data_y):\n",
    "        \n",
    "        Xtrain,Xtest = data_X.iloc[train_i], data_X.iloc[test_i]\n",
    "        ytrain,ytest = data_y.iloc[train_i], data_y.iloc[test_i]\n",
    "    \n",
    "        if hasattr(model, 'predict_proba'):\n",
    "        \n",
    "            model.fit(Xtrain,ytrain)\n",
    "            pred_prob = model.predict_proba(Xtest)\n",
    "            pred_prob = pred_prob[:,1]\n",
    "            pred_ = model.predict(Xtest)\n",
    "            sca = metrics.accuracy_score(ytest,pred_)\n",
    "            sc = metrics.roc_auc_score(ytest,pred_prob)\n",
    "            recall = metrics.recall_score(ytest,pred_)\n",
    "            precision = metrics.precision_score(ytest,pred_)\n",
    "            kappa = cohen_kappa_score(ytest,pred_)\n",
    "            f1 = metrics.f1_score(ytest,pred_)\n",
    "            score_acc = np.append(score_acc,sca)\n",
    "            score_auc = np.append(score_auc,sc)\n",
    "            score_recall = np.append(score_recall,recall)\n",
    "            score_precision = np.append(score_precision,precision)\n",
    "            score_f1 =np.append(score_f1,f1)\n",
    "            score_kappa =np.append(score_kappa,kappa)\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            model.fit(Xtrain,ytrain)\n",
    "            pred_prob = 0.00\n",
    "            pred_prob = 0.00\n",
    "            pred_ = model.predict(Xtest)\n",
    "            sca = metrics.accuracy_score(ytest,pred_)\n",
    "            sc = 0.00\n",
    "            recall = metrics.recall_score(ytest,pred_)\n",
    "            precision = metrics.precision_score(ytest,pred_) #change pred_prob to pred_\n",
    "            kappa = cohen_kappa_score(ytest,pred_)\n",
    "            f1 = metrics.f1_score(ytest,pred_)\n",
    "            score_acc = np.append(score_acc,sca)\n",
    "            score_auc = np.append(score_auc,sc)\n",
    "            score_recall = np.append(score_recall,recall)\n",
    "            score_precision = np.append(score_precision,precision)\n",
    "            score_f1 =np.append(score_f1,f1)\n",
    "            score_kappa =np.append(score_kappa,kappa) \n",
    "       \n",
    "    mean_acc=np.mean(score_acc)\n",
    "    mean_auc=np.mean(score_auc)\n",
    "    mean_recall=np.mean(score_recall)\n",
    "    mean_precision=np.mean(score_precision)\n",
    "    mean_f1=np.mean(score_f1)\n",
    "    mean_kappa=np.mean(score_kappa)\n",
    "    std_acc=np.std(score_acc)\n",
    "    std_auc=np.std(score_auc)\n",
    "    std_recall=np.std(score_recall)\n",
    "    std_precision=np.std(score_precision)\n",
    "    std_f1=np.std(score_f1)\n",
    "    std_kappa=np.std(score_kappa)\n",
    "\n",
    "    avgs_acc = np.append(avgs_acc, mean_acc)\n",
    "    avgs_acc = np.append(avgs_acc, std_acc) \n",
    "    avgs_auc = np.append(avgs_auc, mean_auc)\n",
    "    avgs_auc = np.append(avgs_auc, std_auc)\n",
    "    avgs_recall = np.append(avgs_recall, mean_recall)\n",
    "    avgs_recall = np.append(avgs_recall, std_recall)\n",
    "    avgs_precision = np.append(avgs_precision, mean_precision)\n",
    "    avgs_precision = np.append(avgs_precision, std_precision)\n",
    "    avgs_f1 = np.append(avgs_f1, mean_f1)\n",
    "    avgs_f1 = np.append(avgs_f1, std_f1)\n",
    "    avgs_kappa = np.append(avgs_kappa, mean_kappa)\n",
    "    avgs_kappa = np.append(avgs_kappa, std_kappa)\n",
    "\n",
    "    model_results = pd.DataFrame({'Accuracy': score_acc, 'AUC': score_auc, 'Recall' : score_recall, 'Prec.' : score_precision , \n",
    "                     'F1' : score_f1, 'Kappa' : score_kappa})\n",
    "    model_results_unpivot = pd.melt(model_results,value_vars=['Accuracy', 'AUC', 'Recall', 'Prec.', 'F1', 'Kappa'])\n",
    "    model_results_unpivot.columns = ['Metric', 'Measure']\n",
    "    model_avgs = pd.DataFrame({'Accuracy': avgs_acc, 'AUC': avgs_auc, 'Recall' : avgs_recall, 'Prec.' : avgs_precision , \n",
    "                     'F1' : avgs_f1, 'Kappa' : avgs_kappa},index=['Mean', 'SD'])\n",
    "\n",
    "    model_results = model_results.append(model_avgs)\n",
    "    model_results = model_results.round(round)  \n",
    "    \n",
    "    model = model\n",
    "    \n",
    "    if verbose:\n",
    "        display(model_results)\n",
    "        return model\n",
    "    else:\n",
    "        return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model(estimator, \n",
    "               plot = 'auc', \n",
    "               manifold='tsne',\n",
    "               features=5):\n",
    "    \n",
    "    model = estimator\n",
    "    \n",
    "    if plot == 'auc':\n",
    "        from yellowbrick.classifier import ROCAUC\n",
    "        visualizer = ROCAUC(model)\n",
    "        visualizer.fit(X_train, y_train)\n",
    "        visualizer.score(X_test, y_test)\n",
    "        visualizer.poof()\n",
    "        \n",
    "    elif plot == 'threshold':\n",
    "        from yellowbrick.classifier import DiscriminationThreshold\n",
    "        visualizer = DiscriminationThreshold(model, random_state=seed)\n",
    "        visualizer.fit(X_train, y_train)\n",
    "        visualizer.score(X_test, y_test)\n",
    "        visualizer.poof()\n",
    "    \n",
    "    elif plot == 'pr':\n",
    "        from yellowbrick.classifier import PrecisionRecallCurve\n",
    "        visualizer = PrecisionRecallCurve(model, random_state=seed)\n",
    "        visualizer.fit(X_train, y_train)\n",
    "        visualizer.score(X_test, y_test)\n",
    "        visualizer.poof()\n",
    "\n",
    "    elif plot == 'confusion_matrix':\n",
    "        from yellowbrick.classifier import ConfusionMatrix\n",
    "        visualizer = ConfusionMatrix(model, random_state=seed, fontsize=15, cmap=\"Greens\")\n",
    "        visualizer.fit(X_train, y_train)\n",
    "        visualizer.score(X_test, y_test)\n",
    "        visualizer.poof()\n",
    "    \n",
    "    elif plot == 'error':\n",
    "        from yellowbrick.classifier import ClassPredictionError\n",
    "        visualizer = ClassPredictionError(model, random_state=seed)\n",
    "        visualizer.fit(X_train, y_train)\n",
    "        visualizer.score(X_test, y_test)\n",
    "        visualizer.poof()\n",
    "\n",
    "    elif plot == 'class_report':\n",
    "        from yellowbrick.classifier import ClassificationReport\n",
    "        visualizer = ClassificationReport(model, random_state=seed, support=True)\n",
    "        visualizer.fit(X_train, y_train)\n",
    "        visualizer.score(X_test, y_test)\n",
    "        visualizer.poof()\n",
    "        \n",
    "    elif plot == 'boundary':\n",
    "        \n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        from sklearn.decomposition import PCA\n",
    "        from yellowbrick.contrib.classifier import DecisionViz        \n",
    "\n",
    "        X_train_transformed = X_train.select_dtypes(include='float64')\n",
    "        X_test_transformed = X_test.select_dtypes(include='float64')\n",
    "        X_train_transformed = StandardScaler().fit_transform(X_train_transformed)\n",
    "        X_test_transformed = StandardScaler().fit_transform(X_test_transformed)\n",
    "        pca = PCA(n_components=2, random_state = seed)\n",
    "        X_train_transformed = pca.fit_transform(X_train_transformed)\n",
    "        X_test_transformed = pca.fit_transform(X_test_transformed)\n",
    "\n",
    "        y_train_transformed = np.array(y_train)\n",
    "        y_test_transformed = np.array(y_test)\n",
    "        \n",
    "        model_transformed = model\n",
    "        \n",
    "        viz = DecisionViz(model_transformed)\n",
    "        viz.fit(X_train_transformed, y_train_transformed, features=['Feature One', 'Feature Two'], classes=['A', 'B'])\n",
    "        viz.draw(X_test_transformed, y_test_transformed)\n",
    "        viz.poof()\n",
    "        \n",
    "    elif plot == 'rfe':\n",
    "        from yellowbrick.model_selection import RFECV    \n",
    "        visualizer = RFECV(model, cv=10)\n",
    "        visualizer.fit(X_train, y_train)\n",
    "        visualizer.poof()\n",
    "        \n",
    "        \n",
    "    elif plot == 'learning':\n",
    "        from yellowbrick.model_selection import LearningCurve\n",
    "        sizes = np.linspace(0.3, 1.0, 10)  \n",
    "        visualizer = LearningCurve(model, cv=10, scoring='f1_weighted', train_sizes=sizes, n_jobs=1, random_state=seed)\n",
    "        visualizer.fit(X_train, y_train)\n",
    "        visualizer.poof()\n",
    "        \n",
    "    \n",
    "    elif plot == 'manifold':\n",
    "        from yellowbrick.features import Manifold\n",
    "        X_train_transformed = X_train.select_dtypes(include='float64') \n",
    "        visualizer = Manifold(manifold=manifold, random_state = seed)\n",
    "        visualizer.fit_transform(X_train_transformed, y_train)\n",
    "        visualizer.poof()       \n",
    "        \n",
    "    elif plot == 'calibration':      \n",
    "                \n",
    "        from sklearn.calibration import calibration_curve\n",
    "        \n",
    "        model_name = str(model).split(\"(\")[0]\n",
    "        \n",
    "        plt.figure(figsize=(7, 6))\n",
    "        ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "\n",
    "        ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "        #model.fit(X_train, y_train)\n",
    "        prob_pos = model.predict_proba(X_test)[:, 1]\n",
    "        prob_pos = (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())\n",
    "        fraction_of_positives, mean_predicted_value = calibration_curve(y_test, prob_pos, n_bins=10)\n",
    "\n",
    "        ax1.plot(mean_predicted_value, fraction_of_positives, \"s-\",label=\"%s\" % (model_name, ))\n",
    "\n",
    "        ax1.set_ylabel(\"Fraction of positives\")\n",
    "        ax1.set_ylim([0, 1])\n",
    "        ax1.set_xlim([0, 1])\n",
    "        ax1.legend(loc=\"lower right\")\n",
    "        ax1.set_title('Calibration plots  (reliability curve)')\n",
    "        ax1.set_facecolor('white')\n",
    "        ax1.grid(b=True, color='grey', linewidth=0.5, linestyle = '-')\n",
    "        plt.tight_layout()\n",
    "        plt.show() \n",
    "        \n",
    "    elif plot == 'vc':\n",
    "    \n",
    "        if hasattr(model, 'max_depth'):\n",
    "            param_name='max_depth'\n",
    "        else:\n",
    "            param_name='xxx'\n",
    "       \n",
    "        from yellowbrick.model_selection import ValidationCurve\n",
    "        viz = ValidationCurve(model, param_name=param_name, param_range=np.arange(1,11), scoring='f1_weighted',cv=10, \n",
    "                              random_state=seed)\n",
    "        viz.fit(X_train, y_train)\n",
    "        viz.poof()\n",
    "        \n",
    "    elif plot == 'dimension':\n",
    "    \n",
    "        from yellowbrick.features import RadViz\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        from sklearn.decomposition import PCA\n",
    "\n",
    "        X_train_transformed = X_train.select_dtypes(include='float64') \n",
    "        X_train_transformed = StandardScaler().fit_transform(X_train_transformed)\n",
    "        y_train_transformed = np.array(y_train)\n",
    "\n",
    "        pca = PCA(n_components=features, random_state=seed)\n",
    "        X_train_transformed = pca.fit_transform(X_train_transformed)\n",
    "\n",
    "        classes = [\"1\", \"0\"]\n",
    "        visualizer = RadViz(classes=classes, alpha=0.25)\n",
    "        visualizer.fit(X_train_transformed, y_train_transformed)     \n",
    "        visualizer.transform(X_train_transformed)\n",
    "        visualizer.poof()\n",
    "        \n",
    "    elif plot == 'feature':\n",
    "        variables = abs(model.coef_[0])\n",
    "        col_names = np.array(X_train.columns)\n",
    "        coef_df = pd.DataFrame({'Variable': X_train.columns, 'Value': variables})\n",
    "        sorted_df = coef_df.sort_values(by='Value')\n",
    "        my_range=range(1,len(sorted_df.index)+1)\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.hlines(y=my_range, xmin=0, xmax=sorted_df['Value'], color='skyblue')\n",
    "        plt.plot(sorted_df['Value'], my_range, \"o\")\n",
    "        plt.yticks(my_range, sorted_df['Variable'])\n",
    "        plt.title(\"Feature Importance Plot\")\n",
    "        plt.xlabel('Variable Importance')\n",
    "        plt.ylabel('Features') \n",
    "        var_imp = sorted_df.reset_index(drop=True)\n",
    "        var_imp_array = np.array(var_imp['Variable'])\n",
    "        var_imp_array_top_n = var_imp_array[0:len(var_imp_array)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(model_library = None, \n",
    "                   fold = 10, \n",
    "                   round = 4, \n",
    "                   sort = 'Accuracy', \n",
    "                   blacklist = None):\n",
    "  \n",
    "  #ignore warnings\n",
    "  import warnings\n",
    "  warnings.filterwarnings('ignore') \n",
    "    \n",
    "  #defining X_train and y_train\n",
    "  data_X = X_train\n",
    "  data_y=y_train\n",
    "\n",
    "  from sklearn.linear_model import LogisticRegression\n",
    "  from sklearn.neighbors import KNeighborsClassifier\n",
    "  from sklearn.naive_bayes import GaussianNB\n",
    "  from sklearn.tree import DecisionTreeClassifier\n",
    "  from sklearn.svm import SVC\n",
    "  from sklearn.gaussian_process.kernels import RBF\n",
    "  from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "  from sklearn.neural_network import MLPClassifier\n",
    "  from sklearn.model_selection import StratifiedKFold\n",
    "  from sklearn import preprocessing as pre\n",
    "  from sklearn.pipeline import Pipeline as pipe\n",
    "  from sklearn.model_selection import train_test_split\n",
    "  from sklearn.linear_model import RidgeClassifier\n",
    "  from sklearn.linear_model import Lasso\n",
    "  from sklearn.linear_model import LogisticRegression\n",
    "  from sklearn.linear_model import SGDClassifier\n",
    "  from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "  from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "  from sklearn import metrics\n",
    "  from sklearn.ensemble import RandomForestClassifier\n",
    "  from sklearn.ensemble import ExtraTreesClassifier\n",
    "  from sklearn.model_selection import GridSearchCV\n",
    "  from sklearn.model_selection import RandomizedSearchCV\n",
    "  from scipy import stats\n",
    "  from sklearn.model_selection import cross_val_score\n",
    "  from sklearn.model_selection import cross_val_predict\n",
    "  from sklearn.model_selection import cross_validate\n",
    "  from sklearn.ensemble import AdaBoostClassifier\n",
    "  from sklearn.ensemble import GradientBoostingClassifier\n",
    "  from sklearn.ensemble import BaggingClassifier\n",
    "  from sklearn.ensemble import RandomForestClassifier\n",
    "  from sklearn.model_selection import StratifiedKFold\n",
    "  from sklearn.metrics import roc_auc_score\n",
    "  from sklearn.metrics import cohen_kappa_score\n",
    "  import numpy as np\n",
    "  import pandas as pd\n",
    "  import pandas_profiling as pd_p\n",
    "  import seaborn as sns\n",
    "  import random\n",
    "  import pandas.io.formats.style\n",
    "\n",
    "  lr = LogisticRegression(random_state=seed)\n",
    "  knn = KNeighborsClassifier()\n",
    "  nb = GaussianNB()\n",
    "  dt = DecisionTreeClassifier(random_state=seed)\n",
    "  svm = SGDClassifier(max_iter=1000, tol=0.001, random_state=seed)\n",
    "  rbfsvm = SVC(gamma='auto', C=1, probability=True, kernel='rbf', random_state=seed)\n",
    "  gpc = GaussianProcessClassifier(random_state=seed)\n",
    "  mlp = MLPClassifier(max_iter=500, random_state=seed)\n",
    "  ridge = RidgeClassifier(random_state=seed)\n",
    "  rf = RandomForestClassifier(n_estimators=10, random_state=seed)\n",
    "  qda = QuadraticDiscriminantAnalysis()\n",
    "  ada = AdaBoostClassifier(random_state=seed)\n",
    "  gbc = GradientBoostingClassifier(random_state=seed)\n",
    "  lda = LinearDiscriminantAnalysis()\n",
    "  et = ExtraTreesClassifier(random_state=seed)\n",
    "  \n",
    "  #blacklist models\n",
    "\n",
    "  if model_library != None:\n",
    "    \n",
    "    model_library = model_library\n",
    "    \n",
    "    model_names = []\n",
    "    \n",
    "    for names in model_library:\n",
    "        \n",
    "        model_names = np.append(model_names, str(names).split(\"(\")[0])\n",
    "        \n",
    "        import re \n",
    "        \n",
    "        def putSpace(input):\n",
    "            words = re.findall('[A-Z][a-z]*', input)\n",
    "            words = ' '.join(words)\n",
    "            return words  \n",
    "\n",
    "        model_names_modified = []\n",
    "        for i in model_names:\n",
    "            model_names_modified.append(putSpace(i))\n",
    "            \n",
    "        model_names_modified = []\n",
    "        for i in model_names:\n",
    "            model_names_modified.append(putSpace(i))\n",
    "\n",
    "        model_names = model_names_modified\n",
    "\n",
    "        model_names_final = []\n",
    "        for j in model_names:\n",
    "            if j == 'Gaussian N B':\n",
    "                model_names_final.append('Naive Bayes')\n",
    "            elif j == 'M L P Classifier':\n",
    "                model_names_final.append('MLP Classifier')\n",
    "            elif j == 'S G D Classifier':\n",
    "                model_names_final.append('SVM - Linear Kernel')\n",
    "            elif j == 'S V C':\n",
    "                model_names_final.append('SVM - Radial Kernel')\n",
    "            else: \n",
    "                model_names_final.append(j)\n",
    "\n",
    "        model_names = model_names_final    \n",
    "    \n",
    "  else:\n",
    "        \n",
    "    if blacklist == None:\n",
    "        \n",
    "        model_library = [lr, knn, nb, dt, svm, rbfsvm, gpc, mlp, ridge, rf, qda, ada, gbc, lda, et]\n",
    "    \n",
    "        model_names = []\n",
    "    \n",
    "        for names in model_library:\n",
    "            model_names = np.append(model_names, str(names).split(\"(\")[0])\n",
    "    \n",
    "        import re \n",
    "\n",
    "        def putSpace(input):\n",
    "            words = re.findall('[A-Z][a-z]*', input)\n",
    "            words = ' '.join(words)\n",
    "            return words  \n",
    "\n",
    "        model_names_modified = []\n",
    "        for i in model_names:\n",
    "            model_names_modified.append(putSpace(i))\n",
    "\n",
    "        model_names = model_names_modified\n",
    "\n",
    "        model_names_final = []\n",
    "        for j in model_names:\n",
    "            if j == 'Gaussian N B':\n",
    "                model_names_final.append('Naive Bayes')\n",
    "            elif j == 'M L P Classifier':\n",
    "                model_names_final.append('MLP Classifier')\n",
    "            elif j == 'S G D Classifier':\n",
    "                model_names_final.append('SVM - Linear Kernel')\n",
    "            elif j == 'S V C':\n",
    "                model_names_final.append('SVM - Radial Kernel')\n",
    "            else: \n",
    "                model_names_final.append(j)\n",
    "\n",
    "        model_names = model_names_final\n",
    "\n",
    "    else:\n",
    "        \n",
    "        model_library_values = ['lr', 'knn', 'nb', 'dt', 'svm', 'rbfsvm', 'gpc', 'mlp', 'ridge', 'rf', 'qda', \n",
    "                        'ada', 'gbc', 'lda', 'et']\n",
    "\n",
    "        location = []\n",
    "\n",
    "        for item in blacklist:\n",
    "            location.append(model_library_values.index(item))\n",
    "\n",
    "        model_library = [lr, knn, nb, dt, svm, rbfsvm, gpc, mlp, ridge, rf, qda, ada, gbc, lda, et]\n",
    "\n",
    "        for i in location:\n",
    "            del model_library[i]\n",
    "\n",
    "        model_names = []\n",
    "\n",
    "        for names in model_library:\n",
    "            model_names = np.append(model_names, str(names).split(\"(\")[0])\n",
    "\n",
    "        import re\n",
    "\n",
    "        def putSpace(input):\n",
    "            words = re.findall('[A-Z][a-z]*', input)\n",
    "            words = ' '.join(words)\n",
    "            return words  \n",
    "\n",
    "        model_names_modified = []\n",
    "        for i in model_names:\n",
    "            model_names_modified.append(putSpace(i))\n",
    "\n",
    "        model_names = model_names_modified\n",
    "\n",
    "        model_names_final = []\n",
    "        for j in model_names:\n",
    "            if j == 'Gaussian N B':\n",
    "                model_names_final.append('Naive Bayes')\n",
    "            elif j == 'M L P Classifier':\n",
    "                model_names_final.append('MLP Classifier')\n",
    "            elif j == 'S G D Classifier':\n",
    "                model_names_final.append('SVM - Linear Kernel')\n",
    "            elif j == 'S V C':\n",
    "                model_names_final.append('SVM - Radial Kernel')\n",
    "            else: \n",
    "                model_names_final.append(j)\n",
    "\n",
    "        model_names = model_names_final\n",
    "\n",
    "  kf = StratifiedKFold(fold, random_state=seed)\n",
    "\n",
    "  score_acc =np.empty((0,0))\n",
    "  score_auc =np.empty((0,0))\n",
    "  score_recall =np.empty((0,0))\n",
    "  score_precision =np.empty((0,0))\n",
    "  score_f1 =np.empty((0,0))\n",
    "  score_kappa =np.empty((0,0))\n",
    "  score_acc_running = np.empty((0,0)) ##running total\n",
    "  avg_acc = np.empty((0,0))\n",
    "  avg_auc = np.empty((0,0))\n",
    "  avg_recall = np.empty((0,0))\n",
    "  avg_precision = np.empty((0,0))\n",
    "  avg_f1 = np.empty((0,0))\n",
    "  avg_kappa = np.empty((0,0))\n",
    "      \n",
    "  for model in model_library:\n",
    " \n",
    "    for train_i , test_i in kf.split(data_X,data_y):\n",
    "     \n",
    "        Xtrain,Xtest = data_X.iloc[train_i], data_X.iloc[test_i]\n",
    "        ytrain,ytest = data_y.iloc[train_i], data_y.iloc[test_i]\n",
    "        \n",
    "        if hasattr(model, 'predict_proba'):\n",
    "        \n",
    "          model.fit(Xtrain,ytrain)\n",
    "          pred_prob = model.predict_proba(Xtest)\n",
    "          pred_prob = pred_prob[:,1]\n",
    "          pred_ = model.predict(Xtest)\n",
    "          sca = metrics.accuracy_score(ytest,pred_)\n",
    "          sc = metrics.roc_auc_score(ytest,pred_prob)\n",
    "          recall = metrics.recall_score(ytest,pred_)\n",
    "          precision = metrics.precision_score(ytest,pred_)\n",
    "          kappa = cohen_kappa_score(ytest,pred_)\n",
    "          f1 = metrics.f1_score(ytest,pred_)\n",
    "          score_acc = np.append(score_acc,sca)\n",
    "          score_auc = np.append(score_auc,sc)\n",
    "          score_recall = np.append(score_recall,recall)\n",
    "          score_precision = np.append(score_precision,precision)\n",
    "          score_f1 =np.append(score_f1,f1)\n",
    "          score_kappa =np.append(score_kappa,kappa)              \n",
    "        \n",
    "        else:        \n",
    "        \n",
    "          model.fit(Xtrain,ytrain)\n",
    "          pred_prob = 0.00\n",
    "          pred_prob = 0.00\n",
    "          pred_ = model.predict(Xtest)\n",
    "          sca = metrics.accuracy_score(ytest,pred_)\n",
    "          sc = 0.00\n",
    "          recall = metrics.recall_score(ytest,pred_)\n",
    "          precision = metrics.precision_score(ytest,pred_) #change pred_prob to pred_\n",
    "          kappa = cohen_kappa_score(ytest,pred_)\n",
    "          f1 = metrics.f1_score(ytest,pred_)\n",
    "          score_acc = np.append(score_acc,sca)\n",
    "          score_auc = np.append(score_auc,sc)\n",
    "          score_recall = np.append(score_recall,recall)\n",
    "          score_precision = np.append(score_precision,precision)\n",
    "          score_f1 =np.append(score_f1,f1)\n",
    "          score_kappa =np.append(score_kappa,kappa) \n",
    "        \n",
    "    avg_acc = np.append(avg_acc,np.mean(score_acc))\n",
    "    avg_auc = np.append(avg_auc,np.mean(score_auc))\n",
    "    avg_recall = np.append(avg_recall,np.mean(score_recall))\n",
    "    avg_precision = np.append(avg_precision,np.mean(score_precision))\n",
    "    avg_f1 = np.append(avg_f1,np.mean(score_f1))\n",
    "    avg_kappa = np.append(avg_kappa,np.mean(score_kappa))\n",
    "    score_acc =np.empty((0,0))\n",
    "    score_auc =np.empty((0,0))\n",
    "    score_recall =np.empty((0,0))\n",
    "    score_precision =np.empty((0,0))\n",
    "    score_f1 =np.empty((0,0))\n",
    "    score_kappa =np.empty((0,0))\n",
    "  \n",
    "  def highlight_max(s):\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "  compare_models_ = pd.DataFrame({'Model':model_names, 'Accuracy':avg_acc, 'AUC':avg_auc, \n",
    "                     'Recall':avg_recall, 'Prec.':avg_precision, \n",
    "                     'F1':avg_f1, 'Kappa': avg_kappa}).round(round).sort_values(by=[sort], \n",
    "                      ascending=False).reset_index(drop=True).style.apply(highlight_max,subset=['Accuracy','AUC','Recall',\n",
    "                      'Prec.','F1','Kappa'])\n",
    "  compare_models_ = compare_models_.set_properties(**{'text-align': 'left'})\n",
    "  compare_models_ = compare_models_.set_table_styles([dict(selector='th', props=[('text-align', 'left')])])\n",
    "                  \n",
    "  return compare_models_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model(estimator = None, \n",
    "               fold = 10, \n",
    "               round = 4, \n",
    "               n_iter = 10, \n",
    "               optimize = 'accuracy',\n",
    "               ensemble = False, \n",
    "               method = 'Bagging',\n",
    "               verbose = True):\n",
    "   \n",
    "  data_X = X_train\n",
    "  data_y = y_train\n",
    "\n",
    "  from sklearn.linear_model import LogisticRegression\n",
    "  from sklearn.neighbors import KNeighborsClassifier\n",
    "  from sklearn.naive_bayes import GaussianNB\n",
    "  from sklearn.tree import DecisionTreeClassifier\n",
    "  from sklearn.svm import SVC\n",
    "  from sklearn.gaussian_process.kernels import RBF\n",
    "  from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "  from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "  from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "  from sklearn.neural_network import MLPClassifier\n",
    "  from sklearn.model_selection import StratifiedKFold  \n",
    "  from sklearn.model_selection import StratifiedKFold\n",
    "  from sklearn.linear_model import RidgeClassifier\n",
    "  import numpy as np\n",
    "  import pandas as pd\n",
    "  import pandas_profiling as pd_p\n",
    "  import seaborn as sns\n",
    "  from sklearn import preprocessing as pre\n",
    "  from sklearn.pipeline import Pipeline as pipe\n",
    "  from sklearn.model_selection import train_test_split\n",
    "  from sklearn.linear_model import Lasso\n",
    "  from sklearn.linear_model import LogisticRegression\n",
    "  from sklearn.linear_model import SGDClassifier\n",
    "  from sklearn import metrics\n",
    "  from sklearn.ensemble import RandomForestClassifier\n",
    "  from sklearn.ensemble import ExtraTreesClassifier\n",
    "  from sklearn.model_selection import GridSearchCV\n",
    "  from sklearn.model_selection import RandomizedSearchCV\n",
    "  from scipy import stats\n",
    "  import random\n",
    "  from sklearn.model_selection import cross_val_score\n",
    "  from sklearn.model_selection import cross_val_predict\n",
    "  from sklearn.model_selection import cross_validate\n",
    "  from sklearn.ensemble import AdaBoostClassifier\n",
    "  from sklearn.ensemble import GradientBoostingClassifier\n",
    "  from sklearn.model_selection import StratifiedKFold\n",
    "  from sklearn.metrics import roc_auc_score\n",
    "  from sklearn.metrics import cohen_kappa_score\n",
    "  from sklearn.ensemble import BaggingClassifier\n",
    "    \n",
    "  kf = StratifiedKFold(fold, random_state=seed)\n",
    "\n",
    "  score_auc =np.empty((0,0))\n",
    "  score_acc =np.empty((0,0))\n",
    "  score_recall =np.empty((0,0))\n",
    "  score_precision =np.empty((0,0))\n",
    "  score_f1 =np.empty((0,0))\n",
    "  score_kappa =np.empty((0,0))\n",
    "  avgs_auc =np.empty((0,0))\n",
    "  avgs_acc =np.empty((0,0))\n",
    "  avgs_recall =np.empty((0,0))\n",
    "  avgs_precision =np.empty((0,0))\n",
    "  avgs_f1 =np.empty((0,0))\n",
    "  avgs_kappa =np.empty((0,0))\n",
    "    \n",
    "  if estimator == 'knn':\n",
    "    \n",
    "    param_grid = {'n_neighbors': range(1,51),\n",
    "             'weights' : ['uniform', 'distance'],\n",
    "             'metric':[\"euclidean\", \"manhattan\"]\n",
    "                 }        \n",
    "    model_grid = RandomizedSearchCV(estimator=KNeighborsClassifier(), param_distributions=param_grid, \n",
    "                                    scoring=optimize, n_iter=n_iter, cv=fold, random_state=seed,\n",
    "                                   n_jobs=-1, iid=False)\n",
    " \n",
    "    model_grid.fit(X_train,y_train)\n",
    "    model = model_grid.best_estimator_\n",
    "    best_model = model_grid.best_estimator_\n",
    "    best_model_param = model_grid.best_params_\n",
    " \n",
    "  elif estimator == 'lr':\n",
    "\n",
    "    param_grid = {'C': [1,5,10,25,50,100],\n",
    "              \"penalty\": [ 'l1', 'l2'],\n",
    "              \"class_weight\": [\"balanced\", None]\n",
    "                 }\n",
    "    model_grid = RandomizedSearchCV(estimator=LogisticRegression(random_state=seed), \n",
    "                                    param_distributions=param_grid, scoring=optimize, n_iter=n_iter, cv=fold, \n",
    "                                    random_state=seed, iid=False,n_jobs=-1)\n",
    "    model_grid.fit(X_train,y_train)\n",
    "    model = model_grid.best_estimator_\n",
    "    best_model = model_grid.best_estimator_\n",
    "    best_model_param = model_grid.best_params_\n",
    " \n",
    "  elif estimator == 'dt':\n",
    "        \n",
    "    param_grid = {\"max_depth\": np.random.randint(3, (len(X_train.columns)*.85),4),\n",
    "              \"max_features\": np.random.randint(3, len(X_train.columns),4),\n",
    "              \"min_samples_leaf\": [2,3,4],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "    \n",
    "    model_grid = RandomizedSearchCV(estimator=DecisionTreeClassifier(random_state=seed), param_distributions=param_grid,\n",
    "                                   scoring=optimize, n_iter=n_iter, cv=fold, random_state=seed,\n",
    "                                   iid=False, n_jobs=-1)\n",
    " \n",
    "    model_grid.fit(X_train,y_train)\n",
    "    model = model_grid.best_estimator_\n",
    "    best_model = model_grid.best_estimator_\n",
    "    best_model_param = model_grid.best_params_\n",
    " \n",
    "  elif estimator == 'mlp':\n",
    "    \n",
    "    param_grid = {'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "             'solver' : ['lbfgs', 'sgd', 'adam'],\n",
    "             'alpha': [0.0001, 0.05],\n",
    "             'hidden_layer_sizes': np.random.randint(5,15,5),\n",
    "             'activation': [\"tanh\", \"identity\", \"logistic\",\"relu\"]\n",
    "             }\n",
    "   \n",
    "    model_grid = RandomizedSearchCV(estimator=MLPClassifier(max_iter=1000, random_state=seed), \n",
    "                                    param_distributions=param_grid, scoring=optimize, n_iter=n_iter, cv=fold, \n",
    "                                    random_state=seed, iid=False, n_jobs=-1)\n",
    " \n",
    "    model_grid.fit(X_train,y_train)\n",
    "    model = model_grid.best_estimator_\n",
    "    best_model = model_grid.best_estimator_\n",
    "    best_model_param = model_grid.best_params_\n",
    "    \n",
    "  elif estimator == 'gpc':\n",
    "    \n",
    "    param_grid = {\"max_iter_predict\":[100,200,300,400,500,600,700,800,900,1000]}\n",
    "   \n",
    "    model_grid = RandomizedSearchCV(estimator=GaussianProcessClassifier(random_state=seed), param_distributions=param_grid,\n",
    "                                   scoring=optimize, n_iter=n_iter, cv=fold, random_state=seed,\n",
    "                                   n_jobs=-1)\n",
    " \n",
    "    model_grid.fit(X_train,y_train)\n",
    "    model = model_grid.best_estimator_\n",
    "    best_model = model_grid.best_estimator_\n",
    "    best_model_param = model_grid.best_params_    \n",
    "    \n",
    "  elif estimator == 'rbfsvm':\n",
    "\n",
    "    param_grid = {'C': [.5,1,10,50,100],\n",
    "            \"class_weight\": [\"balanced\", None]}\n",
    "    \n",
    "    model_grid = RandomizedSearchCV(estimator=SVC(gamma='auto', C=1, probability=True, kernel='rbf', random_state=seed), \n",
    "                                    param_distributions=param_grid, scoring=optimize, n_iter=n_iter, \n",
    "                                    cv=fold, random_state=seed, n_jobs=-1)\n",
    " \n",
    "    model_grid.fit(X_train,y_train)\n",
    "    model = model_grid.best_estimator_\n",
    "    best_model = model_grid.best_estimator_\n",
    "    best_model_param = model_grid.best_params_    \n",
    "\n",
    "    \n",
    "  elif estimator == 'nb':\n",
    "\n",
    "    param_grid = {'var_smoothing': [0.000000001, 0.0000001, 0.00001, 0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007,\n",
    "                                    0.008, 0.009, 0.01, 0.1, 1]}\n",
    "    \n",
    "    model_grid = RandomizedSearchCV(estimator=GaussianNB(), \n",
    "                                    param_distributions=param_grid, scoring=optimize, n_iter=n_iter, \n",
    "                                    cv=fold, random_state=seed, n_jobs=-1)\n",
    " \n",
    "    model_grid.fit(X_train,y_train)\n",
    "    model = model_grid.best_estimator_\n",
    "    best_model = model_grid.best_estimator_\n",
    "    best_model_param = model_grid.best_params_        \n",
    "\n",
    "  elif estimator == 'svm':\n",
    "   \n",
    "    param_grid = {'penalty': ['l2', 'l1','elasticnet'],\n",
    "                  'l1_ratio': [0,0.1,0.15,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],\n",
    "                  'alpha': [0.0001, 0.001, 0.01, 0.0002, 0.002, 0.02, 0.0005, 0.005, 0.05],\n",
    "                  'fit_intercept': [True, False],\n",
    "                  'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "                  'eta0': [0.001, 0.01,0.05,0.1,0.2,0.3,0.4,0.5]\n",
    "                 }    \n",
    "    \n",
    "    model_grid = RandomizedSearchCV(estimator=SGDClassifier(loss='hinge', random_state=seed), \n",
    "                                    param_distributions=param_grid, scoring=optimize, n_iter=n_iter, \n",
    "                                    cv=fold, random_state=seed, n_jobs=-1)\n",
    "    \n",
    "    model_grid.fit(X_train,y_train)\n",
    "    model = model_grid.best_estimator_\n",
    "    best_model = model_grid.best_estimator_\n",
    "    best_model_param = model_grid.best_params_     \n",
    "\n",
    "  elif estimator == 'ridge':\n",
    "\n",
    "    param_grid = {'alpha': [0.0001,0.001,0.1,0.15,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],\n",
    "                  'fit_intercept': [True, False],\n",
    "                  'normalize': [True, False]\n",
    "                 }    \n",
    "    \n",
    "    model_grid = RandomizedSearchCV(estimator=RidgeClassifier(random_state=seed), \n",
    "                                    param_distributions=param_grid, scoring=optimize, n_iter=n_iter, \n",
    "                                    cv=fold, random_state=seed, n_jobs=-1)\n",
    "    \n",
    "    model_grid.fit(X_train,y_train)\n",
    "    model = model_grid.best_estimator_\n",
    "    best_model = model_grid.best_estimator_\n",
    "    best_model_param = model_grid.best_params_     \n",
    "   \n",
    "  elif estimator == 'rf':\n",
    "\n",
    "    param_grid = {'n_estimators': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "                  'criterion': ['gini', 'entropy'],\n",
    "                  'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],\n",
    "                  'min_samples_split': [2, 5, 7, 9, 10],\n",
    "                  'min_samples_leaf' : [1, 2, 4],\n",
    "                  'max_features' : ['auto', 'sqrt', 'log2'],\n",
    "                  'bootstrap': [True, False]\n",
    "                 }    \n",
    "    \n",
    "    model_grid = RandomizedSearchCV(estimator=RandomForestClassifier(random_state=seed), \n",
    "                                    param_distributions=param_grid, scoring=optimize, n_iter=n_iter, \n",
    "                                    cv=fold, random_state=seed, n_jobs=-1)\n",
    "    \n",
    "    model_grid.fit(X_train,y_train)\n",
    "    model = model_grid.best_estimator_\n",
    "    best_model = model_grid.best_estimator_\n",
    "    best_model_param = model_grid.best_params_     \n",
    "   \n",
    "  elif estimator == 'ada':\n",
    "\n",
    "    param_grid = {'n_estimators': [10, 40, 70, 80, 90, 100, 120, 140, 150],\n",
    "                  'learning_rate': [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],\n",
    "                  'algorithm' : [\"SAMME\", \"SAMME.R\"]\n",
    "                 }    \n",
    "    \n",
    "    model_grid = RandomizedSearchCV(estimator=AdaBoostClassifier(random_state=seed), \n",
    "                                    param_distributions=param_grid, scoring=optimize, n_iter=n_iter, \n",
    "                                    cv=fold, random_state=seed, n_jobs=-1)\n",
    "    \n",
    "    model_grid.fit(X_train,y_train)\n",
    "    model = model_grid.best_estimator_\n",
    "    best_model = model_grid.best_estimator_\n",
    "    best_model_param = model_grid.best_params_   \n",
    "\n",
    "  elif estimator == 'gbc':\n",
    "\n",
    "    param_grid = {'loss': ['deviance', 'exponential'],\n",
    "                  'n_estimators': [10, 40, 70, 80, 90, 100, 120, 140, 150],\n",
    "                  'learning_rate': [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],\n",
    "                  'subsample' : [0.1,0.3,0.5,0.7,0.9,1],\n",
    "                  'min_samples_split' : [2,4,5,7,9,10],\n",
    "                  'min_samples_leaf' : [1,2,3,4,5],\n",
    "                  'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],\n",
    "                  'max_features' : ['auto', 'sqrt', 'log2']\n",
    "                 }    \n",
    "    \n",
    "    model_grid = RandomizedSearchCV(estimator=GradientBoostingClassifier(random_state=seed), \n",
    "                                    param_distributions=param_grid, scoring=optimize, n_iter=n_iter, \n",
    "                                    cv=fold, random_state=seed, n_jobs=-1)\n",
    "    \n",
    "    model_grid.fit(X_train,y_train)\n",
    "    model = model_grid.best_estimator_\n",
    "    best_model = model_grid.best_estimator_\n",
    "    best_model_param = model_grid.best_params_   \n",
    "\n",
    "  elif estimator == 'qda':\n",
    "\n",
    "    param_grid = {'reg_param': [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]}    \n",
    "    \n",
    "    model_grid = RandomizedSearchCV(estimator=QuadraticDiscriminantAnalysis(), \n",
    "                                    param_distributions=param_grid, scoring=optimize, n_iter=n_iter, \n",
    "                                    cv=fold, random_state=seed, n_jobs=-1)\n",
    "    \n",
    "    model_grid.fit(X_train,y_train)\n",
    "    model = model_grid.best_estimator_\n",
    "    best_model = model_grid.best_estimator_\n",
    "    best_model_param = model_grid.best_params_      \n",
    "\n",
    "  elif estimator == 'lda':\n",
    "\n",
    "    param_grid = {'solver' : ['lsqr', 'eigen'],\n",
    "                  'shrinkage': [0.0001, 0.001, 0.01, 0.0005, 0.005, 0.05, 0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "                 }    \n",
    "    \n",
    "    model_grid = RandomizedSearchCV(estimator=LinearDiscriminantAnalysis(), \n",
    "                                    param_distributions=param_grid, scoring=optimize, n_iter=n_iter, \n",
    "                                    cv=fold, random_state=seed, n_jobs=-1)\n",
    "    \n",
    "    model_grid.fit(X_train,y_train)\n",
    "    model = model_grid.best_estimator_\n",
    "    best_model = model_grid.best_estimator_\n",
    "    best_model_param = model_grid.best_params_        \n",
    "\n",
    "  elif estimator == 'et':\n",
    "\n",
    "    param_grid = {'n_estimators': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "                  'criterion': ['gini', 'entropy'],\n",
    "                  'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],\n",
    "                  'min_samples_split': [2, 5, 7, 9, 10],\n",
    "                  'min_samples_leaf' : [1, 2, 4],\n",
    "                  'max_features' : ['auto', 'sqrt', 'log2'],\n",
    "                  'bootstrap': [True, False]\n",
    "                 }    \n",
    "    \n",
    "    model_grid = RandomizedSearchCV(estimator=ExtraTreesClassifier(random_state=seed), \n",
    "                                    param_distributions=param_grid, scoring=optimize, n_iter=n_iter, \n",
    "                                    cv=fold, random_state=seed, n_jobs=-1)\n",
    "    \n",
    "    model_grid.fit(X_train,y_train)\n",
    "    model = model_grid.best_estimator_\n",
    "    best_model = model_grid.best_estimator_\n",
    "    best_model_param = model_grid.best_params_          \n",
    "    \n",
    "  if estimator == 'dt' and ensemble == True and method == 'Bagging':\n",
    "    \n",
    "    #when using normal BaggingClassifier() DT estimator raise's an exception for max_features parameter. Hence a separate \n",
    "    #call has been made for estimator='dt' and method = 'Bagging' where max_features has been removed from param_grid_dt.\n",
    "    \n",
    "    param_grid = {'n_estimators': [10,15,20,25,30],\n",
    "                 'max_samples': [0.3,0.5,0.6,0.7,0.8,0.9],\n",
    "                 'max_features':[0.3,0.5,0.6,0.7,0.8,0.9],\n",
    "                 'bootstrap': [True, False],\n",
    "                 'bootstrap_features': [True, False],\n",
    "                 }\n",
    "    \n",
    "    param_grid_dt = {\"max_depth\": np.random.randint(3, (len(X_train.columns)*.85),4),\n",
    "                  \"min_samples_leaf\": [2,3,4],\n",
    "                  \"criterion\": [\"gini\", \"entropy\"]}\n",
    "    \n",
    "    \n",
    "    model_grid = RandomizedSearchCV(estimator=DecisionTreeClassifier(random_state=seed), param_distributions=param_grid_dt,\n",
    "                                   scoring=optimize, n_iter=n_iter, cv=fold, random_state=seed,\n",
    "                                   iid=False, n_jobs=-1)\n",
    " \n",
    "    model_grid.fit(X_train,y_train)\n",
    "    model = model_grid.best_estimator_\n",
    "    best_model = model_grid.best_estimator_\n",
    "    best_model_param = model_grid.best_params_\n",
    "    \n",
    "    best_model = BaggingClassifier(best_model, random_state=seed)\n",
    "    \n",
    "    model_grid = RandomizedSearchCV(estimator=best_model, \n",
    "                                    param_distributions=param_grid, scoring=optimize, n_iter=n_iter, \n",
    "                                    cv=fold, random_state=seed, iid=False, n_jobs=-1)\n",
    " \n",
    "    model_grid.fit(X_train,y_train)\n",
    "    model = model_grid.best_estimator_\n",
    "    best_model = model_grid.best_estimator_\n",
    "    best_model_param = model_grid.best_params_    \n",
    "  \n",
    "  elif ensemble and method == 'Bagging':\n",
    "    \n",
    "    param_grid = {'n_estimators': [10,15,20,25,30],\n",
    "                 'max_samples': [0.3,0.5,0.6,0.7,0.8,0.9],\n",
    "                 'max_features':[0.3,0.5,0.6,0.7,0.8,0.9],\n",
    "                 'bootstrap': [True, False],\n",
    "                 'bootstrap_features': [True, False],\n",
    "                 }\n",
    "\n",
    "    best_model = BaggingClassifier(best_model, random_state=seed)\n",
    "    \n",
    "    model_grid = RandomizedSearchCV(estimator=best_model, \n",
    "                                    param_distributions=param_grid, scoring=optimize, n_iter=n_iter, \n",
    "                                    cv=fold, random_state=seed, iid=False, n_jobs=-1)\n",
    " \n",
    "    model_grid.fit(X_train,y_train)\n",
    "    model = model_grid.best_estimator_\n",
    "    best_model = model_grid.best_estimator_\n",
    "    best_model_param = model_grid.best_params_    \n",
    "  \n",
    "      \n",
    "  elif ensemble and method =='Boosting':\n",
    "        \n",
    "    param_grid = {'n_estimators': [25,35,50,60,70,75],\n",
    "                 'learning_rate': [1,0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2],\n",
    "                 }        \n",
    "    \n",
    "    best_model = AdaBoostClassifier(best_model, random_state=seed)\n",
    "    \n",
    "    model_grid = RandomizedSearchCV(estimator=best_model, \n",
    "                                    param_distributions=param_grid, scoring=optimize, n_iter=n_iter, \n",
    "                                    cv=fold, random_state=seed, iid=False, n_jobs=-1)\n",
    "\n",
    "  for train_i , test_i in kf.split(data_X,data_y):\n",
    "    \n",
    "    Xtrain,Xtest = data_X.iloc[train_i], data_X.iloc[test_i]\n",
    "    ytrain,ytest = data_y.iloc[train_i], data_y.iloc[test_i]\n",
    "    \n",
    "    if hasattr(best_model, 'predict_proba'):  \n",
    "        \n",
    "        model.fit(Xtrain,ytrain)\n",
    "        pred_prob = model.predict_proba(Xtest)\n",
    "        pred_prob = pred_prob[:,1]\n",
    "        pred_ = model.predict(Xtest)\n",
    "        sca = metrics.accuracy_score(ytest,pred_)\n",
    "        sc = metrics.roc_auc_score(ytest,pred_prob)\n",
    "        recall = metrics.recall_score(ytest,pred_)\n",
    "        precision = metrics.precision_score(ytest,pred_)\n",
    "        kappa = cohen_kappa_score(ytest,pred_)\n",
    "        f1 = metrics.f1_score(ytest,pred_)\n",
    "        score_acc = np.append(score_acc,sca)\n",
    "        score_auc = np.append(score_auc,sc)\n",
    "        score_recall = np.append(score_recall,recall)\n",
    "        score_precision = np.append(score_precision,precision)\n",
    "        score_f1 =np.append(score_f1,f1)\n",
    "        score_kappa =np.append(score_kappa,kappa)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        model.fit(Xtrain,ytrain)\n",
    "        pred_prob = 0.00\n",
    "        pred_prob = 0.00\n",
    "        pred_ = model.predict(Xtest)\n",
    "        sca = metrics.accuracy_score(ytest,pred_)\n",
    "        sc = 0.00\n",
    "        recall = metrics.recall_score(ytest,pred_)\n",
    "        precision = metrics.precision_score(ytest,pred_) #change pred_prob to pred_\n",
    "        kappa = cohen_kappa_score(ytest,pred_)\n",
    "        f1 = metrics.f1_score(ytest,pred_)\n",
    "        score_acc = np.append(score_acc,sca)\n",
    "        score_auc = np.append(score_auc,sc)\n",
    "        score_recall = np.append(score_recall,recall)\n",
    "        score_precision = np.append(score_precision,precision)\n",
    "        score_f1 =np.append(score_f1,f1)\n",
    "        score_kappa =np.append(score_kappa,kappa) \n",
    "        \n",
    "  mean_acc=np.mean(score_acc)\n",
    "  mean_auc=np.mean(score_auc)\n",
    "  mean_recall=np.mean(score_recall)\n",
    "  mean_precision=np.mean(score_precision)\n",
    "  mean_f1=np.mean(score_f1)\n",
    "  mean_kappa=np.mean(score_kappa)\n",
    "  std_acc=np.std(score_acc)\n",
    "  std_auc=np.std(score_auc)\n",
    "  std_recall=np.std(score_recall)\n",
    "  std_precision=np.std(score_precision)\n",
    "  std_f1=np.std(score_f1)\n",
    "  std_kappa=np.std(score_kappa)\n",
    "    \n",
    "  avgs_acc = np.append(avgs_acc, mean_acc)\n",
    "  avgs_acc = np.append(avgs_acc, std_acc) \n",
    "  avgs_auc = np.append(avgs_auc, mean_auc)\n",
    "  avgs_auc = np.append(avgs_auc, std_auc)\n",
    "  avgs_recall = np.append(avgs_recall, mean_recall)\n",
    "  avgs_recall = np.append(avgs_recall, std_recall)\n",
    "  avgs_precision = np.append(avgs_precision, mean_precision)\n",
    "  avgs_precision = np.append(avgs_precision, std_precision)\n",
    "  avgs_f1 = np.append(avgs_f1, mean_f1)\n",
    "  avgs_f1 = np.append(avgs_f1, std_f1)\n",
    "  avgs_kappa = np.append(avgs_kappa, mean_kappa)\n",
    "  avgs_kappa = np.append(avgs_kappa, std_kappa)\n",
    "       \n",
    "  model_results = pd.DataFrame({'Accuracy': score_acc, 'AUC': score_auc, 'Recall' : score_recall, 'Prec.' : score_precision , \n",
    "                     'F1' : score_f1, 'Kappa' : score_kappa})\n",
    "  model_avgs = pd.DataFrame({'Accuracy': avgs_acc, 'AUC': avgs_auc, 'Recall' : avgs_recall, 'Prec.' : avgs_precision , \n",
    "                     'F1' : avgs_f1, 'Kappa' : avgs_kappa},index=['Mean', 'SD'])\n",
    "  \n",
    "  model_results = model_results.append(model_avgs)\n",
    "  model_results = model_results.round(round)\n",
    "  \n",
    "  if verbose:\n",
    "    display(model_results)\n",
    "    return best_model\n",
    "  else:\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blend_models(estimator_list = None, \n",
    "                 fold = 10, \n",
    "                 round = 4, \n",
    "                 sort = 'Accuracy',\n",
    "                 method = 'soft'):\n",
    "  \n",
    "  data_X = X_train\n",
    "  data_y = y_train\n",
    "    \n",
    "  from sklearn.linear_model import LogisticRegression\n",
    "  from sklearn.neighbors import KNeighborsClassifier\n",
    "  from sklearn.naive_bayes import GaussianNB\n",
    "  from sklearn.tree import DecisionTreeClassifier\n",
    "  from sklearn.svm import SVC\n",
    "  from sklearn.gaussian_process.kernels import RBF\n",
    "  from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "  from sklearn.neural_network import MLPClassifier\n",
    "  from sklearn.model_selection import StratifiedKFold\n",
    "  import numpy as np\n",
    "  import pandas as pd\n",
    "  import pandas_profiling as pd_p\n",
    "  import seaborn as sns\n",
    "  from sklearn import preprocessing as pre\n",
    "  from sklearn.pipeline import Pipeline as pipe\n",
    "  from sklearn.model_selection import train_test_split\n",
    "  from sklearn.linear_model import RidgeClassifier\n",
    "  from sklearn.linear_model import Lasso\n",
    "  from sklearn.linear_model import LogisticRegression\n",
    "  from sklearn.linear_model import SGDClassifier\n",
    "  from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "  from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "  from sklearn import metrics\n",
    "  from sklearn.ensemble import RandomForestClassifier\n",
    "  from sklearn.model_selection import GridSearchCV\n",
    "  from sklearn.model_selection import RandomizedSearchCV\n",
    "  from scipy import stats\n",
    "  import random\n",
    "  from sklearn.model_selection import cross_val_score\n",
    "  from sklearn.model_selection import cross_val_predict\n",
    "  from sklearn.model_selection import cross_validate\n",
    "  from sklearn.ensemble import AdaBoostClassifier\n",
    "  from sklearn.ensemble import GradientBoostingClassifier\n",
    "  from sklearn.ensemble import ExtraTreesClassifier\n",
    "  from sklearn.model_selection import StratifiedKFold\n",
    "  from sklearn.metrics import roc_auc_score\n",
    "  from sklearn.metrics import cohen_kappa_score\n",
    "  from sklearn.ensemble import BaggingClassifier\n",
    "  from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "  kf = StratifiedKFold(fold, random_state=seed)\n",
    "\n",
    "  score_auc =np.empty((0,0))\n",
    "  score_acc =np.empty((0,0))\n",
    "  score_recall =np.empty((0,0))\n",
    "  score_precision =np.empty((0,0))\n",
    "  score_f1 =np.empty((0,0))\n",
    "  score_kappa =np.empty((0,0))\n",
    "  avgs_auc =np.empty((0,0))\n",
    "  avgs_acc =np.empty((0,0))\n",
    "  avgs_recall =np.empty((0,0))\n",
    "  avgs_precision =np.empty((0,0))\n",
    "  avgs_f1 =np.empty((0,0))\n",
    "  avgs_kappa =np.empty((0,0))\n",
    "  avg_acc = np.empty((0,0))\n",
    "  avg_auc = np.empty((0,0))\n",
    "  avg_recall = np.empty((0,0))\n",
    "  avg_precision = np.empty((0,0))\n",
    "  avg_f1 = np.empty((0,0))\n",
    "  avg_kappa = np.empty((0,0))\n",
    "  \n",
    "    \n",
    "  lr = LogisticRegression(random_state=seed)\n",
    "  knn = KNeighborsClassifier()\n",
    "  nb = GaussianNB()\n",
    "  dt = DecisionTreeClassifier(random_state=seed)\n",
    "  svm = SGDClassifier(max_iter=1000, tol=0.001, random_state=seed)\n",
    "  rbfsvm = SVC(gamma='auto', C=1, probability=True, kernel='rbf', random_state=seed)\n",
    "  gpc = GaussianProcessClassifier(random_state=seed)\n",
    "  mlp = MLPClassifier(max_iter=500, random_state=seed)\n",
    "  ridge = RidgeClassifier(random_state=seed)\n",
    "  rf = RandomForestClassifier(n_estimators=10, random_state=seed)\n",
    "  qda = QuadraticDiscriminantAnalysis()\n",
    "  ada = AdaBoostClassifier(random_state=seed)\n",
    "  gbc = GradientBoostingClassifier(random_state=seed)\n",
    "  lda = LinearDiscriminantAnalysis()\n",
    "  et = ExtraTreesClassifier(random_state=seed)  \n",
    "    \n",
    "    \n",
    "  if estimator_list == None:\n",
    "    estimator_list = [lr,knn,nb,dt,svm,rbfsvm,gpc,mlp,ridge,rf,qda,ada,gbc,lda,et]\n",
    "    voting = 'hard'\n",
    "\n",
    "  else:\n",
    "    estimator_list = estimator_list\n",
    "    voting = method  \n",
    "      \n",
    "  model_names = []\n",
    "\n",
    "  for names in estimator_list:\n",
    "    model_names = np.append(model_names, str(names).split(\"(\")[0])\n",
    "\n",
    "  import re\n",
    "\n",
    "  def putSpace(input):\n",
    "        words = re.findall('[A-Z][a-z]*', input)\n",
    "        words = ' '.join(words)\n",
    "        return words  \n",
    "\n",
    "  model_names_modified = []\n",
    "  \n",
    "  for i in model_names:\n",
    "    model_names_modified.append(putSpace(i))\n",
    "\n",
    "    model_names = model_names_modified\n",
    "\n",
    "  model_names_final = []\n",
    "  \n",
    "  for j in model_names:\n",
    "    if j == 'Gaussian N B':\n",
    "        model_names_final.append('Naive Bayes')\n",
    "    elif j == 'M L P Classifier':\n",
    "        model_names_final.append('MLP Classifier')\n",
    "    elif j == 'S G D Classifier':\n",
    "        model_names_final.append('SVM - Linear Kernel')\n",
    "    elif j == 'S V C':\n",
    "        model_names_final.append('SVM - Radial Kernel')\n",
    "    else: \n",
    "        model_names_final.append(j)\n",
    "\n",
    "  model_names = model_names_final\n",
    "  estimator_list = estimator_list\n",
    "  estimator_list = zip(model_names, estimator_list)\n",
    "  estimator_list = set(estimator_list)\n",
    "  estimator_list = list(estimator_list)\n",
    "    \n",
    "  model = VotingClassifier(estimators=estimator_list, voting=voting, n_jobs=-1)\n",
    "  \n",
    "  for train_i , test_i in kf.split(data_X,data_y):\n",
    "    \n",
    "    Xtrain,Xtest = data_X.iloc[train_i], data_X.iloc[test_i]\n",
    "    ytrain,ytest = data_y.iloc[train_i], data_y.iloc[test_i]    \n",
    "    \n",
    "    if voting == 'hard':\n",
    "        \n",
    "        model.fit(Xtrain,ytrain)\n",
    "        pred_prob = 0.0\n",
    "        pred_prob = 0.0\n",
    "        pred_ = model.predict(Xtest)\n",
    "        sca = metrics.accuracy_score(ytest,pred_)\n",
    "        sc = 0.0\n",
    "        recall = metrics.recall_score(ytest,pred_)\n",
    "        precision = metrics.precision_score(ytest,pred_)\n",
    "        kappa = cohen_kappa_score(ytest,pred_)\n",
    "        f1 = metrics.f1_score(ytest,pred_)\n",
    "        score_acc = np.append(score_acc,sca)\n",
    "        score_auc = np.append(score_auc,sc)\n",
    "        score_recall = np.append(score_recall,recall)\n",
    "        score_precision = np.append(score_precision,precision)\n",
    "        score_f1 =np.append(score_f1,f1)\n",
    "        score_kappa =np.append(score_kappa,kappa)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        model.fit(Xtrain,ytrain)\n",
    "        pred_prob = model.predict_proba(Xtest)\n",
    "        pred_prob = pred_prob[:,1]\n",
    "        pred_ = model.predict(Xtest)\n",
    "        sca = metrics.accuracy_score(ytest,pred_)\n",
    "        sc = metrics.roc_auc_score(ytest,pred_prob)\n",
    "        recall = metrics.recall_score(ytest,pred_)\n",
    "        precision = metrics.precision_score(ytest,pred_)\n",
    "        kappa = cohen_kappa_score(ytest,pred_)\n",
    "        f1 = metrics.f1_score(ytest,pred_)\n",
    "        score_acc = np.append(score_acc,sca)\n",
    "        score_auc = np.append(score_auc,sc)\n",
    "        score_recall = np.append(score_recall,recall)\n",
    "        score_precision = np.append(score_precision,precision)\n",
    "        score_f1 =np.append(score_f1,f1)\n",
    "        score_kappa =np.append(score_kappa,kappa)\n",
    "       \n",
    "  mean_acc=np.mean(score_acc)\n",
    "  mean_auc=np.mean(score_auc)\n",
    "  mean_recall=np.mean(score_recall)\n",
    "  mean_precision=np.mean(score_precision)\n",
    "  mean_f1=np.mean(score_f1)\n",
    "  mean_kappa=np.mean(score_kappa)\n",
    "  std_acc=np.std(score_acc)\n",
    "  std_auc=np.std(score_auc)\n",
    "  std_recall=np.std(score_recall)\n",
    "  std_precision=np.std(score_precision)\n",
    "  std_f1=np.std(score_f1)\n",
    "  std_kappa=np.std(score_kappa)\n",
    "    \n",
    "  avgs_acc = np.append(avgs_acc, mean_acc)\n",
    "  avgs_acc = np.append(avgs_acc, std_acc) \n",
    "  avgs_auc = np.append(avgs_auc, mean_auc)\n",
    "  avgs_auc = np.append(avgs_auc, std_auc)\n",
    "  avgs_recall = np.append(avgs_recall, mean_recall)\n",
    "  avgs_recall = np.append(avgs_recall, std_recall)\n",
    "  avgs_precision = np.append(avgs_precision, mean_precision)\n",
    "  avgs_precision = np.append(avgs_precision, std_precision)\n",
    "  avgs_f1 = np.append(avgs_f1, mean_f1)\n",
    "  avgs_f1 = np.append(avgs_f1, std_f1)\n",
    "  avgs_kappa = np.append(avgs_kappa, mean_kappa)\n",
    "  avgs_kappa = np.append(avgs_kappa, std_kappa)\n",
    "      \n",
    "  model_results = pd.DataFrame({'Accuracy': score_acc, 'AUC': score_auc, 'Recall' : score_recall, 'Prec.' : score_precision , \n",
    "                     'F1' : score_f1, 'Kappa' : score_kappa})\n",
    "  model_avgs = pd.DataFrame({'Accuracy': avgs_acc, 'AUC': avgs_auc, 'Recall' : avgs_recall, 'Prec.' : avgs_precision , \n",
    "                     'F1' : avgs_f1, 'Kappa' : avgs_kappa},index=['Mean', 'SD'])\n",
    "  \n",
    "  model_results = model_results.append(model_avgs)\n",
    "  model_results = model_results.round(round)\n",
    "  display(model_results)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_models(estimator_list, \n",
    "                 meta_model = None, \n",
    "                 fold = 10,\n",
    "                 round = 4, \n",
    "                 method = 'soft', \n",
    "                 restack = False, \n",
    "                 plot = False):\n",
    "    \n",
    "    #Capturing the method of stacking required by user. method='soft' means 'predict_proba' else 'predict'\n",
    "    \n",
    "    if method == 'soft':\n",
    "        predict_method = 'predict_proba'\n",
    "    elif method == 'hard':\n",
    "        predict_method = 'predict'\n",
    "    \n",
    "    #Defining meta model. Logistic Regression hardcoded for now\n",
    "    \n",
    "    if meta_model == None:\n",
    "        meta_model = LogisticRegression()\n",
    "    else:\n",
    "        meta_model = meta_model\n",
    "    \n",
    "    #defining model_library model names\n",
    "    \n",
    "    model_names = np.zeros(0)\n",
    "    for item in estimator_list:\n",
    "        model_names = np.append(model_names, str(item).split(\"(\")[0])\n",
    "    \n",
    "    ##########################\n",
    "    ##########################\n",
    "    ##########################\n",
    "    \n",
    "    base_array = np.zeros((0,0))\n",
    "    base_prediction = pd.DataFrame(y_train)\n",
    "    base_prediction = base_prediction.reset_index(drop=True)\n",
    "    \n",
    "    for model in estimator_list:\n",
    "        base_array = cross_val_predict(model,X_train,y_train,cv=fold, method=predict_method)\n",
    "        if method == 'soft':\n",
    "            base_array = base_array[:,1]\n",
    "        elif method == 'hard':\n",
    "            base_array = base_array\n",
    "        base_array_df = pd.DataFrame(base_array)\n",
    "        base_prediction = pd.concat([base_prediction,base_array_df],axis=1)\n",
    "        base_array = np.empty((0,0))\n",
    "        \n",
    "    #defining column names now\n",
    "    target_col_name = np.array(base_prediction.columns[0])\n",
    "    model_names = np.append(target_col_name, model_names)\n",
    "    base_prediction.columns = model_names #defining colum names now\n",
    "    \n",
    "    #defining data_X and data_y dataframe to be used in next stage.\n",
    "    \n",
    "    if restack:\n",
    "        data_X_ = X_train\n",
    "        data_X_ = data_X_.reset_index(drop=True)\n",
    "        data_X = base_prediction.drop(base_prediction.columns[0],axis=1)\n",
    "        data_X = pd.concat([data_X_,data_X],axis=1)\n",
    "        \n",
    "    elif restack == False:\n",
    "        data_X = base_prediction.drop(base_prediction.columns[0],axis=1)\n",
    "        \n",
    "    data_y = base_prediction[base_prediction.columns[0]]\n",
    "    \n",
    "    #Correlation matrix of base_prediction\n",
    "    base_prediction_cor = base_prediction.drop(base_prediction.columns[0],axis=1)\n",
    "    base_prediction_cor = base_prediction_cor.corr()\n",
    "    \n",
    "    #Meta Modeling Starts Here\n",
    "    \n",
    "    model = meta_model #this defines model to be used below as model = meta_model (as captured above)\n",
    "\n",
    "    kf = StratifiedKFold(fold, random_state=seed) #capturing fold requested by user\n",
    "\n",
    "    score_auc =np.empty((0,0))\n",
    "    score_acc =np.empty((0,0))\n",
    "    score_recall =np.empty((0,0))\n",
    "    score_precision =np.empty((0,0))\n",
    "    score_f1 =np.empty((0,0))\n",
    "    score_kappa =np.empty((0,0))\n",
    "    avgs_auc =np.empty((0,0))\n",
    "    avgs_acc =np.empty((0,0))\n",
    "    avgs_recall =np.empty((0,0))\n",
    "    avgs_precision =np.empty((0,0))\n",
    "    avgs_f1 =np.empty((0,0))\n",
    "    avgs_kappa =np.empty((0,0))\n",
    "    \n",
    "    for train_i , test_i in kf.split(data_X,data_y):\n",
    "        Xtrain,Xtest = data_X.iloc[train_i], data_X.iloc[test_i]\n",
    "        ytrain,ytest = data_y.iloc[train_i], data_y.iloc[test_i]\n",
    "\n",
    "        model.fit(Xtrain,ytrain)\n",
    "        pred_prob = model.predict_proba(Xtest)\n",
    "        pred_prob = pred_prob[:,1]\n",
    "        pred_ = model.predict(Xtest)\n",
    "        sca = metrics.accuracy_score(ytest,pred_)\n",
    "        sc = metrics.roc_auc_score(ytest,pred_prob)\n",
    "        recall = metrics.recall_score(ytest,pred_)\n",
    "        precision = metrics.average_precision_score(ytest,pred_prob)\n",
    "        kappa = cohen_kappa_score(ytest,pred_)\n",
    "        f1 = metrics.f1_score(ytest,pred_)\n",
    "        score_acc = np.append(score_acc,sca)\n",
    "        score_auc = np.append(score_auc,sc)\n",
    "        score_recall = np.append(score_recall,recall)\n",
    "        score_precision = np.append(score_precision,precision)\n",
    "        score_f1 =np.append(score_f1,f1)\n",
    "        score_kappa =np.append(score_kappa,kappa)\n",
    "     \n",
    "    mean_acc=np.mean(score_acc)\n",
    "    mean_auc=np.mean(score_auc)\n",
    "    mean_recall=np.mean(score_recall)\n",
    "    mean_precision=np.mean(score_precision)\n",
    "    mean_f1=np.mean(score_f1)\n",
    "    mean_kappa=np.mean(score_kappa)\n",
    "    std_acc=np.std(score_acc)\n",
    "    std_auc=np.std(score_auc)\n",
    "    std_recall=np.std(score_recall)\n",
    "    std_precision=np.std(score_precision)\n",
    "    std_f1=np.std(score_f1)\n",
    "    std_kappa=np.std(score_kappa)\n",
    "    \n",
    "    avgs_acc = np.append(avgs_acc, mean_acc)\n",
    "    avgs_acc = np.append(avgs_acc, std_acc) \n",
    "    avgs_auc = np.append(avgs_auc, mean_auc)\n",
    "    avgs_auc = np.append(avgs_auc, std_auc)\n",
    "    avgs_recall = np.append(avgs_recall, mean_recall)\n",
    "    avgs_recall = np.append(avgs_recall, std_recall)\n",
    "    avgs_precision = np.append(avgs_precision, mean_precision)\n",
    "    avgs_precision = np.append(avgs_precision, std_precision)\n",
    "    avgs_f1 = np.append(avgs_f1, mean_f1)\n",
    "    avgs_f1 = np.append(avgs_f1, std_f1)\n",
    "    avgs_kappa = np.append(avgs_kappa, mean_kappa)\n",
    "    avgs_kappa = np.append(avgs_kappa, std_kappa)\n",
    "      \n",
    "    model_results = pd.DataFrame({'Accuracy': score_acc, 'AUC': score_auc, 'Recall' : score_recall, 'Prec.' : score_precision , \n",
    "                     'F1' : score_f1, 'Kappa' : score_kappa})\n",
    "    model_avgs = pd.DataFrame({'Accuracy': avgs_acc, 'AUC': avgs_auc, 'Recall' : avgs_recall, 'Prec.' : avgs_precision , \n",
    "                     'F1' : avgs_f1, 'Kappa' : avgs_kappa},index=['Mean', 'SD'])\n",
    "  \n",
    "    model_results = model_results.append(model_avgs)\n",
    "    model_results = model_results.round(round)  \n",
    "    \n",
    "    models = []\n",
    "    for i in estimator_list:\n",
    "        models.append(i)\n",
    "    \n",
    "    models.append(meta_model)\n",
    "    \n",
    "    if plot:\n",
    "        ax = sns.heatmap(base_prediction_cor, vmin=-0.5, vmax=1, center=0,cmap='magma', square=True, annot=True, \n",
    "                         linewidths=1)\n",
    "    \n",
    "    else:\n",
    "        display(model_results)\n",
    "        return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stacknet(estimator_list,\n",
    "                    meta_model = None,\n",
    "                    fold = 10,\n",
    "                    round = 4,\n",
    "                    method = 'soft',\n",
    "                    restack = False):\n",
    "    \n",
    "    global base_array_df\n",
    "    \n",
    "    base_level = estimator_list[0]\n",
    "    inter_level = estimator_list[1:]\n",
    "    data_X = X_train\n",
    "    data_y = y_train\n",
    "    \n",
    "    #defining meta model\n",
    "    \n",
    "    if meta_model == None:\n",
    "        meta_model = LogisticRegression()\n",
    "    else:\n",
    "        meta_model = meta_model\n",
    "    \n",
    "    #Capturing the method of stacking required by user. method='soft' means 'predict_proba' else 'predict'\n",
    "    \n",
    "    if method == 'soft':\n",
    "        predict_method = 'predict_proba'\n",
    "    elif method == 'hard':\n",
    "        predict_method = 'predict'\n",
    "        \n",
    "        \n",
    "    base_array = np.zeros((0,0))\n",
    "    base_array_df = pd.DataFrame()\n",
    "    base_prediction = pd.DataFrame(y_train)\n",
    "    base_prediction = base_prediction.reset_index(drop=True)\n",
    "    \n",
    "    for model in base_level:\n",
    "                     \n",
    "        base_array = cross_val_predict(model,X_train,y_train,cv=fold, method=predict_method)\n",
    "        if method == 'soft':\n",
    "            base_array = base_array[:,1]\n",
    "        elif method == 'hard':\n",
    "            base_array = base_array\n",
    "        base_array = pd.DataFrame(base_array)\n",
    "        base_array_df = pd.concat([base_array_df, base_array], axis=1)\n",
    "        base_array = np.empty((0,0))  \n",
    "        \n",
    "    for level in inter_level:\n",
    "        \n",
    "        for model in level:\n",
    "            \n",
    "            base_array = cross_val_predict(model,base_array_df,base_prediction,cv=fold, method=predict_method)\n",
    "            if method == 'soft':\n",
    "                base_array = base_array[:,1]\n",
    "            elif method == 'hard':\n",
    "                base_array = base_array\n",
    "            base_array = pd.DataFrame(base_array)\n",
    "            base_array_df = pd.concat([base_array, base_array_df], axis=1)\n",
    "            base_array = np.empty((0,0))\n",
    "        \n",
    "        if restack == False:\n",
    "            base_array_df = base_array_df.iloc[:,:len(level)]\n",
    "        else:\n",
    "            base_array_df = base_array_df\n",
    "    \n",
    "    model = meta_model\n",
    "    \n",
    "    kf = StratifiedKFold(fold, random_state=seed) #capturing fold requested by user\n",
    "\n",
    "    score_auc =np.empty((0,0))\n",
    "    score_acc =np.empty((0,0))\n",
    "    score_recall =np.empty((0,0))\n",
    "    score_precision =np.empty((0,0))\n",
    "    score_f1 =np.empty((0,0))\n",
    "    score_kappa =np.empty((0,0))\n",
    "    avgs_auc =np.empty((0,0))\n",
    "    avgs_acc =np.empty((0,0))\n",
    "    avgs_recall =np.empty((0,0))\n",
    "    avgs_precision =np.empty((0,0))\n",
    "    avgs_f1 =np.empty((0,0))\n",
    "    avgs_kappa =np.empty((0,0))\n",
    "    \n",
    "    for train_i , test_i in kf.split(data_X,data_y):\n",
    "        \n",
    "        Xtrain,Xtest = data_X.iloc[train_i], data_X.iloc[test_i]\n",
    "        ytrain,ytest = data_y.iloc[train_i], data_y.iloc[test_i]\n",
    "\n",
    "        model.fit(Xtrain,ytrain)\n",
    "        pred_prob = model.predict_proba(Xtest)\n",
    "        pred_prob = pred_prob[:,1]\n",
    "        pred_ = model.predict(Xtest)\n",
    "        sca = metrics.accuracy_score(ytest,pred_)\n",
    "        sc = metrics.roc_auc_score(ytest,pred_prob)\n",
    "        recall = metrics.recall_score(ytest,pred_)\n",
    "        precision = metrics.average_precision_score(ytest,pred_prob)\n",
    "        kappa = cohen_kappa_score(ytest,pred_)\n",
    "        f1 = metrics.f1_score(ytest,pred_)\n",
    "        score_acc = np.append(score_acc,sca)\n",
    "        score_auc = np.append(score_auc,sc)\n",
    "        score_recall = np.append(score_recall,recall)\n",
    "        score_precision = np.append(score_precision,precision)\n",
    "        score_f1 =np.append(score_f1,f1)\n",
    "        score_kappa =np.append(score_kappa,kappa)\n",
    "     \n",
    "    mean_acc=np.mean(score_acc)\n",
    "    mean_auc=np.mean(score_auc)\n",
    "    mean_recall=np.mean(score_recall)\n",
    "    mean_precision=np.mean(score_precision)\n",
    "    mean_f1=np.mean(score_f1)\n",
    "    mean_kappa=np.mean(score_kappa)\n",
    "    std_acc=np.std(score_acc)\n",
    "    std_auc=np.std(score_auc)\n",
    "    std_recall=np.std(score_recall)\n",
    "    std_precision=np.std(score_precision)\n",
    "    std_f1=np.std(score_f1)\n",
    "    std_kappa=np.std(score_kappa)\n",
    "    \n",
    "    avgs_acc = np.append(avgs_acc, mean_acc)\n",
    "    avgs_acc = np.append(avgs_acc, std_acc) \n",
    "    avgs_auc = np.append(avgs_auc, mean_auc)\n",
    "    avgs_auc = np.append(avgs_auc, std_auc)\n",
    "    avgs_recall = np.append(avgs_recall, mean_recall)\n",
    "    avgs_recall = np.append(avgs_recall, std_recall)\n",
    "    avgs_precision = np.append(avgs_precision, mean_precision)\n",
    "    avgs_precision = np.append(avgs_precision, std_precision)\n",
    "    avgs_f1 = np.append(avgs_f1, mean_f1)\n",
    "    avgs_f1 = np.append(avgs_f1, std_f1)\n",
    "    avgs_kappa = np.append(avgs_kappa, mean_kappa)\n",
    "    avgs_kappa = np.append(avgs_kappa, std_kappa)\n",
    "      \n",
    "    model_results = pd.DataFrame({'Accuracy': score_acc, 'AUC': score_auc, 'Recall' : score_recall, 'Prec.' : score_precision , \n",
    "                     'F1' : score_f1, 'Kappa' : score_kappa})\n",
    "    model_avgs = pd.DataFrame({'Accuracy': avgs_acc, 'AUC': avgs_auc, 'Recall' : avgs_recall, 'Prec.' : avgs_precision , \n",
    "                     'F1' : avgs_f1, 'Kappa' : avgs_kappa},index=['Mean', 'SD'])\n",
    "  \n",
    "    model_results = model_results.append(model_avgs)\n",
    "    model_results = model_results.round(round)      \n",
    "    \n",
    "    display(model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_model(estimator='lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = create_model(estimator='lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = create_model(estimator='rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = create_model(estimator='svm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = create_model(estimator='nb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(nb, 'confusion_matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = create_model(estimator='lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(lr, plot='confusion_matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_model(estimator='lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_model(estimator='lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = create_model(estimator='rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model(rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work in Progress / Future Release "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(data_X=X_train, n=3):\n",
    "    global X_train\n",
    "    drop_list = var_imp_array_top_n[0:n]\n",
    "    X_train.drop(drop_list, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_model(estimator='lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules now Available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0. compare_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0. create_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0. plot_model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0. tune_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0. ensemble_model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0 blend_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.0. stack_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.0. create_stacknet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.0. optimze_model (Future Release)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.0. predict_stacknet (Future Release)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.0. calibrate_model (Future Release)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.0. save_model (Future Release) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
